    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2018/11/21/378">First message in thread</a></li><li><a href="/lkml/2018/11/23/504">David Laight</a><ul><li><a href="/lkml/2018/11/23/829">Linus Torvalds</a><ul><li class="origin"><a href="/lkml/2018/11/24/17">Linus Torvalds</a><ul><li><a href="/lkml/2018/11/24/17">Andy Lutomirski</a><ul><li><a href="/lkml/2018/11/24/18">Linus Torvalds</a></li></ul></li></ul></li><li><a href="/lkml/2018/11/26/444">David Laight</a></li><li><a href="/lkml/2018/11/26/474">David Laight</a><ul><li><a href="/lkml/2019/1/4/666">Linus Torvalds</a><ul><li><a href="/lkml/2019/1/7/155">David Laight</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2018/11/23/875/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Fri, 23 Nov 2018 09:42:39 -0800</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH] x86: only use ERMS for user copies for larger sizes</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Fri, Nov 23, 2018 at 8:36 AM Linus Torvalds<br />&lt;torvalds&#64;linux-foundation.org&gt; wrote:<br />&gt;<br />&gt; Let me write a generic routine in lib/iomap_copy.c (which already does<br />&gt; the "user specifies chunk size" cases), and hook it up for x86.<br /><br />Something like this?<br /><br />ENTIRELY UNTESTED! It might not compile. Seriously. And if it does<br />compile, it might not work.<br /><br />And this doesn't actually do the memset_io() function at all, just the<br />memcpy ones.<br /><br />Finally, it's worth noting that on x86, we have this:<br /><br />  /*<br />   * override generic version in lib/iomap_copy.c<br />   */<br />  ENTRY(__iowrite32_copy)<br />          movl %edx,%ecx<br />          rep movsd<br />          ret<br />  ENDPROC(__iowrite32_copy)<br /><br />because back in 2006, we did this:<br /><br />    [PATCH] Add faster __iowrite32_copy routine for x86_64<br /><br />    This assembly version is measurably faster than the generic version in<br />    lib/iomap_copy.c.<br /><br />which actually implies that "rep movsd" is faster than doing<br />__raw_writel() by hand.<br /><br />So it is possible that this should all be arch-specific code rather<br />than that butt-ugly "generic" code I wrote in this patch.<br /><br />End result: I'm not really all that  happy about this patch, but it's<br />perhaps worth testing, and it's definitely worth discussing. Because<br />our current memcpy_{to,from}io() is truly broken garbage.<br /><br />                   Linus<br /> arch/x86/include/asm/io.h |   6 ++<br /> include/linux/io.h        |   2 +<br /> lib/iomap_copy.c          | 153 ++++++++++++++++++++++++++++++++++++++++++++++<br /> 3 files changed, 161 insertions(+)<br /><br />diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h<br />index 832da8229cc7..3b9206ee25b8 100644<br />--- a/arch/x86/include/asm/io.h<br />+++ b/arch/x86/include/asm/io.h<br />&#64;&#64; -92,6 +92,12 &#64;&#64; build_mmio_write(__writel, "l", unsigned int, "r", )<br /> <br /> #define mmiowb() barrier()<br /> <br />+void __iowrite_copy(void __iomem *to, const void *from, size_t count);<br />+void __ioread_copy(void *to, const void __iomem *from, size_t count);<br />+<br />+#define memcpy_toio __iowrite_copy<br />+#define memcpy_fromio __ioread_copy<br />+<br /> #ifdef CONFIG_X86_64<br /> <br /> build_mmio_read(readq, "q", u64, "=r", :"memory")<br />diff --git a/include/linux/io.h b/include/linux/io.h<br />index 32e30e8fb9db..642f78970018 100644<br />--- a/include/linux/io.h<br />+++ b/include/linux/io.h<br />&#64;&#64; -28,6 +28,8 &#64;&#64;<br /> struct device;<br /> struct resource;<br /> <br />+void __ioread_copy(void *to, const void __iomem *from, size_t count);<br />+void __iowrite_copy(void __iomem *to, const void *from, size_t count);<br /> __visible void __iowrite32_copy(void __iomem *to, const void *from, size_t count);<br /> void __ioread32_copy(void *to, const void __iomem *from, size_t count);<br /> void __iowrite64_copy(void __iomem *to, const void *from, size_t count);<br />diff --git a/lib/iomap_copy.c b/lib/iomap_copy.c<br />index b8f1d6cbb200..8edc359dda62 100644<br />--- a/lib/iomap_copy.c<br />+++ b/lib/iomap_copy.c<br />&#64;&#64; -17,6 +17,159 &#64;&#64;<br /> <br /> #include &lt;linux/export.h&gt;<br /> #include &lt;linux/io.h&gt;<br />+#include &lt;asm/unaligned.h&gt;<br />+<br />+static inline bool iomem_align(const void __iomem *ptr, int size, int count)<br />+{<br />+	return count &gt;= size &amp;&amp; (__force unsigned long)ptr &amp; size;<br />+}<br />+<br />+<br />+/**<br />+ * __iowrite_copy - copy data to MMIO space<br />+ * &#64;to: destination, in MMIO space<br />+ * &#64;from: source<br />+ * &#64;count: number of bytes to copy.<br />+ *<br />+ * Copy arbitrarily aligned data from kernel space to MMIO space,<br />+ * using reasonable chunking.<br />+ */<br />+void __attribute__((weak)) __iowrite_copy(void __iomem *to,<br />+					  const void *from,<br />+					  size_t count)<br />+{<br />+	if (iomem_align(to, 1, count)) {<br />+		unsigned char data = *(unsigned char *)from;<br />+		__raw_writeb(data, to);<br />+		from++;<br />+		to++;<br />+		count--;<br />+	}<br />+	if (iomem_align(to, 2, count)) {<br />+		unsigned short data = get_unaligned((unsigned short *)from);<br />+		__raw_writew(data, to);<br />+		from += 2;<br />+		to += 2;<br />+		count -= 2;<br />+	}<br />+#ifdef CONFIG_64BIT<br />+	if (iomem_align(to, 4, count)) {<br />+		unsigned int data = get_unaligned((unsigned int *)from);<br />+		__raw_writel(data, to);<br />+		from += 4;<br />+		to += 4;<br />+		count -= 4;<br />+	}<br />+#endif<br />+	while (count &gt;= sizeof(unsigned long)) {<br />+		unsigned long data = get_unaligned((unsigned long *)from);<br />+#ifdef CONFIG_64BIT<br />+		__raw_writeq(data, to);<br />+#else<br />+		__raw_writel(data, to);<br />+#endif<br />+		from += sizeof(unsigned long);<br />+		to += sizeof(unsigned long);<br />+		count -= sizeof(unsigned long);<br />+	}<br />+<br />+#ifdef CONFIG_64BIT<br />+	if (count &gt;= 4) {<br />+		unsigned int data = get_unaligned((unsigned int *)from);<br />+		__raw_writel(data, to);<br />+		from += 4;<br />+		to += 4;<br />+		count -= 4;<br />+	}<br />+#endif<br />+<br />+	if (count &gt;= 2) {<br />+		unsigned short data = get_unaligned((unsigned short *)from);<br />+		__raw_writew(data, to);<br />+		from += 2;<br />+		to += 2;<br />+		count -= 2;<br />+	}<br />+<br />+	if (count) {<br />+		unsigned char data = *(unsigned char *)from;<br />+		__raw_writeb(data, to);<br />+	}<br />+}<br />+EXPORT_SYMBOL_GPL(__iowrite_copy);<br />+<br />+/**<br />+ * __ioread_copy - copy data from MMIO space<br />+ * &#64;to: destination<br />+ * &#64;from: source, in MMIO space<br />+ * &#64;count: number of bytes to copy.<br />+ *<br />+ * Copy arbitrarily aligned data from MMIO space to kernel space,<br />+ * using reasonable chunking.<br />+ */<br />+void __attribute__((weak)) __ioread_copy(void *to,<br />+					 const void __iomem *from,<br />+					 size_t count)<br />+{<br />+	if (iomem_align(from, 1, count)) {<br />+		unsigned char data = __raw_readb(from);<br />+		put_unaligned(data, (unsigned char *) to);<br />+		from++;<br />+		to++;<br />+		count--;<br />+	}<br />+	if (iomem_align(to, 2, count)) {<br />+		unsigned short data = __raw_readw(from);<br />+		put_unaligned(data, (unsigned short *) to);<br />+		from += 2;<br />+		to += 2;<br />+		count -= 2;<br />+	}<br />+#ifdef CONFIG_64BIT<br />+	if (iomem_align(to, 4, count)) {<br />+		unsigned int data = __raw_readl(from);<br />+		put_unaligned(data, (unsigned int *) to);<br />+		from += 4;<br />+		to += 4;<br />+		count -= 4;<br />+	}<br />+#endif<br />+	while (count &gt;= sizeof(unsigned long)) {<br />+#ifdef CONFIG_64BIT<br />+		unsigned long data = __raw_readq(from);<br />+#else<br />+		unsigned long data = __raw_readl(from);<br />+#endif<br />+		put_unaligned(data, (unsigned long *) to);<br />+		from += sizeof(unsigned long);<br />+		to += sizeof(unsigned long);<br />+		count -= sizeof(unsigned long);<br />+	}<br />+<br />+#ifdef CONFIG_64BIT<br />+	if (count &gt;= 4) {<br />+		unsigned int data = __raw_readl(from);<br />+		put_unaligned(data, (unsigned int *) to);<br />+		from += 4;<br />+		to += 4;<br />+		count -= 4;<br />+	}<br />+#endif<br />+<br />+	if (count &gt;= 2) {<br />+		unsigned short data = __raw_readw(from);<br />+		put_unaligned(data, (unsigned short *) to);<br />+		from += 2;<br />+		to += 2;<br />+		count -= 2;<br />+	}<br />+<br />+	if (count) {<br />+		unsigned char data = __raw_readb(from);<br />+		put_unaligned(data, (unsigned char *) to);<br />+	}<br />+}<br />+EXPORT_SYMBOL_GPL(__ioread_copy);<br /> <br /> /**<br />  * __iowrite32_copy - copy data to MMIO space, in 32-bit units</pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
