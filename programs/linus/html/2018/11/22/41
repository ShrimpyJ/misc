    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2018/11/21/378">First message in thread</a></li><li><a href="/lkml/2018/11/21/915">Linus Torvalds</a><ul><li><a href="/lkml/2018/11/21/962">Jens Axboe</a><ul><li><a href="/lkml/2018/11/22/2">Andy Lutomirski</a><ul><li><a href="/lkml/2018/11/22/30">Linus Torvalds</a><ul><li><a href="/lkml/2018/11/22/336">Andy Lutomirski</a></li></ul></li></ul></li></ul></li><li><a href="/lkml/2018/11/21/971">Linus Torvalds</a><ul><li class="origin"><a href="/lkml/2018/11/22/875">Linus Torvalds</a><ul><li><a href="/lkml/2018/11/22/875">Ingo Molnar</a><ul><li><a href="/lkml/2018/11/22/924">Ingo Molnar</a></li><li><a href="/lkml/2018/11/22/1242">Linus Torvalds</a></li></ul></li></ul></li><li><a href="/lkml/2018/11/24/173">Jens Axboe</a></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2018/11/22/41/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Wed, 21 Nov 2018 11:01:35 -0800</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH] x86: only use ERMS for user copies for larger sizes</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Wed, Nov 21, 2018 at 10:16 AM Linus Torvalds<br />&lt;torvalds&#64;linux-foundation.org&gt; wrote:<br />&gt;<br />&gt; It might be interesting to just change raw_copy_to/from_user() to<br />&gt; handle a lot more cases (in particular, handle cases where 'size' is<br />&gt; 8-byte aligned). The special cases we *do* have may not be the right<br />&gt; ones (the 10-byte case in particular looks odd).<br />&gt;<br />&gt; For example, instead of having a "if constant size is 8 bytes, do one<br />&gt; get/put_user()" case, we might have a "if constant size is &lt; 64 just<br />&gt; unroll it into get/put_user()" calls.<br /><br />Actually, x86 doesn't even set INLINE_COPY_TO_USER, so I don't think<br />the constant size cases ever trigger at all the way they are set up<br />now.<br /><br />I do have a random patch that makes "unsafe_put_user()" actually use<br />"asm goto" for the error case, and that, together with the attached<br />patch seems to generate fairly nice code, but even then it would<br />depend on gcc actually unrolling things (which we do *not* want in<br />general).<br /><br />But for a 32-byte user copy (cp_old_stat), and that<br />INLINE_COPY_TO_USER, it generates this:<br /><br />        stac<br />        movl    $32, %edx       #, size<br />        movq    %rsp, %rax      #, src<br />.L201:<br />        movq    (%rax), %rcx    # MEM[base: src_155, offset: 0B],<br />MEM[base: src_155, offset: 0B]<br />1:      movq %rcx,0(%rbp)       # MEM[base: src_155, offset: 0B],<br />MEM[(struct __large_struct *)dst_156]<br />ASM_EXTABLE_HANDLE from=1b to=.L200 handler="ex_handler_uaccess"        #<br /><br />        addq    $8, %rax        #, src<br />        addq    $8, %rbp        #, statbuf<br />        subq    $8, %rdx        #, size<br />        jne     .L201   #,<br />        clac<br /><br />which is actually fairly close to "optimal".<br /><br />Random patch (with my "asm goto" hack included) attached, in case<br />people want to play with it.<br /><br />Impressively, it actually removes more lines of code than it adds. But<br />I didn't actually check whether the end result *works*, so hey..<br /><br />                  Linus<br /> arch/x86/include/asm/uaccess.h    |  96 +++++++++----------<br /> arch/x86/include/asm/uaccess_64.h | 191 ++++++++++++++++++--------------------<br /> fs/readdir.c                      |  22 +++--<br /> 3 files changed, 149 insertions(+), 160 deletions(-)<br /><br />diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h<br />index b5e58cc0c5e7..3f4c89deb7a1 100644<br />--- a/arch/x86/include/asm/uaccess.h<br />+++ b/arch/x86/include/asm/uaccess.h<br />&#64;&#64; -12,6 +12,9 &#64;&#64;<br /> #include &lt;asm/smap.h&gt;<br /> #include &lt;asm/extable.h&gt;<br /> <br />+#define INLINE_COPY_TO_USER<br />+#define INLINE_COPY_FROM_USER<br />+<br /> /*<br />  * The fs value determines whether argument validity checking should be<br />  * performed or not.  If get_fs() == USER_DS, checking is performed, with<br />&#64;&#64; -189,19 +192,14 &#64;&#64; __typeof__(__builtin_choose_expr(sizeof(x) &gt; sizeof(0UL), 0ULL, 0UL))<br /> <br /> <br /> #ifdef CONFIG_X86_32<br />-#define __put_user_asm_u64(x, addr, err, errret)			\<br />-	asm volatile("\n"						\<br />-		     "1:	movl %%eax,0(%2)\n"			\<br />-		     "2:	movl %%edx,4(%2)\n"			\<br />-		     "3:"						\<br />-		     ".section .fixup,\"ax\"\n"				\<br />-		     "4:	movl %3,%0\n"				\<br />-		     "	jmp 3b\n"					\<br />-		     ".previous\n"					\<br />-		     _ASM_EXTABLE_UA(1b, 4b)				\<br />-		     _ASM_EXTABLE_UA(2b, 4b)				\<br />-		     : "=r" (err)					\<br />-		     : "A" (x), "r" (addr), "i" (errret), "0" (err))<br />+#define __put_user_goto_u64(x, addr, label)			\<br />+	asm volatile("\n"					\<br />+		     "1:	movl %%eax,0(%2)\n"		\<br />+		     "2:	movl %%edx,4(%2)\n"		\<br />+		     _ASM_EXTABLE_UA(1b, %2l)			\<br />+		     _ASM_EXTABLE_UA(2b, %2l)			\<br />+		     : : "A" (x), "r" (addr)			\<br />+		     : : label)<br /> <br /> #define __put_user_asm_ex_u64(x, addr)					\<br /> 	asm volatile("\n"						\<br />&#64;&#64; -216,8 +214,8 &#64;&#64; __typeof__(__builtin_choose_expr(sizeof(x) &gt; sizeof(0UL), 0ULL, 0UL))<br /> 	asm volatile("call __put_user_8" : "=a" (__ret_pu)	\<br /> 		     : "A" ((typeof(*(ptr)))(x)), "c" (ptr) : "ebx")<br /> #else<br />-#define __put_user_asm_u64(x, ptr, retval, errret) \<br />-	__put_user_asm(x, ptr, retval, "q", "", "er", errret)<br />+#define __put_user_goto_u64(x, ptr, label) \<br />+	__put_user_goto(x, ptr, "q", "", "er", label)<br /> #define __put_user_asm_ex_u64(x, addr)	\<br /> 	__put_user_asm_ex(x, addr, "q", "", "er")<br /> #define __put_user_x8(x, ptr, __ret_pu) __put_user_x(8, x, ptr, __ret_pu)<br />&#64;&#64; -278,23 +276,21 &#64;&#64; extern void __put_user_8(void);<br /> 	__builtin_expect(__ret_pu, 0);				\<br /> })<br /> <br />-#define __put_user_size(x, ptr, size, retval, errret)			\<br />+#define __put_user_size(x, ptr, size, label)				\<br /> do {									\<br />-	retval = 0;							\<br /> 	__chk_user_ptr(ptr);						\<br /> 	switch (size) {							\<br /> 	case 1:								\<br />-		__put_user_asm(x, ptr, retval, "b", "b", "iq", errret);	\<br />+		__put_user_goto(x, ptr, "b", "b", "iq", label);	\<br /> 		break;							\<br /> 	case 2:								\<br />-		__put_user_asm(x, ptr, retval, "w", "w", "ir", errret);	\<br />+		__put_user_goto(x, ptr, "w", "w", "ir", label);		\<br /> 		break;							\<br /> 	case 4:								\<br />-		__put_user_asm(x, ptr, retval, "l", "k", "ir", errret);	\<br />+		__put_user_goto(x, ptr, "l", "k", "ir", label);		\<br /> 		break;							\<br /> 	case 8:								\<br />-		__put_user_asm_u64((__typeof__(*ptr))(x), ptr, retval,	\<br />-				   errret);				\<br />+		__put_user_goto_u64((__typeof__(*ptr))(x), ptr, label);	\<br /> 		break;							\<br /> 	default:							\<br /> 		__put_user_bad();					\<br />&#64;&#64; -439,9 +435,12 &#64;&#64; do {									\<br /> <br /> #define __put_user_nocheck(x, ptr, size)			\<br /> ({								\<br />-	int __pu_err;						\<br />+	__label__ __pu_label;					\<br />+	int __pu_err = -EFAULT;					\<br /> 	__uaccess_begin();					\<br />-	__put_user_size((x), (ptr), (size), __pu_err, -EFAULT);	\<br />+	__put_user_size((x), (ptr), (size), __pu_label);	\<br />+	__pu_err = 0;						\<br />+__pu_label:							\<br /> 	__uaccess_end();					\<br /> 	__builtin_expect(__pu_err, 0);				\<br /> })<br />&#64;&#64; -466,17 +465,23 &#64;&#64; struct __large_struct { unsigned long buf[100]; };<br />  * we do not write to any memory gcc knows about, so there are no<br />  * aliasing issues.<br />  */<br />-#define __put_user_asm(x, addr, err, itype, rtype, ltype, errret)	\<br />-	asm volatile("\n"						\<br />-		     "1:	mov"itype" %"rtype"1,%2\n"		\<br />-		     "2:\n"						\<br />-		     ".section .fixup,\"ax\"\n"				\<br />-		     "3:	mov %3,%0\n"				\<br />-		     "	jmp 2b\n"					\<br />-		     ".previous\n"					\<br />-		     _ASM_EXTABLE_UA(1b, 3b)				\<br />-		     : "=r"(err)					\<br />-		     : ltype(x), "m" (__m(addr)), "i" (errret), "0" (err))<br />+#define __put_user_goto(x, addr, itype, rtype, ltype, label)	\<br />+	asm volatile goto("\n"						\<br />+		"1:	mov"itype" %"rtype"0,%1\n"			\<br />+		_ASM_EXTABLE_UA(1b, %l2)					\<br />+		: : ltype(x), "m" (__m(addr))				\<br />+		: : label)<br />+<br />+#define __put_user_failed(x, addr, itype, rtype, ltype, errret)		\<br />+	({	__label__ __puflab;					\<br />+		int __pufret = errret;					\<br />+		__put_user_goto(x,addr,itype,rtype,ltype,__puflab);	\<br />+		__pufret = 0;						\<br />+	__puflab: __pufret; })<br />+<br />+#define __put_user_asm(x, addr, retval, itype, rtype, ltype, errret)	do {	\<br />+	retval = __put_user_failed(x, addr, itype, rtype, ltype, errret);	\<br />+} while (0)<br /> <br /> #define __put_user_asm_ex(x, addr, itype, rtype, ltype)			\<br /> 	asm volatile("1:	mov"itype" %"rtype"0,%1\n"		\<br />&#64;&#64; -687,12 +692,6 &#64;&#64; extern struct movsl_mask {<br /> <br /> #define ARCH_HAS_NOCACHE_UACCESS 1<br /> <br />-#ifdef CONFIG_X86_32<br />-# include &lt;asm/uaccess_32.h&gt;<br />-#else<br />-# include &lt;asm/uaccess_64.h&gt;<br />-#endif<br />-<br /> /*<br />  * We rely on the nested NMI work to allow atomic faults from the NMI path; the<br />  * nested NMI paths are careful to preserve CR2.<br />&#64;&#64; -711,13 +710,8 &#64;&#64; extern struct movsl_mask {<br /> #define user_access_begin()	__uaccess_begin()<br /> #define user_access_end()	__uaccess_end()<br /> <br />-#define unsafe_put_user(x, ptr, err_label)					\<br />-do {										\<br />-	int __pu_err;								\<br />-	__typeof__(*(ptr)) __pu_val = (x);					\<br />-	__put_user_size(__pu_val, (ptr), sizeof(*(ptr)), __pu_err, -EFAULT);	\<br />-	if (unlikely(__pu_err)) goto err_label;					\<br />-} while (0)<br />+#define unsafe_put_user(x, ptr, label)	\<br />+	__put_user_size((x), (ptr), sizeof(*(ptr)), label)<br /> <br /> #define unsafe_get_user(x, ptr, err_label)					\<br /> do {										\<br />&#64;&#64; -728,5 +722,11 &#64;&#64; do {										\<br /> 	if (unlikely(__gu_err)) goto err_label;					\<br /> } while (0)<br /> <br />+#ifdef CONFIG_X86_32<br />+# include &lt;asm/uaccess_32.h&gt;<br />+#else<br />+# include &lt;asm/uaccess_64.h&gt;<br />+#endif<br />+<br /> #endif /* _ASM_X86_UACCESS_H */<br /> <br />diff --git a/arch/x86/include/asm/uaccess_64.h b/arch/x86/include/asm/uaccess_64.h<br />index a9d637bc301d..8dd85d2fce28 100644<br />--- a/arch/x86/include/asm/uaccess_64.h<br />+++ b/arch/x86/include/asm/uaccess_64.h<br />&#64;&#64; -65,117 +65,104 &#64;&#64; copy_to_user_mcsafe(void *to, const void *from, unsigned len)<br /> static __always_inline __must_check unsigned long<br /> raw_copy_from_user(void *dst, const void __user *src, unsigned long size)<br /> {<br />-	int ret = 0;<br />-<br />-	if (!__builtin_constant_p(size))<br />-		return copy_user_generic(dst, (__force void *)src, size);<br />-	switch (size) {<br />-	case 1:<br />-		__uaccess_begin_nospec();<br />-		__get_user_asm_nozero(*(u8 *)dst, (u8 __user *)src,<br />-			      ret, "b", "b", "=q", 1);<br />-		__uaccess_end();<br />-		return ret;<br />-	case 2:<br />-		__uaccess_begin_nospec();<br />-		__get_user_asm_nozero(*(u16 *)dst, (u16 __user *)src,<br />-			      ret, "w", "w", "=r", 2);<br />-		__uaccess_end();<br />-		return ret;<br />-	case 4:<br />-		__uaccess_begin_nospec();<br />-		__get_user_asm_nozero(*(u32 *)dst, (u32 __user *)src,<br />-			      ret, "l", "k", "=r", 4);<br />-		__uaccess_end();<br />-		return ret;<br />-	case 8:<br />-		__uaccess_begin_nospec();<br />-		__get_user_asm_nozero(*(u64 *)dst, (u64 __user *)src,<br />-			      ret, "q", "", "=r", 8);<br />-		__uaccess_end();<br />-		return ret;<br />-	case 10:<br />-		__uaccess_begin_nospec();<br />-		__get_user_asm_nozero(*(u64 *)dst, (u64 __user *)src,<br />-			       ret, "q", "", "=r", 10);<br />-		if (likely(!ret))<br />-			__get_user_asm_nozero(*(u16 *)(8 + (char *)dst),<br />-				       (u16 __user *)(8 + (char __user *)src),<br />-				       ret, "w", "w", "=r", 2);<br />-		__uaccess_end();<br />-		return ret;<br />-	case 16:<br />-		__uaccess_begin_nospec();<br />-		__get_user_asm_nozero(*(u64 *)dst, (u64 __user *)src,<br />-			       ret, "q", "", "=r", 16);<br />-		if (likely(!ret))<br />-			__get_user_asm_nozero(*(u64 *)(8 + (char *)dst),<br />-				       (u64 __user *)(8 + (char __user *)src),<br />-				       ret, "q", "", "=r", 8);<br />-		__uaccess_end();<br />-		return ret;<br />-	default:<br />+	if (!__builtin_constant_p(size) || size &gt; 128)<br /> 		return copy_user_generic(dst, (__force void *)src, size);<br />+<br />+	/* Small constant size: unroll */<br />+	user_access_begin();<br />+	while (size &gt;= sizeof(unsigned long)) {<br />+		unsigned long val;<br />+		unsafe_get_user(val, (const unsigned long __user *) src, out);<br />+		*(unsigned long *)dst = val;<br />+		size -= sizeof(unsigned long);<br />+		src += sizeof(unsigned long);<br />+		dst += sizeof(unsigned long);<br /> 	}<br />+<br />+	while (size &gt;= sizeof(unsigned int)) {<br />+		unsigned int val;<br />+		unsafe_get_user(val, (const unsigned int __user *) src, out);<br />+		*(unsigned int *)dst = val;<br />+		size -= sizeof(unsigned int);<br />+		src += sizeof(unsigned int);<br />+		dst += sizeof(unsigned int);<br />+	}<br />+<br />+	while (size &gt;= sizeof(unsigned short)) {<br />+		unsigned short val;<br />+		unsafe_get_user(val, (const unsigned short __user *) src, out);<br />+		*(unsigned short *)dst = val;<br />+		size -= sizeof(unsigned short);<br />+		src += sizeof(unsigned short);<br />+		dst += sizeof(unsigned short);<br />+	}<br />+<br />+	while (size &gt;= sizeof(unsigned char)) {<br />+		unsigned char val;<br />+		unsafe_get_user(val, (const unsigned char __user *) src, out);<br />+		*(unsigned char *)dst = val;<br />+		size -= sizeof(unsigned char);<br />+		src += sizeof(unsigned char);<br />+		dst += sizeof(unsigned char);<br />+	}<br />+	user_access_end();<br />+	return 0;<br />+<br />+out:<br />+	user_access_end();<br />+	return size;<br /> }<br /> <br /> static __always_inline __must_check unsigned long<br /> raw_copy_to_user(void __user *dst, const void *src, unsigned long size)<br /> {<br />-	int ret = 0;<br />-<br />-	if (!__builtin_constant_p(size))<br />-		return copy_user_generic((__force void *)dst, src, size);<br />-	switch (size) {<br />-	case 1:<br />-		__uaccess_begin();<br />-		__put_user_asm(*(u8 *)src, (u8 __user *)dst,<br />-			      ret, "b", "b", "iq", 1);<br />-		__uaccess_end();<br />-		return ret;<br />-	case 2:<br />-		__uaccess_begin();<br />-		__put_user_asm(*(u16 *)src, (u16 __user *)dst,<br />-			      ret, "w", "w", "ir", 2);<br />-		__uaccess_end();<br />-		return ret;<br />-	case 4:<br />-		__uaccess_begin();<br />-		__put_user_asm(*(u32 *)src, (u32 __user *)dst,<br />-			      ret, "l", "k", "ir", 4);<br />-		__uaccess_end();<br />-		return ret;<br />-	case 8:<br />-		__uaccess_begin();<br />-		__put_user_asm(*(u64 *)src, (u64 __user *)dst,<br />-			      ret, "q", "", "er", 8);<br />-		__uaccess_end();<br />-		return ret;<br />-	case 10:<br />-		__uaccess_begin();<br />-		__put_user_asm(*(u64 *)src, (u64 __user *)dst,<br />-			       ret, "q", "", "er", 10);<br />-		if (likely(!ret)) {<br />-			asm("":::"memory");<br />-			__put_user_asm(4[(u16 *)src], 4 + (u16 __user *)dst,<br />-				       ret, "w", "w", "ir", 2);<br />-		}<br />-		__uaccess_end();<br />-		return ret;<br />-	case 16:<br />-		__uaccess_begin();<br />-		__put_user_asm(*(u64 *)src, (u64 __user *)dst,<br />-			       ret, "q", "", "er", 16);<br />-		if (likely(!ret)) {<br />-			asm("":::"memory");<br />-			__put_user_asm(1[(u64 *)src], 1 + (u64 __user *)dst,<br />-				       ret, "q", "", "er", 8);<br />-		}<br />-		__uaccess_end();<br />-		return ret;<br />-	default:<br />+	if (!__builtin_constant_p(size) || size &gt; 128)<br /> 		return copy_user_generic((__force void *)dst, src, size);<br />+<br />+	/* Small constant size: unroll */<br />+	user_access_begin();<br />+	while (size &gt;= sizeof(unsigned long)) {<br />+		unsigned long val;<br />+		val = *(const unsigned long *)src;<br />+		unsafe_put_user(val, (unsigned long __user *) dst, out);<br />+		size -= sizeof(unsigned long);<br />+		src += sizeof(unsigned long);<br />+		dst += sizeof(unsigned long);<br />+	}<br />+<br />+	while (size &gt;= sizeof(unsigned int)) {<br />+		unsigned int val;<br />+		val = *(const unsigned int *)src;<br />+		unsafe_put_user(val, (unsigned int __user *) dst, out);<br />+		size -= sizeof(unsigned int);<br />+		src += sizeof(unsigned int);<br />+		dst += sizeof(unsigned int);<br />+	}<br />+<br />+	while (size &gt;= sizeof(unsigned short)) {<br />+		unsigned short val;<br />+		val = *(const unsigned short *)src;<br />+		unsafe_put_user(val, (unsigned short __user *) dst, out);<br />+		size -= sizeof(unsigned short);<br />+		src += sizeof(unsigned short);<br />+		dst += sizeof(unsigned short);<br />+	}<br />+<br />+	while (size &gt;= sizeof(unsigned char)) {<br />+		unsigned char val;<br />+		val = *(const unsigned char *)src;<br />+		unsafe_put_user(val, (unsigned char __user *) dst, out);<br />+		size -= sizeof(unsigned char);<br />+		src += sizeof(unsigned char);<br />+		dst += sizeof(unsigned char);<br /> 	}<br />+<br />+	user_access_end();<br />+	return 0;<br />+<br />+out:<br />+	user_access_end();<br />+	return size;<br /> }<br /> <br /> static __always_inline __must_check<br />diff --git a/fs/readdir.c b/fs/readdir.c<br />index d97f548e6323..f1e159e17125 100644<br />--- a/fs/readdir.c<br />+++ b/fs/readdir.c<br />&#64;&#64; -185,25 +185,27 &#64;&#64; static int filldir(struct dir_context *ctx, const char *name, int namlen,<br /> 	if (dirent) {<br /> 		if (signal_pending(current))<br /> 			return -EINTR;<br />-		if (__put_user(offset, &amp;dirent-&gt;d_off))<br />-			goto efault;<br /> 	}<br />+<br />+	user_access_begin();<br />+	if (dirent)<br />+		unsafe_put_user(offset, &amp;dirent-&gt;d_off, efault_end);<br /> 	dirent = buf-&gt;current_dir;<br />-	if (__put_user(d_ino, &amp;dirent-&gt;d_ino))<br />-		goto efault;<br />-	if (__put_user(reclen, &amp;dirent-&gt;d_reclen))<br />-		goto efault;<br />+	unsafe_put_user(d_ino, &amp;dirent-&gt;d_ino, efault_end);<br />+	unsafe_put_user(reclen, &amp;dirent-&gt;d_reclen, efault_end);<br />+	unsafe_put_user(0, dirent-&gt;d_name + namlen, efault_end);<br />+	unsafe_put_user(d_type, (char __user *) dirent + reclen - 1, efault_end);<br />+	user_access_end();<br />+<br /> 	if (copy_to_user(dirent-&gt;d_name, name, namlen))<br /> 		goto efault;<br />-	if (__put_user(0, dirent-&gt;d_name + namlen))<br />-		goto efault;<br />-	if (__put_user(d_type, (char __user *) dirent + reclen - 1))<br />-		goto efault;<br /> 	buf-&gt;previous = dirent;<br /> 	dirent = (void __user *)dirent + reclen;<br /> 	buf-&gt;current_dir = dirent;<br /> 	buf-&gt;count -= reclen;<br /> 	return 0;<br />+efault_end:<br />+	user_access_end();<br /> efault:<br /> 	buf-&gt;error = -EFAULT;<br /> 	return -EFAULT;</pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
