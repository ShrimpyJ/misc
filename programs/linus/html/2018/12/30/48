    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2018/12/26/541">First message in thread</a></li><li><a href="/lkml/2018/12/26/541">Xie XiuQi</a><ul><li><a href="/lkml/2018/12/27/59">Vincent Guittot</a><ul><li><a href="/lkml/2018/12/27/79">Vincent Guittot</a><ul><li><a href="/lkml/2018/12/27/80">Vincent Guittot</a><ul><li><a href="/lkml/2018/12/27/144">Sargun Dhillon</a></li></ul></li></ul></li><li><a href="/lkml/2018/12/30/42">Ingo Molnar</a><ul><li><a href="/lkml/2018/12/30/43">Ingo Molnar</a></li><li><a href="/lkml/2018/12/30/45">Vincent Guittot</a><ul><li><a href="/lkml/2018/12/30/47">Ingo Molnar</a></li></ul></li></ul></li></ul></li><li class="origin"><a href="">tip-bot for Linus Torvalds</a></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2018/12/30/48/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><script type="text/javascript" src="//pagead2.googlesyndication.com/pagead/show_ads.js"></script><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Sun, 30 Dec 2018 05:00:58 -0800</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">tip-bot for Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">[tip:sched/urgent] sched/fair: Fix infinite loop in update_blocked_averages() by reverting a9e7f6544b9c</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">Commit-ID:  c40f7d74c741a907cfaeb73a7697081881c497d0<br />Gitweb:     <a href="https://git.kernel.org/tip/c40f7d74c741a907cfaeb73a7697081881c497d0">https://git.kernel.org/tip/c40f7d74c741a907cfaeb73a7697081881c497d0</a><br />Author:     Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />AuthorDate: Thu, 27 Dec 2018 13:46:17 -0800<br />Committer:  Ingo Molnar &lt;mingo&#64;kernel.org&gt;<br />CommitDate: Sun, 30 Dec 2018 13:54:31 +0100<br /><br />sched/fair: Fix infinite loop in update_blocked_averages() by reverting a9e7f6544b9c<br /><br />Zhipeng Xie, Xie XiuQi and Sargun Dhillon reported lockups in the<br />scheduler under high loads, starting at around the v4.18 time frame,<br />and Zhipeng Xie tracked it down to bugs in the rq-&gt;leaf_cfs_rq_list<br />manipulation.<br /><br />Do a (manual) revert of:<br /><br />  a9e7f6544b9c ("sched/fair: Fix O(nr_cgroups) in load balance path")<br /><br />It turns out that the list_del_leaf_cfs_rq() introduced by this commit<br />is a surprising property that was not considered in followup commits<br />such as:<br /><br />  9c2791f936ef ("sched/fair: Fix hierarchical order in rq-&gt;leaf_cfs_rq_list")<br /><br />As Vincent Guittot explains:<br /><br /> "I think that there is a bigger problem with commit a9e7f6544b9c and<br />  cfs_rq throttling:<br /><br />  Let take the example of the following topology TG2 --&gt; TG1 --&gt; root:<br /><br />   1) The 1st time a task is enqueued, we will add TG2 cfs_rq then TG1<br />      cfs_rq to leaf_cfs_rq_list and we are sure to do the whole branch in<br />      one path because it has never been used and can't be throttled so<br />      tmp_alone_branch will point to leaf_cfs_rq_list at the end.<br /><br />   2) Then TG1 is throttled<br /><br />   3) and we add TG3 as a new child of TG1.<br /><br />   4) The 1st enqueue of a task on TG3 will add TG3 cfs_rq just before TG1<br />      cfs_rq and tmp_alone_branch will stay  on rq-&gt;leaf_cfs_rq_list.<br /><br />  With commit a9e7f6544b9c, we can del a cfs_rq from rq-&gt;leaf_cfs_rq_list.<br />  So if the load of TG1 cfs_rq becomes NULL before step 2) above, TG1<br />  cfs_rq is removed from the list.<br />  Then at step 4), TG3 cfs_rq is added at the beginning of rq-&gt;leaf_cfs_rq_list<br />  but tmp_alone_branch still points to TG3 cfs_rq because its throttled<br />  parent can't be enqueued when the lock is released.<br />  tmp_alone_branch doesn't point to rq-&gt;leaf_cfs_rq_list whereas it should.<br /><br />  So if TG3 cfs_rq is removed or destroyed before tmp_alone_branch<br />  points on another TG cfs_rq, the next TG cfs_rq that will be added,<br />  will be linked outside rq-&gt;leaf_cfs_rq_list - which is bad.<br /><br />  In addition, we can break the ordering of the cfs_rq in<br />  rq-&gt;leaf_cfs_rq_list but this ordering is used to update and<br />  propagate the update from leaf down to root."<br /><br />Instead of trying to work through all these cases and trying to reproduce<br />the very high loads that produced the lockup to begin with, simplify<br />the code temporarily by reverting a9e7f6544b9c - which change was clearly<br />not thought through completely.<br /><br />This (hopefully) gives us a kernel that doesn't lock up so people<br />can continue to enjoy their holidays without worrying about regressions. ;-)<br /><br />[ mingo: Wrote changelog, fixed weird spelling in code comment while at it. ]<br /><br />Analyzed-by: Xie XiuQi &lt;xiexiuqi&#64;huawei.com&gt;<br />Analyzed-by: Vincent Guittot &lt;vincent.guittot&#64;linaro.org&gt;<br />Reported-by: Zhipeng Xie &lt;xiezhipeng1&#64;huawei.com&gt;<br />Reported-by: Sargun Dhillon &lt;sargun&#64;sargun.me&gt;<br />Reported-by: Xie XiuQi &lt;xiexiuqi&#64;huawei.com&gt;<br />Tested-by: Zhipeng Xie &lt;xiezhipeng1&#64;huawei.com&gt;<br />Tested-by: Sargun Dhillon &lt;sargun&#64;sargun.me&gt;<br />Signed-off-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />Acked-by: Vincent Guittot &lt;vincent.guittot&#64;linaro.org&gt;<br />Cc: &lt;stable&#64;vger.kernel.org&gt; # v4.13+<br />Cc: Bin Li &lt;huawei.libin&#64;huawei.com&gt;<br />Cc: Mike Galbraith &lt;efault&#64;gmx.de&gt;<br />Cc: Peter Zijlstra &lt;peterz&#64;infradead.org&gt;<br />Cc: Tejun Heo &lt;tj&#64;kernel.org&gt;<br />Cc: Thomas Gleixner &lt;tglx&#64;linutronix.de&gt;<br />Fixes: a9e7f6544b9c ("sched/fair: Fix O(nr_cgroups) in load balance path")<br />Link: <a href="https://lkml.kernel.org/r/1545879866-27809-1-git-send-email-xiexiuqi&#64;huawei.com">http://lkml.kernel.org/r/1545879866-27809-1-git-send-email-xiexiuqi&#64;huawei.com</a><br />Signed-off-by: Ingo Molnar &lt;mingo&#64;kernel.org&gt;<br />---<br /> kernel/sched/fair.c | 43 +++++++++----------------------------------<br /> 1 file changed, 9 insertions(+), 34 deletions(-)<br /><br />diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c<br />index d1907506318a..6483834f1278 100644<br />--- a/kernel/sched/fair.c<br />+++ b/kernel/sched/fair.c<br />&#64;&#64; -352,10 +352,9 &#64;&#64; static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)<br /> 	}<br /> }<br /> <br />-/* Iterate thr' all leaf cfs_rq's on a runqueue */<br />-#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)			\<br />-	list_for_each_entry_safe(cfs_rq, pos, &amp;rq-&gt;leaf_cfs_rq_list,	\<br />-				 leaf_cfs_rq_list)<br />+/* Iterate through all leaf cfs_rq's on a runqueue: */<br />+#define for_each_leaf_cfs_rq(rq, cfs_rq) \<br />+	list_for_each_entry_rcu(cfs_rq, &amp;rq-&gt;leaf_cfs_rq_list, leaf_cfs_rq_list)<br /> <br /> /* Do the two (enqueued) entities belong to the same group ? */<br /> static inline struct cfs_rq *<br />&#64;&#64; -447,8 +446,8 &#64;&#64; static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)<br /> {<br /> }<br /> <br />-#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)	\<br />-		for (cfs_rq = &amp;rq-&gt;cfs, pos = NULL; cfs_rq; cfs_rq = pos)<br />+#define for_each_leaf_cfs_rq(rq, cfs_rq)	\<br />+		for (cfs_rq = &amp;rq-&gt;cfs; cfs_rq; cfs_rq = NULL)<br /> <br /> static inline struct sched_entity *parent_entity(struct sched_entity *se)<br /> {<br />&#64;&#64; -7647,27 +7646,10 &#64;&#64; static inline bool others_have_blocked(struct rq *rq)<br /> <br /> #ifdef CONFIG_FAIR_GROUP_SCHED<br /> <br />-static inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)<br />-{<br />-	if (cfs_rq-&gt;load.weight)<br />-		return false;<br />-<br />-	if (cfs_rq-&gt;avg.load_sum)<br />-		return false;<br />-<br />-	if (cfs_rq-&gt;avg.util_sum)<br />-		return false;<br />-<br />-	if (cfs_rq-&gt;avg.runnable_load_sum)<br />-		return false;<br />-<br />-	return true;<br />-}<br />-<br /> static void update_blocked_averages(int cpu)<br /> {<br /> 	struct rq *rq = cpu_rq(cpu);<br />-	struct cfs_rq *cfs_rq, *pos;<br />+	struct cfs_rq *cfs_rq;<br /> 	const struct sched_class *curr_class;<br /> 	struct rq_flags rf;<br /> 	bool done = true;<br />&#64;&#64; -7679,7 +7661,7 &#64;&#64; static void update_blocked_averages(int cpu)<br /> 	 * Iterates the task_group tree in a bottom up fashion, see<br /> 	 * list_add_leaf_cfs_rq() for details.<br /> 	 */<br />-	for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {<br />+	for_each_leaf_cfs_rq(rq, cfs_rq) {<br /> 		struct sched_entity *se;<br /> <br /> 		/* throttled entities do not contribute to load */<br />&#64;&#64; -7694,13 +7676,6 &#64;&#64; static void update_blocked_averages(int cpu)<br /> 		if (se &amp;&amp; !skip_blocked_update(se))<br /> 			update_load_avg(cfs_rq_of(se), se, 0);<br /> <br />-		/*<br />-		 * There can be a lot of idle CPU cgroups.  Don't let fully<br />-		 * decayed cfs_rqs linger on the list.<br />-		 */<br />-		if (cfs_rq_is_decayed(cfs_rq))<br />-			list_del_leaf_cfs_rq(cfs_rq);<br />-<br /> 		/* Don't need periodic decay once load/util_avg are null */<br /> 		if (cfs_rq_has_blocked(cfs_rq))<br /> 			done = false;<br />&#64;&#64; -10570,10 +10545,10 &#64;&#64; const struct sched_class fair_sched_class = {<br /> #ifdef CONFIG_SCHED_DEBUG<br /> void print_cfs_stats(struct seq_file *m, int cpu)<br /> {<br />-	struct cfs_rq *cfs_rq, *pos;<br />+	struct cfs_rq *cfs_rq;<br /> <br /> 	rcu_read_lock();<br />-	for_each_leaf_cfs_rq_safe(cpu_rq(cpu), cfs_rq, pos)<br />+	for_each_leaf_cfs_rq(cpu_rq(cpu), cfs_rq)<br /> 		print_cfs_rq(m, cpu, cfs_rq);<br /> 	rcu_read_unlock();<br /> }<br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
