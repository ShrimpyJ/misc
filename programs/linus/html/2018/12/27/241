    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2018/12/26/541">First message in thread</a></li><li><a href="/lkml/2018/12/27/178">Linus Torvalds</a><ul><li><a href="/lkml/2018/12/27/230">Sargun Dhillon</a><ul><li class="origin"><a href="">Linus Torvalds</a></li></ul></li><li><a href="/lkml/2018/12/27/305">Tejun Heo</a><ul><li><a href="/lkml/2018/12/27/310">Linus Torvalds</a><ul><li><a href="/lkml/2018/12/27/318">Tejun Heo</a><ul><li><a href="/lkml/2018/12/27/319">Tejun Heo</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2018/12/27/241/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Thu, 27 Dec 2018 13:46:17 -0800</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH] sched: fix infinity loop in update_blocked_averages</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Thu, Dec 27, 2018 at 1:09 PM Sargun Dhillon &lt;sargun&#64;sargun.me&gt; wrote:<br />&gt;<br />&gt; This appears to be broken since October on 4.18.5. We've only noticed<br />&gt; it recently with a workload which does ridiculously parallel compiles<br />&gt; in cgroups that are rapidly churned.<br /><br />Yeah, that's probably unusual enough that people will have missed it.<br /><br />Because it really looks like the bug has been there since 4.13, unless<br />I'm mis-reading things. Other things have changed there since, so<br />maybe I am.<br /><br />&gt; It's also an awkward bug to catch, because none of the lockup<br />&gt; detectors, were catching it in our environment. The only reason we<br />&gt; caught it was that it was blocking other cores, and those other cores<br />&gt; were missing IPIs, resulting in catastrophic failure.<br /><br />My gut feel is that we just need to revert that commit. It doesn't<br />revert clealy, but it doesn't look hard to do manually.<br /><br />Something like the attached?<br /><br />But we do need Tejun and PeterZ to take a look, since there might be<br />something subtle going on.<br /><br />Everybody is probably still on well-deserved vacations, so it might be<br />a while. But testing the attached patch is probably a good idea<br />regardless.<br /><br />                  Linus<br /> kernel/sched/fair.c | 41 ++++++++---------------------------------<br /> 1 file changed, 8 insertions(+), 33 deletions(-)<br /><br />diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c<br />index d1907506318a..01f3cb89d188 100644<br />--- a/kernel/sched/fair.c<br />+++ b/kernel/sched/fair.c<br />&#64;&#64; -353,9 +353,8 &#64;&#64; static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)<br /> }<br /> <br /> /* Iterate thr' all leaf cfs_rq's on a runqueue */<br />-#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)			\<br />-	list_for_each_entry_safe(cfs_rq, pos, &amp;rq-&gt;leaf_cfs_rq_list,	\<br />-				 leaf_cfs_rq_list)<br />+#define for_each_leaf_cfs_rq(rq, cfs_rq) \<br />+	list_for_each_entry_rcu(cfs_rq, &amp;rq-&gt;leaf_cfs_rq_list, leaf_cfs_rq_list)<br /> <br /> /* Do the two (enqueued) entities belong to the same group ? */<br /> static inline struct cfs_rq *<br />&#64;&#64; -447,8 +446,8 &#64;&#64; static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)<br /> {<br /> }<br /> <br />-#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)	\<br />-		for (cfs_rq = &amp;rq-&gt;cfs, pos = NULL; cfs_rq; cfs_rq = pos)<br />+#define for_each_leaf_cfs_rq(rq, cfs_rq)	\<br />+		for (cfs_rq = &amp;rq-&gt;cfs; cfs_rq; cfs_rq = NULL)<br /> <br /> static inline struct sched_entity *parent_entity(struct sched_entity *se)<br /> {<br />&#64;&#64; -7647,27 +7646,10 &#64;&#64; static inline bool others_have_blocked(struct rq *rq)<br /> <br /> #ifdef CONFIG_FAIR_GROUP_SCHED<br /> <br />-static inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)<br />-{<br />-	if (cfs_rq-&gt;load.weight)<br />-		return false;<br />-<br />-	if (cfs_rq-&gt;avg.load_sum)<br />-		return false;<br />-<br />-	if (cfs_rq-&gt;avg.util_sum)<br />-		return false;<br />-<br />-	if (cfs_rq-&gt;avg.runnable_load_sum)<br />-		return false;<br />-<br />-	return true;<br />-}<br />-<br /> static void update_blocked_averages(int cpu)<br /> {<br /> 	struct rq *rq = cpu_rq(cpu);<br />-	struct cfs_rq *cfs_rq, *pos;<br />+	struct cfs_rq *cfs_rq;<br /> 	const struct sched_class *curr_class;<br /> 	struct rq_flags rf;<br /> 	bool done = true;<br />&#64;&#64; -7679,7 +7661,7 &#64;&#64; static void update_blocked_averages(int cpu)<br /> 	 * Iterates the task_group tree in a bottom up fashion, see<br /> 	 * list_add_leaf_cfs_rq() for details.<br /> 	 */<br />-	for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {<br />+	for_each_leaf_cfs_rq(rq, cfs_rq) {<br /> 		struct sched_entity *se;<br /> <br /> 		/* throttled entities do not contribute to load */<br />&#64;&#64; -7694,13 +7676,6 &#64;&#64; static void update_blocked_averages(int cpu)<br /> 		if (se &amp;&amp; !skip_blocked_update(se))<br /> 			update_load_avg(cfs_rq_of(se), se, 0);<br /> <br />-		/*<br />-		 * There can be a lot of idle CPU cgroups.  Don't let fully<br />-		 * decayed cfs_rqs linger on the list.<br />-		 */<br />-		if (cfs_rq_is_decayed(cfs_rq))<br />-			list_del_leaf_cfs_rq(cfs_rq);<br />-<br /> 		/* Don't need periodic decay once load/util_avg are null */<br /> 		if (cfs_rq_has_blocked(cfs_rq))<br /> 			done = false;<br />&#64;&#64; -10570,10 +10545,10 &#64;&#64; const struct sched_class fair_sched_class = {<br /> #ifdef CONFIG_SCHED_DEBUG<br /> void print_cfs_stats(struct seq_file *m, int cpu)<br /> {<br />-	struct cfs_rq *cfs_rq, *pos;<br />+	struct cfs_rq *cfs_rq;<br /> <br /> 	rcu_read_lock();<br />-	for_each_leaf_cfs_rq_safe(cpu_rq(cpu), cfs_rq, pos)<br />+	for_each_leaf_cfs_rq(cpu_rq(cpu), cfs_rq)<br /> 		print_cfs_rq(m, cpu, cfs_rq);<br /> 	rcu_read_unlock();<br /> }</pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
