    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2017/8/14/1000">First message in thread</a></li><li><a href="/lkml/2017/8/14/1127">Linus Torvalds</a><ul><li><a href="/lkml/2017/8/14/1190">Andi Kleen</a><ul><li class="origin"><a href="/lkml/2017/8/14/1214">Linus Torvalds</a><ul><li><a href="/lkml/2017/8/14/1214">Andi Kleen</a><ul><li><a href="/lkml/2017/8/14/1225">Linus Torvalds</a></li></ul></li></ul></li></ul></li><li><a href="/lkml/2017/8/17/678">"Liang, Kan"</a><ul><li><a href="/lkml/2017/8/17/683">Linus Torvalds</a><ul><li><a href="/lkml/2017/8/17/874">"Liang, Kan"</a><ul><li><a href="/lkml/2017/8/17/882">Linus Torvalds</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Mon, 14 Aug 2017 19:52:16 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH 1/2] sched/wait: Break up long wake list walk</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Mon, Aug 14, 2017 at 7:27 PM, Andi Kleen &lt;ak&#64;linux.intel.com&gt; wrote:<br />&gt;<br />&gt; We could try it and it may even help in this case and it may<br />&gt; be a good idea in any case on such a system, but:<br />&gt;<br />&gt; - Even with a large hash table it might be that by chance all CPUs<br />&gt; will be queued up on the same page<br />&gt; - There are a lot of other wait queues in the kernel and they all<br />&gt; could run into a similar problem<br />&gt; - I suspect it's even possible to construct it from user space<br />&gt; as a kind of DoS attack<br /><br />Maybe. Which is why I didn't NAK the patch outright.<br /><br />But I don't think it's the solution for the scalability issue you guys<br />found. It's just a workaround, and it's likely a bad one at that.<br /><br />&gt; Now in one case (on a smaller system) we debugged we had<br />&gt;<br />&gt; - 4S system with 208 logical threads<br />&gt; - during the test the wait queue length was 3700 entries.<br />&gt; - the last CPUs queued had to wait roughly 0.8s<br />&gt;<br />&gt; This gives a budget of roughly 1us per wake up.<br /><br />I'm not at all convinced that follows.<br /><br />When bad scaling happens, you often end up hitting quadratic (or<br />worse) behavior. So if you are able to fix the scaling by some fixed<br />amount, it's possible that almost _all_ the problems just go away.<br /><br />The real issue is that "3700 entries" part. What was it that actually<br />triggered them? In particular, if it's just a hashing issue, and we<br />can trivially just make the hash table be bigger (256 entries is<br />*tiny*) then the whole thing goes away.<br /><br />Which is why I really want to hear what happens if you just change<br />PAGE_WAIT_TABLE_BITS to 16. The right fix would be to just make it<br />scale by memory, but before we even do that, let's just look at what<br />happens when you increase the size the stupid way.<br /><br />Maybe those 3700 entries will just shrink down to 14 entries because<br />the hash just works fine and 256 entries was just much much too small<br />when you have hundreds of thousands of threads or whatever<br /><br />But it is *also* possible that it's actually all waiting on the exact<br />same page, and there's some way to do a thundering herd on the page<br />lock bit, for example. But then it would be really good to hear what<br />it is that triggers that.<br /><br />The thing is, the reason we perform well on many loads in the kernel<br />is that I have *always* pushed back against bad workarounds.<br /><br />We do *not* do lock back-off in our locks, for example, because I told<br />people that lock contention gets fixed by not contending, not by<br />trying to act better when things have already become bad.<br /><br />This is the same issue. We don't "fix" things by papering over some<br />symptom. We try to fix the _actual_ underlying problem. Maybe there is<br />some caller that can simply be rewritten. Maybe we can do other tricks<br />than just make the wait tables bigger. But we should not say "3700<br />entries is ok, let's just make that sh*t be interruptible".<br /><br />That is what the patch does now, and that is why I dislike the patch.<br /><br />So I _am_ NAK'ing the patch if nobody is willing to even try alternatives.<br /><br />Because a band-aid is ok for "some theoretical worst-case behavior".<br /><br />But a band-aid is *not* ok for "we can't even be bothered to try to<br />figure out the right thing, so we're just adding this hack and leaving<br />it".<br /><br />             Linus<br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
