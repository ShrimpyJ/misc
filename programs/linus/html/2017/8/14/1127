    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2017/8/14/1000">First message in thread</a></li><li><a href="/lkml/2017/8/14/1000">Tim Chen</a><ul><li><a href="/lkml/2017/8/14/1001">Tim Chen</a></li><li class="origin"><a href="/lkml/2017/8/14/1190">Linus Torvalds</a><ul><li><a href="/lkml/2017/8/14/1190">Andi Kleen</a><ul><li><a href="/lkml/2017/8/14/1197">Linus Torvalds</a><ul><li><a href="/lkml/2017/8/14/1214">Andi Kleen</a></li></ul></li></ul></li><li><a href="/lkml/2017/8/17/678">"Liang, Kan"</a><ul><li><a href="/lkml/2017/8/17/683">Linus Torvalds</a><ul><li><a href="/lkml/2017/8/17/874">"Liang, Kan"</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Mon, 14 Aug 2017 18:48:06 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH 1/2] sched/wait: Break up long wake list walk</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Mon, Aug 14, 2017 at 5:52 PM, Tim Chen &lt;tim.c.chen&#64;linux.intel.com&gt; wrote:<br />&gt; We encountered workloads that have very long wake up list on large<br />&gt; systems. A waker takes a long time to traverse the entire wake list and<br />&gt; execute all the wake functions.<br />&gt;<br />&gt; We saw page wait list that are up to 3700+ entries long in tests of large<br />&gt; 4 and 8 socket systems.  It took 0.8 sec to traverse such list during<br />&gt; wake up.  Any other CPU that contends for the list spin lock will spin<br />&gt; for a long time.  As page wait list is shared by many pages so it could<br />&gt; get very long on systems with large memory.<br /><br />I really dislike this patch.<br /><br />The patch seems a band-aid for really horrible kernel behavior, rather<br />than fixing the underlying problem itself.<br /><br />Now, it may well be that we do end up needing this band-aid in the<br />end, so this isn't a NAK of the patch per se. But I'd *really* like to<br />see if we can fix the underlying cause for what you see somehow..<br /><br />In particular, if this is about the page wait table, maybe we can just<br />make the wait table bigger. IOW, are people actually waiting on the<br />*same* page, or are they mainly waiting on totally different pages,<br />just hashing to the same wait queue?<br /><br />Because right now that page wait table is a small fixed size, and the<br />only reason it's a small fixed size is that nobody reported any issues<br />with it - particularly since we now avoid the wait table entirely for<br />the common cases by having that "contention" bit.<br /><br />But it really is a *small* table. We literally have<br /><br />   #define PAGE_WAIT_TABLE_BITS 8<br /><br />so it's just 256 entries. We could easily it much bigger, if we are<br />actually seeing a lot of collissions.<br /><br />We *used* to have a very complex per-zone thing for bit-waitiqueues,<br />but that was because we got lots and lots of contention issues, and<br />everybody *always* touched the wait-queues whether they waited or not<br />(so being per-zone was a big deal)<br /><br />We got rid of all that per-zone complexity when the normal case didn't<br />hit in the page wait queues at all, but we may have over-done the<br />simplification a bit since nobody showed any issue.<br /><br />In particular, we used to size the per-zone thing by amount of memory.<br />We could easily re-introduce that for the new simpler page queues.<br /><br />The page_waitiqueue() is a simple helper function inside mm/filemap.c,<br />and thanks to the per-page "do we have actual waiters" bit that we<br />have now, we can actually afford to make it bigger and more complex<br />now if we want to.<br /><br />What happens to your load if you just make that table bigger? You can<br />literally test by just changing the constant from 8 to 16 or<br />something, making us use twice as many bits for hashing. A "real"<br />patch would size it by amount of memory, but just for testing the<br />contention on your load, you can do the hacky one-liner.<br /><br />             Linus<br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
