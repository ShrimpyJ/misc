    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2017/8/25/542">First message in thread</a></li><li><a href="/lkml/2017/8/25/676">Linus Torvalds</a><ul><li><a href="/lkml/2017/8/25/779">Tim Chen</a><ul><li class="origin"><a href="/lkml/2017/8/25/793">Linus Torvalds</a><ul><li><a href="/lkml/2017/8/25/793">Linus Torvalds</a><ul><li><a href="/lkml/2017/8/25/804">Linus Torvalds</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patches in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2017/8/25/789/1">Get diff 1</a></li><li><a href="/lkml/diff/2017/8/25/789/2">Get diff 2</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Fri, 25 Aug 2017 15:51:17 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH 2/2 v2] sched/wait: Introduce lock breaker in wake_up_page_bit</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Fri, Aug 25, 2017 at 3:19 PM, Tim Chen &lt;tim.c.chen&#64;linux.intel.com&gt; wrote:<br />&gt;<br />&gt; Also I think patch 1 is still a good idea for a fail safe mechanism<br />&gt; in case there are other long wait list.<br /><br />Yeah, I don't hate patch #1.<br /><br />But that other patch is just nasty.<br /><br />&gt; That said, I do think your suggested approach is cleaner.  However, it<br />&gt; is a much more substantial change.  Let me take a look and see if I<br />&gt; have any issues implementing it.<br /><br />Actually, I tried it myself.<br /><br />It was painful. But I actually have a TOTALLY UNTESTED set of two<br />patches that implements the idea.<br /><br />And by "implements the idea" I mean "it kind of compiles, and it kind<br />of looks like it might work".<br /><br />But by "kind of compiles" I mean that I didn't implement the nasty<br />add_page_wait_queue() thing that the cachefiles interface wants.<br />Honestly, I think the sanest way to do that is to just have a hashed<br />wait queue *just* for cachefiles.<br /><br />And by "kind of looks like it might work" I really mean just that.<br />It's entirely untested. It's more of a "let's take that description of<br />mine and turn it into code". I really have not tested this AT ALL.<br /><br />And it's subtle enough that I suspect it really is majorly buggy. It<br />uses the locking hash list code (hlist_bl_node) to keep the hash table<br />fairly small and hide the lock in the hash table itself.<br /><br />And then it plays horrible games with linked lists. Yeah, that may<br />have been a mistake, but I thought I should try to avoid the doubly<br />linked lists in that "struct page_wait_struct" because it's allocated<br />on the stack, and each list_head is 16 bytes on 64-bit architectures.<br /><br />But that "let's save 24 bytes in the structure" made it much nastier<br />to remove entries, so it was probably a bad trade-off.<br /><br />But I'm attaching the two patches because I have no shame. If somebody<br />is willing to look at my completely untested crap code.<br /><br />I *really* want to emphasize that "untested crap".<br /><br />This is meant to be example code of the *concept* rather than anything<br />that actually works.<br /><br />So take it as that: example pseudo-code that happens to pass a<br />compiler, but is meant as a RFD rather than actually working.<br /><br />The first patch just moves code around because I wanted to experiment<br />with the new code in a new file. That first patch is probably fine. It<br />shouldn't change any code, just move it.<br /><br />The second patch is the "broken patch to illustrate the idea".<br /><br />                    Linus<br />From b0bec1de8a7dba69b223dd30dd2a734401f335aa Mon Sep 17 00:00:00 2001<br />From: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />Date: Fri, 25 Aug 2017 13:25:43 -0700<br />Subject: [PATCH 1/2] Split page bit waiting functions into their own file<br /><br />It turns out that the page-bit waiting logic is very special indeed, and<br />will get even more so.  Split it up into its own file to make this clear.<br /><br />I'll rewrite the logic of the page wait queue completely, but this is<br />pure prep-work with no actual code changes, just code movement.<br /><br />Signed-off-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />---<br /> mm/Makefile        |   2 +-<br /> mm/filemap.c       | 260 --------------------------------------------------<br /> mm/page_wait_bit.c | 274 +++++++++++++++++++++++++++++++++++++++++++++++++++++<br /> 3 files changed, 275 insertions(+), 261 deletions(-)<br /> create mode 100644 mm/page_wait_bit.c<br /><br />diff --git a/mm/Makefile b/mm/Makefile<br />index 411bd24d4a7c..8bc0194207a3 100644<br />--- a/mm/Makefile<br />+++ b/mm/Makefile<br />&#64;&#64; -32,7 +32,7 &#64;&#64; ifdef CONFIG_CROSS_MEMORY_ATTACH<br /> mmu-$(CONFIG_MMU)	+= process_vm_access.o<br /> endif<br /> <br />-obj-y			:= filemap.o mempool.o oom_kill.o \<br />+obj-y			:= filemap.o page_wait_bit.o mempool.o oom_kill.o \<br /> 			   maccess.o page_alloc.o page-writeback.o \<br /> 			   readahead.o swap.o truncate.o vmscan.o shmem.o \<br /> 			   util.o mmzone.o vmstat.o backing-dev.o \<br />diff --git a/mm/filemap.c b/mm/filemap.c<br />index a49702445ce0..f9b710e8f76e 100644<br />--- a/mm/filemap.c<br />+++ b/mm/filemap.c<br />&#64;&#64; -856,195 +856,6 &#64;&#64; struct page *__page_cache_alloc(gfp_t gfp)<br /> EXPORT_SYMBOL(__page_cache_alloc);<br /> #endif<br /> <br />-/*<br />- * In order to wait for pages to become available there must be<br />- * waitqueues associated with pages. By using a hash table of<br />- * waitqueues where the bucket discipline is to maintain all<br />- * waiters on the same queue and wake all when any of the pages<br />- * become available, and for the woken contexts to check to be<br />- * sure the appropriate page became available, this saves space<br />- * at a cost of "thundering herd" phenomena during rare hash<br />- * collisions.<br />- */<br />-#define PAGE_WAIT_TABLE_BITS 8<br />-#define PAGE_WAIT_TABLE_SIZE (1 &lt;&lt; PAGE_WAIT_TABLE_BITS)<br />-static wait_queue_head_t page_wait_table[PAGE_WAIT_TABLE_SIZE] __cacheline_aligned;<br />-<br />-static wait_queue_head_t *page_waitqueue(struct page *page)<br />-{<br />-	return &amp;page_wait_table[hash_ptr(page, PAGE_WAIT_TABLE_BITS)];<br />-}<br />-<br />-void __init pagecache_init(void)<br />-{<br />-	int i;<br />-<br />-	for (i = 0; i &lt; PAGE_WAIT_TABLE_SIZE; i++)<br />-		init_waitqueue_head(&amp;page_wait_table[i]);<br />-<br />-	page_writeback_init();<br />-}<br />-<br />-struct wait_page_key {<br />-	struct page *page;<br />-	int bit_nr;<br />-	int page_match;<br />-};<br />-<br />-struct wait_page_queue {<br />-	struct page *page;<br />-	int bit_nr;<br />-	wait_queue_entry_t wait;<br />-};<br />-<br />-static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)<br />-{<br />-	struct wait_page_key *key = arg;<br />-	struct wait_page_queue *wait_page<br />-		= container_of(wait, struct wait_page_queue, wait);<br />-<br />-	if (wait_page-&gt;page != key-&gt;page)<br />-	       return 0;<br />-	key-&gt;page_match = 1;<br />-<br />-	if (wait_page-&gt;bit_nr != key-&gt;bit_nr)<br />-		return 0;<br />-	if (test_bit(key-&gt;bit_nr, &amp;key-&gt;page-&gt;flags))<br />-		return 0;<br />-<br />-	return autoremove_wake_function(wait, mode, sync, key);<br />-}<br />-<br />-static void wake_up_page_bit(struct page *page, int bit_nr)<br />-{<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	struct wait_page_key key;<br />-	unsigned long flags;<br />-<br />-	key.page = page;<br />-	key.bit_nr = bit_nr;<br />-	key.page_match = 0;<br />-<br />-	spin_lock_irqsave(&amp;q-&gt;lock, flags);<br />-	__wake_up_locked_key(q, TASK_NORMAL, &amp;key);<br />-	/*<br />-	 * It is possible for other pages to have collided on the waitqueue<br />-	 * hash, so in that case check for a page match. That prevents a long-<br />-	 * term waiter<br />-	 *<br />-	 * It is still possible to miss a case here, when we woke page waiters<br />-	 * and removed them from the waitqueue, but there are still other<br />-	 * page waiters.<br />-	 */<br />-	if (!waitqueue_active(q) || !key.page_match) {<br />-		ClearPageWaiters(page);<br />-		/*<br />-		 * It's possible to miss clearing Waiters here, when we woke<br />-		 * our page waiters, but the hashed waitqueue has waiters for<br />-		 * other pages on it.<br />-		 *<br />-		 * That's okay, it's a rare case. The next waker will clear it.<br />-		 */<br />-	}<br />-	spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<br />-}<br />-<br />-static void wake_up_page(struct page *page, int bit)<br />-{<br />-	if (!PageWaiters(page))<br />-		return;<br />-	wake_up_page_bit(page, bit);<br />-}<br />-<br />-static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br />-		struct page *page, int bit_nr, int state, bool lock)<br />-{<br />-	struct wait_page_queue wait_page;<br />-	wait_queue_entry_t *wait = &amp;wait_page.wait;<br />-	int ret = 0;<br />-<br />-	init_wait(wait);<br />-	wait-&gt;func = wake_page_function;<br />-	wait_page.page = page;<br />-	wait_page.bit_nr = bit_nr;<br />-<br />-	for (;;) {<br />-		spin_lock_irq(&amp;q-&gt;lock);<br />-<br />-		if (likely(list_empty(&amp;wait-&gt;entry))) {<br />-			if (lock)<br />-				__add_wait_queue_entry_tail_exclusive(q, wait);<br />-			else<br />-				__add_wait_queue(q, wait);<br />-			SetPageWaiters(page);<br />-		}<br />-<br />-		set_current_state(state);<br />-<br />-		spin_unlock_irq(&amp;q-&gt;lock);<br />-<br />-		if (likely(test_bit(bit_nr, &amp;page-&gt;flags))) {<br />-			io_schedule();<br />-			if (unlikely(signal_pending_state(state, current))) {<br />-				ret = -EINTR;<br />-				break;<br />-			}<br />-		}<br />-<br />-		if (lock) {<br />-			if (!test_and_set_bit_lock(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />-		} else {<br />-			if (!test_bit(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />-		}<br />-	}<br />-<br />-	finish_wait(q, wait);<br />-<br />-	/*<br />-	 * A signal could leave PageWaiters set. Clearing it here if<br />-	 * !waitqueue_active would be possible (by open-coding finish_wait),<br />-	 * but still fail to catch it in the case of wait hash collision. We<br />-	 * already can fail to clear wait hash collision cases, so don't<br />-	 * bother with signals either.<br />-	 */<br />-<br />-	return ret;<br />-}<br />-<br />-void wait_on_page_bit(struct page *page, int bit_nr)<br />-{<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	wait_on_page_bit_common(q, page, bit_nr, TASK_UNINTERRUPTIBLE, false);<br />-}<br />-EXPORT_SYMBOL(wait_on_page_bit);<br />-<br />-int wait_on_page_bit_killable(struct page *page, int bit_nr)<br />-{<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	return wait_on_page_bit_common(q, page, bit_nr, TASK_KILLABLE, false);<br />-}<br />-<br />-/**<br />- * add_page_wait_queue - Add an arbitrary waiter to a page's wait queue<br />- * &#64;page: Page defining the wait queue of interest<br />- * &#64;waiter: Waiter to add to the queue<br />- *<br />- * Add an arbitrary &#64;waiter to the wait queue for the nominated &#64;page.<br />- */<br />-void add_page_wait_queue(struct page *page, wait_queue_entry_t *waiter)<br />-{<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	unsigned long flags;<br />-<br />-	spin_lock_irqsave(&amp;q-&gt;lock, flags);<br />-	__add_wait_queue(q, waiter);<br />-	SetPageWaiters(page);<br />-	spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<br />-}<br />-EXPORT_SYMBOL_GPL(add_page_wait_queue);<br />-<br /> #ifndef clear_bit_unlock_is_negative_byte<br /> <br /> /*<br />&#64;&#64; -1068,57 +879,6 &#64;&#64; static inline bool clear_bit_unlock_is_negative_byte(long nr, volatile void *mem<br /> <br /> #endif<br /> <br />-/**<br />- * unlock_page - unlock a locked page<br />- * &#64;page: the page<br />- *<br />- * Unlocks the page and wakes up sleepers in ___wait_on_page_locked().<br />- * Also wakes sleepers in wait_on_page_writeback() because the wakeup<br />- * mechanism between PageLocked pages and PageWriteback pages is shared.<br />- * But that's OK - sleepers in wait_on_page_writeback() just go back to sleep.<br />- *<br />- * Note that this depends on PG_waiters being the sign bit in the byte<br />- * that contains PG_locked - thus the BUILD_BUG_ON(). That allows us to<br />- * clear the PG_locked bit and test PG_waiters at the same time fairly<br />- * portably (architectures that do LL/SC can test any bit, while x86 can<br />- * test the sign bit).<br />- */<br />-void unlock_page(struct page *page)<br />-{<br />-	BUILD_BUG_ON(PG_waiters != 7);<br />-	page = compound_head(page);<br />-	VM_BUG_ON_PAGE(!PageLocked(page), page);<br />-	if (clear_bit_unlock_is_negative_byte(PG_locked, &amp;page-&gt;flags))<br />-		wake_up_page_bit(page, PG_locked);<br />-}<br />-EXPORT_SYMBOL(unlock_page);<br />-<br />-/**<br />- * end_page_writeback - end writeback against a page<br />- * &#64;page: the page<br />- */<br />-void end_page_writeback(struct page *page)<br />-{<br />-	/*<br />-	 * TestClearPageReclaim could be used here but it is an atomic<br />-	 * operation and overkill in this particular case. Failing to<br />-	 * shuffle a page marked for immediate reclaim is too mild to<br />-	 * justify taking an atomic operation penalty at the end of<br />-	 * ever page writeback.<br />-	 */<br />-	if (PageReclaim(page)) {<br />-		ClearPageReclaim(page);<br />-		rotate_reclaimable_page(page);<br />-	}<br />-<br />-	if (!test_clear_page_writeback(page))<br />-		BUG();<br />-<br />-	smp_mb__after_atomic();<br />-	wake_up_page(page, PG_writeback);<br />-}<br />-EXPORT_SYMBOL(end_page_writeback);<br />-<br /> /*<br />  * After completing I/O on a page, call this routine to update the page<br />  * flags appropriately<br />&#64;&#64; -1147,26 +907,6 &#64;&#64; void page_endio(struct page *page, bool is_write, int err)<br /> }<br /> EXPORT_SYMBOL_GPL(page_endio);<br /> <br />-/**<br />- * __lock_page - get a lock on the page, assuming we need to sleep to get it<br />- * &#64;__page: the page to lock<br />- */<br />-void __lock_page(struct page *__page)<br />-{<br />-	struct page *page = compound_head(__page);<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	wait_on_page_bit_common(q, page, PG_locked, TASK_UNINTERRUPTIBLE, true);<br />-}<br />-EXPORT_SYMBOL(__lock_page);<br />-<br />-int __lock_page_killable(struct page *__page)<br />-{<br />-	struct page *page = compound_head(__page);<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	return wait_on_page_bit_common(q, page, PG_locked, TASK_KILLABLE, true);<br />-}<br />-EXPORT_SYMBOL_GPL(__lock_page_killable);<br />-<br /> /*<br />  * Return values:<br />  * 1 - page is locked; mmap_sem is still held.<br />diff --git a/mm/page_wait_bit.c b/mm/page_wait_bit.c<br />new file mode 100644<br />index 000000000000..7550b6d2715a<br />--- /dev/null<br />+++ b/mm/page_wait_bit.c<br />&#64;&#64; -0,0 +1,274 &#64;&#64;<br />+/*<br />+ *	linux/mm/page_wait_bit.c<br />+ *<br />+ * Copyright (C) 2017  Linus Torvalds<br />+ */<br />+<br />+#include &lt;linux/swap.h&gt;<br />+#include &lt;linux/pagemap.h&gt;<br />+#include &lt;linux/wait.h&gt;<br />+#include &lt;linux/export.h&gt;<br />+#include &lt;linux/sched/signal.h&gt;<br />+<br />+#include "internal.h"<br />+<br />+/*<br />+ * In order to wait for pages to become available there must be<br />+ * waitqueues associated with pages. By using a hash table of<br />+ * waitqueues where the bucket discipline is to maintain all<br />+ * waiters on the same queue and wake all when any of the pages<br />+ * become available, and for the woken contexts to check to be<br />+ * sure the appropriate page became available, this saves space<br />+ * at a cost of "thundering herd" phenomena during rare hash<br />+ * collisions.<br />+ */<br />+#define PAGE_WAIT_TABLE_BITS 8<br />+#define PAGE_WAIT_TABLE_SIZE (1 &lt;&lt; PAGE_WAIT_TABLE_BITS)<br />+static wait_queue_head_t page_wait_table[PAGE_WAIT_TABLE_SIZE] __cacheline_aligned;<br />+<br />+static wait_queue_head_t *page_waitqueue(struct page *page)<br />+{<br />+	return &amp;page_wait_table[hash_ptr(page, PAGE_WAIT_TABLE_BITS)];<br />+}<br />+<br />+void __init pagecache_init(void)<br />+{<br />+	int i;<br />+<br />+	for (i = 0; i &lt; PAGE_WAIT_TABLE_SIZE; i++)<br />+		init_waitqueue_head(&amp;page_wait_table[i]);<br />+<br />+	page_writeback_init();<br />+}<br />+<br />+struct wait_page_key {<br />+	struct page *page;<br />+	int bit_nr;<br />+	int page_match;<br />+};<br />+<br />+struct wait_page_queue {<br />+	struct page *page;<br />+	int bit_nr;<br />+	wait_queue_entry_t wait;<br />+};<br />+<br />+static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)<br />+{<br />+	struct wait_page_key *key = arg;<br />+	struct wait_page_queue *wait_page<br />+		= container_of(wait, struct wait_page_queue, wait);<br />+<br />+	if (wait_page-&gt;page != key-&gt;page)<br />+	       return 0;<br />+	key-&gt;page_match = 1;<br />+<br />+	if (wait_page-&gt;bit_nr != key-&gt;bit_nr)<br />+		return 0;<br />+	if (test_bit(key-&gt;bit_nr, &amp;key-&gt;page-&gt;flags))<br />+		return 0;<br />+<br />+	return autoremove_wake_function(wait, mode, sync, key);<br />+}<br />+<br />+static void wake_up_page_bit(struct page *page, int bit_nr)<br />+{<br />+	wait_queue_head_t *q = page_waitqueue(page);<br />+	struct wait_page_key key;<br />+	unsigned long flags;<br />+<br />+	key.page = page;<br />+	key.bit_nr = bit_nr;<br />+	key.page_match = 0;<br />+<br />+	spin_lock_irqsave(&amp;q-&gt;lock, flags);<br />+	__wake_up_locked_key(q, TASK_NORMAL, &amp;key);<br />+	/*<br />+	 * It is possible for other pages to have collided on the waitqueue<br />+	 * hash, so in that case check for a page match. That prevents a long-<br />+	 * term waiter<br />+	 *<br />+	 * It is still possible to miss a case here, when we woke page waiters<br />+	 * and removed them from the waitqueue, but there are still other<br />+	 * page waiters.<br />+	 */<br />+	if (!waitqueue_active(q) || !key.page_match) {<br />+		ClearPageWaiters(page);<br />+		/*<br />+		 * It's possible to miss clearing Waiters here, when we woke<br />+		 * our page waiters, but the hashed waitqueue has waiters for<br />+		 * other pages on it.<br />+		 *<br />+		 * That's okay, it's a rare case. The next waker will clear it.<br />+		 */<br />+	}<br />+	spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<br />+}<br />+<br />+static void wake_up_page(struct page *page, int bit)<br />+{<br />+	if (!PageWaiters(page))<br />+		return;<br />+	wake_up_page_bit(page, bit);<br />+}<br />+<br />+static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br />+		struct page *page, int bit_nr, int state, bool lock)<br />+{<br />+	struct wait_page_queue wait_page;<br />+	wait_queue_entry_t *wait = &amp;wait_page.wait;<br />+	int ret = 0;<br />+<br />+	init_wait(wait);<br />+	wait-&gt;func = wake_page_function;<br />+	wait_page.page = page;<br />+	wait_page.bit_nr = bit_nr;<br />+<br />+	for (;;) {<br />+		spin_lock_irq(&amp;q-&gt;lock);<br />+<br />+		if (likely(list_empty(&amp;wait-&gt;entry))) {<br />+			if (lock)<br />+				__add_wait_queue_entry_tail_exclusive(q, wait);<br />+			else<br />+				__add_wait_queue(q, wait);<br />+			SetPageWaiters(page);<br />+		}<br />+<br />+		set_current_state(state);<br />+<br />+		spin_unlock_irq(&amp;q-&gt;lock);<br />+<br />+		if (likely(test_bit(bit_nr, &amp;page-&gt;flags))) {<br />+			io_schedule();<br />+			if (unlikely(signal_pending_state(state, current))) {<br />+				ret = -EINTR;<br />+				break;<br />+			}<br />+		}<br />+<br />+		if (lock) {<br />+			if (!test_and_set_bit_lock(bit_nr, &amp;page-&gt;flags))<br />+				break;<br />+		} else {<br />+			if (!test_bit(bit_nr, &amp;page-&gt;flags))<br />+				break;<br />+		}<br />+	}<br />+<br />+	finish_wait(q, wait);<br />+<br />+	/*<br />+	 * A signal could leave PageWaiters set. Clearing it here if<br />+	 * !waitqueue_active would be possible (by open-coding finish_wait),<br />+	 * but still fail to catch it in the case of wait hash collision. We<br />+	 * already can fail to clear wait hash collision cases, so don't<br />+	 * bother with signals either.<br />+	 */<br />+<br />+	return ret;<br />+}<br />+<br />+void wait_on_page_bit(struct page *page, int bit_nr)<br />+{<br />+	wait_queue_head_t *q = page_waitqueue(page);<br />+	wait_on_page_bit_common(q, page, bit_nr, TASK_UNINTERRUPTIBLE, false);<br />+}<br />+EXPORT_SYMBOL(wait_on_page_bit);<br />+<br />+int wait_on_page_bit_killable(struct page *page, int bit_nr)<br />+{<br />+	wait_queue_head_t *q = page_waitqueue(page);<br />+	return wait_on_page_bit_common(q, page, bit_nr, TASK_KILLABLE, false);<br />+}<br />+<br />+/**<br />+ * add_page_wait_queue - Add an arbitrary waiter to a page's wait queue<br />+ * &#64;page: Page defining the wait queue of interest<br />+ * &#64;waiter: Waiter to add to the queue<br />+ *<br />+ * Add an arbitrary &#64;waiter to the wait queue for the nominated &#64;page.<br />+ */<br />+void add_page_wait_queue(struct page *page, wait_queue_entry_t *waiter)<br />+{<br />+	wait_queue_head_t *q = page_waitqueue(page);<br />+	unsigned long flags;<br />+<br />+	spin_lock_irqsave(&amp;q-&gt;lock, flags);<br />+	__add_wait_queue(q, waiter);<br />+	SetPageWaiters(page);<br />+	spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<br />+}<br />+EXPORT_SYMBOL_GPL(add_page_wait_queue);<br />+<br />+<br />+/**<br />+ * __lock_page - get a lock on the page, assuming we need to sleep to get it<br />+ * &#64;__page: the page to lock<br />+ */<br />+void __lock_page(struct page *__page)<br />+{<br />+	struct page *page = compound_head(__page);<br />+	wait_queue_head_t *q = page_waitqueue(page);<br />+	wait_on_page_bit_common(q, page, PG_locked, TASK_UNINTERRUPTIBLE, true);<br />+}<br />+EXPORT_SYMBOL(__lock_page);<br />+<br />+int __lock_page_killable(struct page *__page)<br />+{<br />+	struct page *page = compound_head(__page);<br />+	wait_queue_head_t *q = page_waitqueue(page);<br />+	return wait_on_page_bit_common(q, page, PG_locked, TASK_KILLABLE, true);<br />+}<br />+EXPORT_SYMBOL_GPL(__lock_page_killable);<br />+<br />+/**<br />+ * unlock_page - unlock a locked page<br />+ * &#64;page: the page<br />+ *<br />+ * Unlocks the page and wakes up sleepers in ___wait_on_page_locked().<br />+ * Also wakes sleepers in wait_on_page_writeback() because the wakeup<br />+ * mechanism between PageLocked pages and PageWriteback pages is shared.<br />+ * But that's OK - sleepers in wait_on_page_writeback() just go back to sleep.<br />+ *<br />+ * Note that this depends on PG_waiters being the sign bit in the byte<br />+ * that contains PG_locked - thus the BUILD_BUG_ON(). That allows us to<br />+ * clear the PG_locked bit and test PG_waiters at the same time fairly<br />+ * portably (architectures that do LL/SC can test any bit, while x86 can<br />+ * test the sign bit).<br />+ */<br />+void unlock_page(struct page *page)<br />+{<br />+	BUILD_BUG_ON(PG_waiters != 7);<br />+	page = compound_head(page);<br />+	VM_BUG_ON_PAGE(!PageLocked(page), page);<br />+	if (clear_bit_unlock_is_negative_byte(PG_locked, &amp;page-&gt;flags))<br />+		wake_up_page_bit(page, PG_locked);<br />+}<br />+EXPORT_SYMBOL(unlock_page);<br />+<br />+/**<br />+ * end_page_writeback - end writeback against a page<br />+ * &#64;page: the page<br />+ */<br />+void end_page_writeback(struct page *page)<br />+{<br />+	/*<br />+	 * TestClearPageReclaim could be used here but it is an atomic<br />+	 * operation and overkill in this particular case. Failing to<br />+	 * shuffle a page marked for immediate reclaim is too mild to<br />+	 * justify taking an atomic operation penalty at the end of<br />+	 * ever page writeback.<br />+	 */<br />+	if (PageReclaim(page)) {<br />+		ClearPageReclaim(page);<br />+		rotate_reclaimable_page(page);<br />+	}<br />+<br />+	if (!test_clear_page_writeback(page))<br />+		BUG();<br />+<br />+	smp_mb__after_atomic();<br />+	wake_up_page(page, PG_writeback);<br />+}<br />+EXPORT_SYMBOL(end_page_writeback);<br />-- <br />2.14.0.rc1.2.g4c8247ec3<br />From 8cf9377c98d77b2a2de0dbc451e8ac283684a4e2 Mon Sep 17 00:00:00 2001<br />From: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />Date: Fri, 25 Aug 2017 15:45:43 -0700<br />Subject: [PATCH 2/2] Re-implement the page bit-wait code<br /><br />The page wait-queues have some horrible scaling issues, and they seem to<br />be hard to fix.  And the way they use the regular wait-queues made that<br />really bad, with interrupts disabled for long times etc.<br /><br />This tries to re-implement them with a totally different model.  It's<br />known broken, and the add_page_wait_queue() thing that the cachefiles<br />code wants to use is not implemented at all (it probably will just need<br />to have a parallel set of wait-queues that are *only* used for that).<br /><br />The code is untested and probably horribly buggy, but I'm hoping others<br />will take a look at this.<br /><br />Not-signed-off-yet-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />---<br /> mm/page_wait_bit.c | 309 ++++++++++++++++++++++++++++++++++++-----------------<br /> 1 file changed, 209 insertions(+), 100 deletions(-)<br /><br />diff --git a/mm/page_wait_bit.c b/mm/page_wait_bit.c<br />index 7550b6d2715a..47fdb305ddae 100644<br />--- a/mm/page_wait_bit.c<br />+++ b/mm/page_wait_bit.c<br />&#64;&#64; -9,9 +9,38 &#64;&#64;<br /> #include &lt;linux/wait.h&gt;<br /> #include &lt;linux/export.h&gt;<br /> #include &lt;linux/sched/signal.h&gt;<br />+#include &lt;linux/list_bl.h&gt;<br /> <br /> #include "internal.h"<br /> <br />+/*<br />+ * Each waiter on a page will register this<br />+ * 'page_wait_struct' as part of waiting.<br />+ *<br />+ * Note that for any particular page, only one of the<br />+ * structs will actually be visible at the head of the<br />+ * wait queue hash table at any particular time, but<br />+ * everybody has one, because as one waiter is woken<br />+ * up we will need to pick another head for waiters.<br />+ *<br />+ * NOTE! All the list operations are protected by the<br />+ * hlist_bl_lock on the hash table.<br />+ */<br />+struct page_wait_struct {<br />+	// This is the hash queue head entry<br />+	// only used once per { page, bit }<br />+	struct hlist_bl_node list;<br />+	struct page *page;<br />+	int bit;<br />+<br />+	struct page_wait_struct *all;<br />+	struct page_wait_struct *exclusive;<br />+<br />+	// This is the waiter list<br />+	struct page_wait_struct *next;<br />+	struct task_struct *wake;<br />+};<br />+<br /> /*<br />  * In order to wait for pages to become available there must be<br />  * waitqueues associated with pages. By using a hash table of<br />&#64;&#64; -22,11 +51,11 &#64;&#64;<br />  * at a cost of "thundering herd" phenomena during rare hash<br />  * collisions.<br />  */<br />-#define PAGE_WAIT_TABLE_BITS 8<br />+#define PAGE_WAIT_TABLE_BITS 12<br /> #define PAGE_WAIT_TABLE_SIZE (1 &lt;&lt; PAGE_WAIT_TABLE_BITS)<br />-static wait_queue_head_t page_wait_table[PAGE_WAIT_TABLE_SIZE] __cacheline_aligned;<br />+static struct hlist_bl_head page_wait_table[PAGE_WAIT_TABLE_SIZE] __cacheline_aligned;<br /> <br />-static wait_queue_head_t *page_waitqueue(struct page *page)<br />+static struct hlist_bl_head *page_waitqueue(struct page *page)<br /> {<br /> 	return &amp;page_wait_table[hash_ptr(page, PAGE_WAIT_TABLE_BITS)];<br /> }<br />&#64;&#64; -36,73 +65,129 &#64;&#64; void __init pagecache_init(void)<br /> 	int i;<br /> <br /> 	for (i = 0; i &lt; PAGE_WAIT_TABLE_SIZE; i++)<br />-		init_waitqueue_head(&amp;page_wait_table[i]);<br />+		INIT_HLIST_BL_HEAD(&amp;page_wait_table[i]);<br /> <br /> 	page_writeback_init();<br /> }<br /> <br />-struct wait_page_key {<br />-	struct page *page;<br />-	int bit_nr;<br />-	int page_match;<br />-};<br />+/*<br />+ * We found a wait entry for the requested page and bit.<br />+ *<br />+ * We now need to create a wakeup list, which includes the<br />+ * first exclusive waiter (if any), and all the non-exclusive<br />+ * ones.<br />+ *<br />+ * If there are more than one exclusive waiters, we need to<br />+ * turns the next exclusive waiter into a wait entry, and<br />+ * add it back to the page wait list.<br />+ */<br />+static struct page_wait_struct *create_wake_up_list(struct page_wait_struct *entry, struct hlist_bl_head *head)<br />+{<br />+	struct page_wait_struct *all = entry-&gt;all;<br />+	struct page_wait_struct *exclusive = entry-&gt;exclusive;<br /> <br />-struct wait_page_queue {<br />-	struct page *page;<br />-	int bit_nr;<br />-	wait_queue_entry_t wait;<br />-};<br />+	if (exclusive) {<br />+		struct page_wait_struct *remain = exclusive-&gt;next;<br /> <br />-static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)<br />+		if (remain) {<br />+			remain-&gt;all = NULL;<br />+			remain-&gt;exclusive = remain;<br />+			hlist_bl_add_head(&amp;remain-&gt;list, head);<br />+		}<br />+		exclusive-&gt;next = all;<br />+		all = exclusive;<br />+	}<br />+	return all;<br />+}<br />+<br />+static inline int remove_myself_from_one_list(struct page_wait_struct **p, struct page_wait_struct *entry)<br />+{<br />+	while (*p) {<br />+		struct page_wait_struct *n = *p;<br />+		if (n == entry) {<br />+			*p = n-&gt;next;<br />+			return 1;<br />+		}<br />+		p = &amp;n-&gt;next;<br />+	}<br />+	return 0;<br />+}<br />+<br />+/*<br />+ * We got woken up, and we need to make sure there is no more<br />+ * access to us in the list.<br />+ */<br />+static void remove_myself_from(struct page_wait_struct *old, struct page_wait_struct *entry, struct hlist_bl_head *head)<br /> {<br />-	struct wait_page_key *key = arg;<br />-	struct wait_page_queue *wait_page<br />-		= container_of(wait, struct wait_page_queue, wait);<br />+	/* We can be on only one list */<br />+	if (!remove_myself_from_one_list(&amp;old-&gt;all, entry))<br />+		remove_myself_from_one_list(&amp;old-&gt;exclusive, entry);<br /> <br />-	if (wait_page-&gt;page != key-&gt;page)<br />-	       return 0;<br />-	key-&gt;page_match = 1;<br />+	hlist_bl_del_init(&amp;entry-&gt;list);<br />+}<br /> <br />-	if (wait_page-&gt;bit_nr != key-&gt;bit_nr)<br />-		return 0;<br />-	if (test_bit(key-&gt;bit_nr, &amp;key-&gt;page-&gt;flags))<br />-		return 0;<br /> <br />-	return autoremove_wake_function(wait, mode, sync, key);<br />+/*<br />+ * Find and remove the matching page/bit entry from the (locked) bl list<br />+ *<br />+ * Return ERR_PTR(-ESRCH) if no matching page at all, NULL if page found<br />+ * but not with matching bit.<br />+ */<br />+static struct page_wait_struct *find_del_entry(struct page *page, int bit_nr, struct hlist_bl_head *head)<br />+{<br />+	struct page_wait_struct *entry;<br />+	struct page_wait_struct *ret = ERR_PTR(-ESRCH);<br />+	struct hlist_bl_node *node;<br />+<br />+	hlist_bl_for_each_entry(entry, node, head, list) {<br />+		if (entry-&gt;page != page)<br />+			continue;<br />+		ret = NULL;<br />+		if (entry-&gt;bit != bit_nr)<br />+			continue;<br />+		__hlist_bl_del(node);<br />+		INIT_HLIST_BL_NODE(node);<br />+		ret = entry;<br />+		break;<br />+	}<br />+	return ret;<br /> }<br /> <br /> static void wake_up_page_bit(struct page *page, int bit_nr)<br /> {<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	struct wait_page_key key;<br />+	struct hlist_bl_head *head = page_waitqueue(page);<br />+	struct page_wait_struct *wake;<br /> 	unsigned long flags;<br /> <br />-	key.page = page;<br />-	key.bit_nr = bit_nr;<br />-	key.page_match = 0;<br />+	local_save_flags(flags);<br />+	hlist_bl_lock(head);<br />+<br />+	wake = find_del_entry(page, bit_nr, head);<br />+	if (IS_ERR(wake)) {<br />+		ClearPageWaiters(page);<br />+		wake = NULL;<br />+	} else if (wake) {<br />+		wake = create_wake_up_list(wake, head);<br />+	}<br />+<br />+	hlist_bl_unlock(head);<br />+	local_irq_restore(flags);<br /> <br />-	spin_lock_irqsave(&amp;q-&gt;lock, flags);<br />-	__wake_up_locked_key(q, TASK_NORMAL, &amp;key);<br /> 	/*<br />-	 * It is possible for other pages to have collided on the waitqueue<br />-	 * hash, so in that case check for a page match. That prevents a long-<br />-	 * term waiter<br />+	 * Actually wake everybody up. Note that as we<br />+	 * wake them up, we can't use the 'wake_list'<br />+	 * entry any more, because it's on their stack.<br /> 	 *<br />-	 * It is still possible to miss a case here, when we woke page waiters<br />-	 * and removed them from the waitqueue, but there are still other<br />-	 * page waiters.<br />+	 * We also clear the 'wake' field so that the<br />+	 * target process can see if they got woken up<br />+	 * by a page bit event.<br /> 	 */<br />-	if (!waitqueue_active(q) || !key.page_match) {<br />-		ClearPageWaiters(page);<br />-		/*<br />-		 * It's possible to miss clearing Waiters here, when we woke<br />-		 * our page waiters, but the hashed waitqueue has waiters for<br />-		 * other pages on it.<br />-		 *<br />-		 * That's okay, it's a rare case. The next waker will clear it.<br />-		 */<br />-	}<br />-	spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<br />+	while (wake) {<br />+		struct task_struct *p = wake-&gt;wake;<br />+		wake = wake-&gt;next;<br />+		smp_store_release(&amp;wake-&gt;wake, NULL);<br />+		wake_up_process(p);<br />+	};<br /> }<br /> <br /> static void wake_up_page(struct page *page, int bit)<br />&#64;&#64; -112,76 +197,101 &#64;&#64; static void wake_up_page(struct page *page, int bit)<br /> 	wake_up_page_bit(page, bit);<br /> }<br /> <br />-static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br />-		struct page *page, int bit_nr, int state, bool lock)<br />+/*<br />+ * Wait for the specific page bit to clear.<br />+ */<br />+static void wait_once_on_page_bit(struct page *page, int bit_nr, int state, bool lock)<br /> {<br />-	struct wait_page_queue wait_page;<br />-	wait_queue_entry_t *wait = &amp;wait_page.wait;<br />-	int ret = 0;<br />+	struct page_wait_struct entry, *old;<br />+	struct hlist_bl_head *head;<br />+	unsigned long flags;<br /> <br />-	init_wait(wait);<br />-	wait-&gt;func = wake_page_function;<br />-	wait_page.page = page;<br />-	wait_page.bit_nr = bit_nr;<br />+	INIT_HLIST_BL_NODE(&amp;entry.list);<br />+	entry.page = page;<br />+	entry.bit = bit_nr;<br />+	entry.all = entry.exclusive = NULL;<br />+	entry.next = NULL;<br />+	entry.wake = current;<br />+<br />+	head = page_waitqueue(page);<br />+	local_save_flags(flags);<br />+	hlist_bl_lock(head);<br />+<br />+	old = find_del_entry(page, bit_nr, head);<br />+	if (IS_ERR(old))<br />+		old = NULL;<br />+	if (old) {<br />+		entry.all = old-&gt;all;<br />+		entry.exclusive = old-&gt;exclusive;<br />+	}<br /> <br />-	for (;;) {<br />-		spin_lock_irq(&amp;q-&gt;lock);<br />-<br />-		if (likely(list_empty(&amp;wait-&gt;entry))) {<br />-			if (lock)<br />-				__add_wait_queue_entry_tail_exclusive(q, wait);<br />-			else<br />-				__add_wait_queue(q, wait);<br />-			SetPageWaiters(page);<br />-		}<br />+	if (lock) {<br />+		entry.next = entry.exclusive;<br />+		entry.exclusive = &amp;entry;<br />+	} else {<br />+		entry.next = entry.all;<br />+		entry.all = &amp;entry;<br />+	}<br /> <br />-		set_current_state(state);<br />+	hlist_bl_add_head(&amp;entry.list, head);<br />+	current-&gt;state = state;<br /> <br />-		spin_unlock_irq(&amp;q-&gt;lock);<br />+	hlist_bl_unlock(head);<br />+	local_irq_restore(flags);<br /> <br />-		if (likely(test_bit(bit_nr, &amp;page-&gt;flags))) {<br />-			io_schedule();<br />-			if (unlikely(signal_pending_state(state, current))) {<br />-				ret = -EINTR;<br />-				break;<br />-			}<br />-		}<br />+	if (likely(test_bit(bit_nr, &amp;page-&gt;flags)))<br />+		io_schedule();<br />+<br />+	/*<br />+	 * NOTE! If we were woken up by something else,<br />+	 * we have to remove ourselves from the hash list.<br />+	 *<br />+	 * But in order to avoid extra locking overhead in<br />+	 * the common case, we only do this if we can't<br />+	 * already tell that we were woken up (and thus<br />+	 * no longer on the lists).<br />+	 */<br />+	if (smp_load_acquire(&amp;entry.wake) != NULL) {<br />+		local_save_flags(flags);<br />+		hlist_bl_lock(head);<br />+<br />+		old = find_del_entry(page, bit_nr, head);<br />+		if (old &amp;&amp; !IS_ERR(old))<br />+			remove_myself_from(old, &amp;entry, head);<br /> <br />+		hlist_bl_unlock(head);<br />+		local_irq_restore(flags);<br />+	}<br />+}<br />+<br />+static inline int wait_on_page_bit_common(struct page *page, int bit_nr, int state, bool lock)<br />+{<br />+	for (;;) {<br />+		wait_once_on_page_bit(page, bit_nr, state, lock);<br /> 		if (lock) {<br /> 			if (!test_and_set_bit_lock(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />+				return 0;<br /> 		} else {<br /> 			if (!test_bit(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />+				return 0;<br /> 		}<br />+		if (unlikely(signal_pending_state(state, current)))<br />+			return -EINTR;<br /> 	}<br />-<br />-	finish_wait(q, wait);<br />-<br />-	/*<br />-	 * A signal could leave PageWaiters set. Clearing it here if<br />-	 * !waitqueue_active would be possible (by open-coding finish_wait),<br />-	 * but still fail to catch it in the case of wait hash collision. We<br />-	 * already can fail to clear wait hash collision cases, so don't<br />-	 * bother with signals either.<br />-	 */<br />-<br />-	return ret;<br /> }<br /> <br /> void wait_on_page_bit(struct page *page, int bit_nr)<br /> {<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	wait_on_page_bit_common(q, page, bit_nr, TASK_UNINTERRUPTIBLE, false);<br />+	wait_on_page_bit_common(page, bit_nr, TASK_UNINTERRUPTIBLE, false);<br /> }<br /> EXPORT_SYMBOL(wait_on_page_bit);<br /> <br /> int wait_on_page_bit_killable(struct page *page, int bit_nr)<br /> {<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	return wait_on_page_bit_common(q, page, bit_nr, TASK_KILLABLE, false);<br />+	return wait_on_page_bit_common(page, bit_nr, TASK_KILLABLE, false);<br /> }<br /> <br />+#if 0<br /> /**<br />  * add_page_wait_queue - Add an arbitrary waiter to a page's wait queue<br />  * &#64;page: Page defining the wait queue of interest<br />&#64;&#64; -200,6 +310,7 &#64;&#64; void add_page_wait_queue(struct page *page, wait_queue_entry_t *waiter)<br /> 	spin_unlock_irqrestore(&amp;q-&gt;lock, flags);<br /> }<br /> EXPORT_SYMBOL_GPL(add_page_wait_queue);<br />+#endif<br /> <br /> <br /> /**<br />&#64;&#64; -209,16 +320,14 &#64;&#64; EXPORT_SYMBOL_GPL(add_page_wait_queue);<br /> void __lock_page(struct page *__page)<br /> {<br /> 	struct page *page = compound_head(__page);<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	wait_on_page_bit_common(q, page, PG_locked, TASK_UNINTERRUPTIBLE, true);<br />+	wait_on_page_bit_common(page, PG_locked, TASK_UNINTERRUPTIBLE, true);<br /> }<br /> EXPORT_SYMBOL(__lock_page);<br /> <br /> int __lock_page_killable(struct page *__page)<br /> {<br /> 	struct page *page = compound_head(__page);<br />-	wait_queue_head_t *q = page_waitqueue(page);<br />-	return wait_on_page_bit_common(q, page, PG_locked, TASK_KILLABLE, true);<br />+	return wait_on_page_bit_common(page, PG_locked, TASK_KILLABLE, true);<br /> }<br /> EXPORT_SYMBOL_GPL(__lock_page_killable);<br /> <br />-- <br />2.14.0.rc1.2.g4c8247ec3<br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
