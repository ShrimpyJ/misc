    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2016/11/21/748">First message in thread</a></li><li><a href="/lkml/2016/11/29/584">Michal Hocko</a><ul><li><a href="/lkml/2016/11/29/611">Marc MERLIN</a><ul><li class="origin"><a href="/lkml/2016/11/29/736">Linus Torvalds</a><ul><li><a href="/lkml/2016/11/29/736">Marc MERLIN</a><ul><li><a href="/lkml/2016/11/29/751">Linus Torvalds</a></li><li><a href="/lkml/2016/11/29/984">Marc MERLIN</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Tue, 29 Nov 2016 09:07:03 -0800</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: 4.8.8 kernel trigger OOM killer repeatedly when I have lots of RAM that should be free</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Tue, Nov 29, 2016 at 8:34 AM, Marc MERLIN &lt;marc&#64;merlins.org&gt; wrote:<br />&gt; Now, to be fair, this is not a new problem, it's just varying degrees of<br />&gt; bad and usually only happens when I do a lot of I/O with btrfs.<br /><br />One situation where I've seen something like this happen is<br /><br /> (a) lots and lots of dirty data queued up<br /> (b) horribly slow storage<br /> (c) filesystem that ends up serializing on writeback under certain<br />circumstances<br /><br />The usual case for (b) in the modern world is big SSD's that have bad<br />worst-case behavior (ie they may do gbps speeds when doing well, and<br />then they come to a screeching halt when their buffers fill up and<br />they have to do rewrites, and their gbps throughput drops to mbps or<br />lower).<br /><br />Generally you only find that kind of really nasty SSD in the USB stick<br />world these days.<br /><br />The usual case for (c) is "fsync" or similar - often on a totally<br />unrelated file - which then ends up waiting for everything else to<br />flush too. Looks like btrfs_start_ordered_extent() does something kind<br />of like that, where it waits for data to be flushed.<br /><br />The usual *fix* for this is to just not get into situation (a).<br /><br />Sadly, our defaults for "how much dirty data do we allow" are somewhat<br />buggered. The global defaults are in "percent of memory", and are<br />generally _much_ too high for big-memory machines:<br /><br />    [torvalds&#64;i7 linux]$ cat /proc/sys/vm/dirty_ratio<br />    20<br />    [torvalds&#64;i7 linux]$ cat /proc/sys/vm/dirty_background_ratio<br />    10<br /><br />says that it only starts really throttling writes when you hit 20% of<br />all memory used. You don't say how much memory you have in that<br />machine, but if it's the same one you talked about earlier, it was<br />24GB. So you can have 4GB of dirty data waiting to be flushed out.<br /><br />And we *try* to do this per-device backing-dev congestion thing to<br />make things work better, but it generally seems to not work very well.<br />Possibly because of inconsistent write speeds (ie _sometimes_ the SSD<br />does really well, and we want to open up, and then it shuts down).<br /><br />One thing you can try is to just make the global limits much lower. As in<br /><br />   echo 2 &gt; /proc/sys/vm/dirty_ratio<br />   echo 1 &gt; /proc/sys/vm/dirty_background_ratio<br /><br />(if you want to go lower than 1%, you'll have to use the<br />"dirty_*ratio_bytes" byte limits instead of percentage limits).<br /><br />Obviously you'll need to be root for this, and equally obviously it's<br />really a failure of the kernel. I'd *love* to get something like this<br />right automatically, but sadly it depends so much on memory size,<br />load, disk subsystem, etc etc that I despair at it.<br /><br />On x86-32 we "fixed" this long ago by just saying "high memory is not<br />dirtyable", so you were always limited to a maximum of 10/20% of 1GB,<br />rather than the full memory range. It worked better, but it's a sad<br />kind of fix.<br /><br />(See commit dc6e29da9162: "Fix balance_dirty_page() calculations with<br />CONFIG_HIGHMEM")<br /><br />             Linus<br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
