    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2016/12/24/103">First message in thread</a></li><li><a href="/lkml/2016/12/25/95">Linus Torvalds</a><ul><li><a href="/lkml/2016/12/25/113">Nicholas Piggin</a><ul><li class="origin"><a href="/lkml/2016/12/27/84">Linus Torvalds</a><ul><li><a href="/lkml/2016/12/27/84">Nicholas Piggin</a><ul><li><a href="/lkml/2016/12/27/242">Linus Torvalds</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Mon, 26 Dec 2016 11:07:52 -0800</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH 2/2] mm: add PageWaiters indicating tasks are waiting for a page bit</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Sun, Dec 25, 2016 at 5:16 PM, Nicholas Piggin &lt;npiggin&#64;gmail.com&gt; wrote:<br />&gt;<br />&gt; I did actually play around with that. I could not get my skylake<br />&gt; to forward the result from a lock op to a subsequent load (the<br />&gt; latency was the same whether you use lock ; andb or lock ; andl<br />&gt; (32 cycles for my test loop) whereas with non-atomic versions I<br />&gt; was getting about 15 cycles for andb vs 2 for andl.<br /><br />Yes, interesting. It does look like the locked ops don't end up having<br />the partial write issue and the size of the op doesn't matter.<br /><br />But it's definitely the case that the write buffer hit immediately<br />after the atomic read-modify-write ends up slowing things down, so the<br />profile oddity isn't just a profile artifact. I wrote a stupid test<br />program that did an atomic increment, and then read either the same<br />value, or an adjacent value in memory (so same instruvtion sequence,<br />the difference just being what memory location the read accessed).<br /><br />Reading the same value after the atomic update was *much* more<br />expensive than reading the adjacent value, so it causes some kind of<br />pipeline hickup (by about 50% of the cost of the atomic op itself:<br />iow, the "atomic-op followed by read same location" was over 1.5x<br />slower than "atomic op followed by read of another location").<br /><br />So the atomic ops don't serialize things entirely, but they *hate*<br />having the value read (regardless of size) right after being updated,<br />because it causes some kind of nasty pipeline issue.<br /><br />A cmpxchg does seem to avoid the issue.<br /><br />             Linus<br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
