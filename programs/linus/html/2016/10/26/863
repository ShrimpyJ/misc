    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2016/10/26/494">First message in thread</a></li><li><a href="/lkml/2016/10/26/786">Andy Lutomirski</a><ul><li><a href="/lkml/2016/10/26/832">Linus Torvalds</a><ul><li class="origin"><a href="/lkml/2016/10/26/911">Linus Torvalds</a><ul><li><a href="/lkml/2016/10/26/911">Linus Torvalds</a><ul><li><a href="/lkml/2016/10/26/922">Bob Peterson</a></li></ul></li><li><a href="/lkml/2016/10/26/1047">Mel Gorman</a><ul><li><a href="/lkml/2016/10/26/1088">Linus Torvalds</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2016/10/26/863/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Wed, 26 Oct 2016 10:15:30 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: CONFIG_VMAP_STACK, on-stack struct, and wake_up_bit</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Wed, Oct 26, 2016 at 9:32 AM, Linus Torvalds<br />&lt;torvalds&#64;linux-foundation.org&gt; wrote:<br />&gt;<br />&gt; Quite frankly, I think the solution is to just rip out all the insane<br />&gt; zone crap.<br /><br />IOW, something like the attached.<br /><br />Advantage:<br /><br /> - just look at the number of garbage lines removed!  21<br />insertions(+), 182 deletions(-)<br /><br /> - it will actually speed up even the current case for all common<br />situations: no idiotic extra indirections that will take extra cache<br />misses<br /><br /> - because the bit_wait_table array is now denser (256 entries is<br />about 6kB of data on 64-bit with no spinlock debugging, so ~100<br />cachelines), maybe it gets fewer cache misses too<br /><br /> - we know how to handle the page_waitqueue contention issue, and it<br />has nothing to do with the stupid NUMA zones<br /><br />The only case you actually get real page wait activity is IO, and I<br />suspect that hashing it out over ~100 cachelines will be more than<br />sufficient to avoid excessive contention, plus it's a cache-miss vs an<br />IO, so nobody sane cares.<br /><br />The only reason it did that insane per-zone thing in the first place<br />that right now we access those wait-queues even when we damn well<br />shouldn't, and we have the solution for that.<br /><br />Guys, holler if you hate this, but I think it's realistically the only<br />sane solution to the "wait queue on stack" issue.<br /><br />Oh, and the patch is obviously entirely untested. I wouldn't want to<br />ruin my reputation by *testing* the patches I send out. What would be<br />the fun in that?<br /><br />             Linus<br /> include/linux/mmzone.h |  30 +------------<br /> kernel/sched/core.c    |  16 +++++++<br /> kernel/sched/wait.c    |  10 -----<br /> mm/filemap.c           |   4 +-<br /> mm/memory_hotplug.c    |  28 ------------<br /> mm/page_alloc.c        | 115 +------------------------------------------------<br /> 6 files changed, 21 insertions(+), 182 deletions(-)<br /><br />diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h<br />index 7f2ae99e5daf..0f088f3a2fed 100644<br />--- a/include/linux/mmzone.h<br />+++ b/include/linux/mmzone.h<br />&#64;&#64; -440,33 +440,7 &#64;&#64; struct zone {<br /> 	seqlock_t		span_seqlock;<br /> #endif<br /> <br />-	/*<br />-	 * wait_table		-- the array holding the hash table<br />-	 * wait_table_hash_nr_entries	-- the size of the hash table array<br />-	 * wait_table_bits	-- wait_table_size == (1 &lt;&lt; wait_table_bits)<br />-	 *<br />-	 * The purpose of all these is to keep track of the people<br />-	 * waiting for a page to become available and make them<br />-	 * runnable again when possible. The trouble is that this<br />-	 * consumes a lot of space, especially when so few things<br />-	 * wait on pages at a given time. So instead of using<br />-	 * per-page waitqueues, we use a waitqueue hash table.<br />-	 *<br />-	 * The bucket discipline is to sleep on the same queue when<br />-	 * colliding and wake all in that wait queue when removing.<br />-	 * When something wakes, it must check to be sure its page is<br />-	 * truly available, a la thundering herd. The cost of a<br />-	 * collision is great, but given the expected load of the<br />-	 * table, they should be so rare as to be outweighed by the<br />-	 * benefits from the saved space.<br />-	 *<br />-	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the<br />-	 * primary users of these fields, and in mm/page_alloc.c<br />-	 * free_area_init_core() performs the initialization of them.<br />-	 */<br />-	wait_queue_head_t	*wait_table;<br />-	unsigned long		wait_table_hash_nr_entries;<br />-	unsigned long		wait_table_bits;<br />+	int initialized;<br /> <br /> 	/* Write-intensive fields used from the page allocator */<br /> 	ZONE_PADDING(_pad1_)<br />&#64;&#64; -546,7 +520,7 &#64;&#64; static inline bool zone_spans_pfn(const struct zone *zone, unsigned long pfn)<br /> <br /> static inline bool zone_is_initialized(struct zone *zone)<br /> {<br />-	return !!zone-&gt;wait_table;<br />+	return zone-&gt;initialized;<br /> }<br /> <br /> static inline bool zone_is_empty(struct zone *zone)<br />diff --git a/kernel/sched/core.c b/kernel/sched/core.c<br />index 94732d1ab00a..42d4027f9e26 100644<br />--- a/kernel/sched/core.c<br />+++ b/kernel/sched/core.c<br />&#64;&#64; -7515,11 +7515,27 &#64;&#64; static struct kmem_cache *task_group_cache __read_mostly;<br /> DECLARE_PER_CPU(cpumask_var_t, load_balance_mask);<br /> DECLARE_PER_CPU(cpumask_var_t, select_idle_mask);<br /> <br />+#define WAIT_TABLE_BITS 8<br />+#define WAIT_TABLE_SIZE (1 &lt;&lt; WAIT_TABLE_BITS)<br />+static wait_queue_head_t bit_wait_table[WAIT_TABLE_SIZE] __cacheline_aligned;<br />+<br />+wait_queue_head_t *bit_waitqueue(void *word, int bit)<br />+{<br />+	const int shift = BITS_PER_LONG == 32 ? 5 : 6;<br />+	unsigned long val = (unsigned long)word &lt;&lt; shift | bit;<br />+<br />+	return bit_wait_table + hash_long(val, WAIT_TABLE_BITS);<br />+}<br />+EXPORT_SYMBOL(bit_waitqueue);<br />+<br /> void __init sched_init(void)<br /> {<br /> 	int i, j;<br /> 	unsigned long alloc_size = 0, ptr;<br /> <br />+	for (i = 0; i &lt; WAIT_TABLE_SIZE; i++)<br />+		init_waitqueue_head(bit_wait_table + i);<br />+<br /> #ifdef CONFIG_FAIR_GROUP_SCHED<br /> 	alloc_size += 2 * nr_cpu_ids * sizeof(void **);<br /> #endif<br />diff --git a/kernel/sched/wait.c b/kernel/sched/wait.c<br />index 4f7053579fe3..9453efe9b25a 100644<br />--- a/kernel/sched/wait.c<br />+++ b/kernel/sched/wait.c<br />&#64;&#64; -480,16 +480,6 &#64;&#64; void wake_up_bit(void *word, int bit)<br /> }<br /> EXPORT_SYMBOL(wake_up_bit);<br /> <br />-wait_queue_head_t *bit_waitqueue(void *word, int bit)<br />-{<br />-	const int shift = BITS_PER_LONG == 32 ? 5 : 6;<br />-	const struct zone *zone = page_zone(virt_to_page(word));<br />-	unsigned long val = (unsigned long)word &lt;&lt; shift | bit;<br />-<br />-	return &amp;zone-&gt;wait_table[hash_long(val, zone-&gt;wait_table_bits)];<br />-}<br />-EXPORT_SYMBOL(bit_waitqueue);<br />-<br /> /*<br />  * Manipulate the atomic_t address to produce a better bit waitqueue table hash<br />  * index (we're keying off bit -1, but that would produce a horrible hash<br />diff --git a/mm/filemap.c b/mm/filemap.c<br />index 849f459ad078..c7fe2f16503f 100644<br />--- a/mm/filemap.c<br />+++ b/mm/filemap.c<br />&#64;&#64; -790,9 +790,7 &#64;&#64; EXPORT_SYMBOL(__page_cache_alloc);<br />  */<br /> wait_queue_head_t *page_waitqueue(struct page *page)<br /> {<br />-	const struct zone *zone = page_zone(page);<br />-<br />-	return &amp;zone-&gt;wait_table[hash_ptr(page, zone-&gt;wait_table_bits)];<br />+	return bit_waitqueue(page, 0);<br /> }<br /> EXPORT_SYMBOL(page_waitqueue);<br /> <br />diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c<br />index 962927309b6e..b18dab401be6 100644<br />--- a/mm/memory_hotplug.c<br />+++ b/mm/memory_hotplug.c<br />&#64;&#64; -268,7 +268,6 &#64;&#64; void __init register_page_bootmem_info_node(struct pglist_data *pgdat)<br /> 	unsigned long i, pfn, end_pfn, nr_pages;<br /> 	int node = pgdat-&gt;node_id;<br /> 	struct page *page;<br />-	struct zone *zone;<br /> <br /> 	nr_pages = PAGE_ALIGN(sizeof(struct pglist_data)) &gt;&gt; PAGE_SHIFT;<br /> 	page = virt_to_page(pgdat);<br />&#64;&#64; -276,19 +275,6 &#64;&#64; void __init register_page_bootmem_info_node(struct pglist_data *pgdat)<br /> 	for (i = 0; i &lt; nr_pages; i++, page++)<br /> 		get_page_bootmem(node, page, NODE_INFO);<br /> <br />-	zone = &amp;pgdat-&gt;node_zones[0];<br />-	for (; zone &lt; pgdat-&gt;node_zones + MAX_NR_ZONES - 1; zone++) {<br />-		if (zone_is_initialized(zone)) {<br />-			nr_pages = zone-&gt;wait_table_hash_nr_entries<br />-				* sizeof(wait_queue_head_t);<br />-			nr_pages = PAGE_ALIGN(nr_pages) &gt;&gt; PAGE_SHIFT;<br />-			page = virt_to_page(zone-&gt;wait_table);<br />-<br />-			for (i = 0; i &lt; nr_pages; i++, page++)<br />-				get_page_bootmem(node, page, NODE_INFO);<br />-		}<br />-	}<br />-<br /> 	pfn = pgdat-&gt;node_start_pfn;<br /> 	end_pfn = pgdat_end_pfn(pgdat);<br /> <br />&#64;&#64; -2158,20 +2144,6 &#64;&#64; void try_offline_node(int nid)<br /> 	 */<br /> 	node_set_offline(nid);<br /> 	unregister_one_node(nid);<br />-<br />-	/* free waittable in each zone */<br />-	for (i = 0; i &lt; MAX_NR_ZONES; i++) {<br />-		struct zone *zone = pgdat-&gt;node_zones + i;<br />-<br />-		/*<br />-		 * wait_table may be allocated from boot memory,<br />-		 * here only free if it's allocated by vmalloc.<br />-		 */<br />-		if (is_vmalloc_addr(zone-&gt;wait_table)) {<br />-			vfree(zone-&gt;wait_table);<br />-			zone-&gt;wait_table = NULL;<br />-		}<br />-	}<br /> }<br /> EXPORT_SYMBOL(try_offline_node);<br /> <br />diff --git a/mm/page_alloc.c b/mm/page_alloc.c<br />index 2b3bf6767d54..de7c6e43b1c9 100644<br />--- a/mm/page_alloc.c<br />+++ b/mm/page_alloc.c<br />&#64;&#64; -4977,72 +4977,6 &#64;&#64; void __ref build_all_zonelists(pg_data_t *pgdat, struct zone *zone)<br /> }<br /> <br /> /*<br />- * Helper functions to size the waitqueue hash table.<br />- * Essentially these want to choose hash table sizes sufficiently<br />- * large so that collisions trying to wait on pages are rare.<br />- * But in fact, the number of active page waitqueues on typical<br />- * systems is ridiculously low, less than 200. So this is even<br />- * conservative, even though it seems large.<br />- *<br />- * The constant PAGES_PER_WAITQUEUE specifies the ratio of pages to<br />- * waitqueues, i.e. the size of the waitq table given the number of pages.<br />- */<br />-#define PAGES_PER_WAITQUEUE	256<br />-<br />-#ifndef CONFIG_MEMORY_HOTPLUG<br />-static inline unsigned long wait_table_hash_nr_entries(unsigned long pages)<br />-{<br />-	unsigned long size = 1;<br />-<br />-	pages /= PAGES_PER_WAITQUEUE;<br />-<br />-	while (size &lt; pages)<br />-		size &lt;&lt;= 1;<br />-<br />-	/*<br />-	 * Once we have dozens or even hundreds of threads sleeping<br />-	 * on IO we've got bigger problems than wait queue collision.<br />-	 * Limit the size of the wait table to a reasonable size.<br />-	 */<br />-	size = min(size, 4096UL);<br />-<br />-	return max(size, 4UL);<br />-}<br />-#else<br />-/*<br />- * A zone's size might be changed by hot-add, so it is not possible to determine<br />- * a suitable size for its wait_table.  So we use the maximum size now.<br />- *<br />- * The max wait table size = 4096 x sizeof(wait_queue_head_t).   ie:<br />- *<br />- *    i386 (preemption config)    : 4096 x 16 = 64Kbyte.<br />- *    ia64, x86-64 (no preemption): 4096 x 20 = 80Kbyte.<br />- *    ia64, x86-64 (preemption)   : 4096 x 24 = 96Kbyte.<br />- *<br />- * The maximum entries are prepared when a zone's memory is (512K + 256) pages<br />- * or more by the traditional way. (See above).  It equals:<br />- *<br />- *    i386, x86-64, powerpc(4K page size) : =  ( 2G + 1M)byte.<br />- *    ia64(16K page size)                 : =  ( 8G + 4M)byte.<br />- *    powerpc (64K page size)             : =  (32G +16M)byte.<br />- */<br />-static inline unsigned long wait_table_hash_nr_entries(unsigned long pages)<br />-{<br />-	return 4096UL;<br />-}<br />-#endif<br />-<br />-/*<br />- * This is an integer logarithm so that shifts can be used later<br />- * to extract the more random high bits from the multiplicative<br />- * hash function before the remainder is taken.<br />- */<br />-static inline unsigned long wait_table_bits(unsigned long size)<br />-{<br />-	return ffz(~size);<br />-}<br />-<br />-/*<br />  * Initially all pages are reserved - free ones are freed<br />  * up by free_all_bootmem() once the early boot process is<br />  * done. Non-atomic initialization, single-pass.<br />&#64;&#64; -5304,49 +5238,6 &#64;&#64; void __init setup_per_cpu_pageset(void)<br /> 			alloc_percpu(struct per_cpu_nodestat);<br /> }<br /> <br />-static noinline __ref<br />-int zone_wait_table_init(struct zone *zone, unsigned long zone_size_pages)<br />-{<br />-	int i;<br />-	size_t alloc_size;<br />-<br />-	/*<br />-	 * The per-page waitqueue mechanism uses hashed waitqueues<br />-	 * per zone.<br />-	 */<br />-	zone-&gt;wait_table_hash_nr_entries =<br />-		 wait_table_hash_nr_entries(zone_size_pages);<br />-	zone-&gt;wait_table_bits =<br />-		wait_table_bits(zone-&gt;wait_table_hash_nr_entries);<br />-	alloc_size = zone-&gt;wait_table_hash_nr_entries<br />-					* sizeof(wait_queue_head_t);<br />-<br />-	if (!slab_is_available()) {<br />-		zone-&gt;wait_table = (wait_queue_head_t *)<br />-			memblock_virt_alloc_node_nopanic(<br />-				alloc_size, zone-&gt;zone_pgdat-&gt;node_id);<br />-	} else {<br />-		/*<br />-		 * This case means that a zone whose size was 0 gets new memory<br />-		 * via memory hot-add.<br />-		 * But it may be the case that a new node was hot-added.  In<br />-		 * this case vmalloc() will not be able to use this new node's<br />-		 * memory - this wait_table must be initialized to use this new<br />-		 * node itself as well.<br />-		 * To use this new node's memory, further consideration will be<br />-		 * necessary.<br />-		 */<br />-		zone-&gt;wait_table = vmalloc(alloc_size);<br />-	}<br />-	if (!zone-&gt;wait_table)<br />-		return -ENOMEM;<br />-<br />-	for (i = 0; i &lt; zone-&gt;wait_table_hash_nr_entries; ++i)<br />-		init_waitqueue_head(zone-&gt;wait_table + i);<br />-<br />-	return 0;<br />-}<br />-<br /> static __meminit void zone_pcp_init(struct zone *zone)<br /> {<br /> 	/*<br />&#64;&#64; -5367,10 +5258,7 &#64;&#64; int __meminit init_currently_empty_zone(struct zone *zone,<br /> 					unsigned long size)<br /> {<br /> 	struct pglist_data *pgdat = zone-&gt;zone_pgdat;<br />-	int ret;<br />-	ret = zone_wait_table_init(zone, size);<br />-	if (ret)<br />-		return ret;<br />+<br /> 	pgdat-&gt;nr_zones = zone_idx(zone) + 1;<br /> <br /> 	zone-&gt;zone_start_pfn = zone_start_pfn;<br />&#64;&#64; -5382,6 +5270,7 &#64;&#64; int __meminit init_currently_empty_zone(struct zone *zone,<br /> 			zone_start_pfn, (zone_start_pfn + size));<br /> <br /> 	zone_init_free_lists(zone);<br />+	zone-&gt;initialized = 1;<br /> <br /> 	return 0;<br /> }</pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
