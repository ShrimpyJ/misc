    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2016/10/11/389">First message in thread</a></li><li><a href="/lkml/2016/10/24/876">Linus Torvalds</a><ul><li><a href="/lkml/2016/10/24/900">Linus Torvalds</a><ul><li><a href="/lkml/2016/10/24/921">Linus Torvalds</a><ul><li><a href="/lkml/2016/10/24/933">Chris Mason</a></li></ul></li></ul></li><li><a href="/lkml/2016/10/24/969">Andy Lutomirski</a><ul><li class="origin"><a href="/lkml/2016/10/24/1063">Linus Torvalds</a><ul><li><a href="/lkml/2016/10/24/1063">Andy Lutomirski</a></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Mon, 24 Oct 2016 17:00:35 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: bio linked list corruption.</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Mon, Oct 24, 2016 at 3:42 PM, Andy Lutomirski &lt;luto&#64;amacapital.net&gt; wrote:<br />&gt;<br />&gt; Here's my theory: I think you're looking at the right code but the<br />&gt; wrong stack.  shmem_fault_wait is fine, but shmem_fault_waitq looks<br />&gt; really dicey.<br /><br />Hmm.<br /><br />&gt; Consider:<br />&gt;<br />&gt; fallocate calls wake_up_all(), which calls autoremove_wait_function().<br />&gt; That wakes up the shmem_fault thread.  Suppose that the shmem_fault<br />&gt; thread gets moving really quickly (presumably because it never went to<br />&gt; sleep in the first place -- suppose it hadn't made it to schedule(),<br />&gt; so schedule() turns into a no-op).  It calls finish_wait() *before*<br />&gt; autoremove_wake_function() does the list_del_init().  finish_wait()<br />&gt; gets to the list_empty_careful() call and it returns true.<br /><br />All of this happens under inode-&gt;i_lock, so the different parts are<br />serialized. So if this happens before the wakeup, then finish_wait()<br />will se that the wait-queue entry is not empty (it points to the wait<br />queue head in shmem_falloc_waitq.<br /><br />But then it will just remove itself carefully with list_del_init()<br />under the waitqueue lock, and things are fine.<br /><br />Yes, it uses the waitiqueue lock on the other stack, but that stack is<br />still ok since<br /> (a) we're serialized by inode-&gt;i_lock<br /> (b) this code ran before the fallocate thread catches up and exits.<br /><br />In other words, your scenario seems to be dependent on those two<br />threads being able to race. But they both hold inode-&gt;i_lock in the<br />critical region you are talking about.<br /><br />&gt; Now the fallocate thread catches up and *exits*.  Dave's test makes a<br />&gt; new thread that reuses the stack (the vmap area or the backing store).<br />&gt;<br />&gt; Now the shmem_fault thread continues on its merry way and takes<br />&gt; q-&gt;lock.  But oh crap, q-&gt;lock is pointing at some random spot on some<br />&gt; other thread's stack.  Kaboom!<br /><br />Note that q-&gt;lock should be entirely immaterial, since inode-&gt;i_lock<br />nests outside of it in all uses.<br /><br />Now, if there is some code that runs *without* the inode-&gt;i_lock, then<br />that would be a big bug.<br /><br />But I'm not seeing it.<br /><br />I do agree that some race on some stack data structure could easily be<br />the cause of these issues. And yes, the vmap code obviously starts<br />reusing the stack much earlier, and would trigger problems that would<br />essentially be hidden by the fact that the kernel stack used to stay<br />around not just until exit(), but until the process was reaped.<br /><br />I just think that in this case i_lock really looks like it should<br />serialize things correctly.<br /><br />Or are you seeing something I'm not?<br /><br />                   Linus<br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
