    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2016/8/9/440">First message in thread</a></li><li><a href="/lkml/2016/8/15/801">Linus Torvalds</a><ul><li><a href="/lkml/2016/8/15/830">Dave Chinner</a></li><li><a href="/lkml/2016/8/16/507">Mel Gorman</a><ul><li class="origin"><a href="/lkml/2016/8/17/360">Linus Torvalds</a><ul><li><a href="/lkml/2016/8/17/360">Michal Hocko</a><ul><li><a href="/lkml/2016/8/17/388">Michal Hocko</a></li></ul></li><li><a href="/lkml/2016/8/17/361">Mel Gorman</a><ul><li><a href="/lkml/2016/8/17/600">Mel Gorman</a></li><li><a href="/lkml/2016/8/17/642">Dave Chinner</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Tue, 16 Aug 2016 10:47:36 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [LKP] [lkp] [xfs] 68a9f5e700: aim7.jobs-per-min -13.6% regression</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">Mel,<br /> thanks for taking a look. Your theory sounds more complete than mine,<br />and since Dave is able to see the problem with 4.7, it would be nice<br />to hear about the 4.6 behavior and commit ede37713737 in particular.<br /><br />That one seems more likely to affect contention than the zone/node one<br />I found during the merge window anyway, since it actually removes a<br />sleep in kswapd during congestion.<br /><br />I've always preferred to see direct reclaim as the primary model for<br />reclaim, partly in order to throttle the actual "bad" process, but<br />also because "kswapd uses lots of CPU time" is such a nasty thing to<br />even begin guessing about.<br /><br />So I have to admit to liking that "make kswapd sleep a bit if it's<br />just looping" logic that got removed in that commit.<br /><br />And looking at DaveC's numbers, it really feels like it's not even<br />what we do inside the locked region that is the problem. Sure,<br />__delete_from_page_cache() (which is most of it) is at 1.86% of CPU<br />time (when including all the things it calls), but that still isn't<br />all that much. Especially when compared to just:<br /><br />   0.78%  [kernel]  [k] _raw_spin_unlock_irqrestore<br /><br />from his flat profile. That's not some spinning wait, that's just<br />releasing the lock with a single write (and the popf, but while that's<br />an expensive instruction, it's just tens of cpu cycles).<br /><br />So I'm more and more getting the feeling that it's not what we do<br />inside the lock that is problematic. I started out blaming memcg<br />accounting or something, but none of the numbers seem to back that up.<br />So it's primarily really just the fact that kswapd is simply hammering<br />on that lock way too much.<br /><br />So yeah, I'm blaming kswapd itself doing something wrong. It's not a<br />problem in a single-node environment (since there's only one), but<br />with multiple nodes it clearly just devolves.<br /><br />Yes, we could try to batch the locking like DaveC already suggested<br />(ie we could move the locking to the caller, and then make<br />shrink_page_list() just try to keep the lock held for a few pages if<br />the mapping doesn't change), and that might result in fewer crazy<br />cacheline ping-pongs overall. But that feels like exactly the wrong<br />kind of workaround.<br /><br />I'd much rather re-instate some "if kswapd is just spinning on CPU<br />time and not actually improving IO parallelism, kswapd should just get<br />the hell out" logic.<br /><br />Adding Michal Hocko to the participant list too, I think he's one of<br />the gang in this area. Who else should be made aware of this thread?<br />Minchan? Vladimir?<br /><br />[ I'm assuming the new people can look up this thread on lkml. Note to<br />new people: the subject line (and about 75% of the posts) are about an<br />unrelated AIM7 regression, but there's this sub-thread about nasty<br />lock contention on mapping-&gt;tree_lock within that bigger context ]<br /><br />              Linus<br /><br />On Tue, Aug 16, 2016 at 8:05 AM, Mel Gorman &lt;mgorman&#64;techsingularity.net&gt; wrote:<br />&gt;<br />&gt; However, historically there have been multiple indirect throttling mechanism<br />&gt; that were branded as congestion control but basically said "I don't know<br />&gt; what's going on so it's nap time". Many of these have been removed over<br />&gt; time and the last major one was ede37713737 ("mm: throttle on IO only when<br />&gt; there are too many dirty and writeback pages").<br />&gt;<br />&gt; Before that commit, a process that entered direct reclaim and failed to make<br />&gt; progress would sleep before retrying. It's possible that sleep was enough<br />&gt; to reduce contention by temporarily stalling the writer and letting reclaim<br />&gt; make progress. After that commit, it may only do a cond_resched() check<br />&gt; and go back to allocating/reclaiming as quickly as possible. This active<br />&gt; writer may be enough to increase contention. If so, it also means it<br />&gt; stops kswapd making forward progress, leading to more direct reclaim and<br />&gt; more contention.<br />&gt;<br />&gt; It's not a perfect theory and assumes;<br />&gt;<br />&gt; 1. The writer is direct reclaiming<br />&gt; 2. The writer was previously failing to __remove_mapping<br />&gt; 3. The writer calling congestion_wait due to __remove_mapping failing<br />&gt;    was enough to allow kswapd or writeback to make enough progress to<br />&gt;    avoid contention<br />&gt; 4. The writer staying awake allocating and dirtying pages is keeping all<br />&gt;    the kswapd instances awake and writeback continually active and<br />&gt;    increasing the contention overall.<br />&gt;<br />&gt; If it was possible to trigger this problem in 4.7 then it would also be<br />&gt; worth checking 4.6. If 4.6 is immune, check that before and after commit<br />&gt; ede37713737.<br />&gt;<br />&gt; --<br />&gt; Mel Gorman<br />&gt; SUSE Labs<br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
