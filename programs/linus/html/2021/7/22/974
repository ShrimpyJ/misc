    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2021/7/21/524">First message in thread</a></li><li><a href="/lkml/2021/7/21/871">Linus Torvalds</a><ul><li><a href="/lkml/2021/7/22/517">Nikolay Borisov</a><ul><li class="origin"><a href="/lkml/2021/7/22/911">Linus Torvalds</a><ul><li><a href="/lkml/2021/7/22/911">Nikolay Borisov</a></li><li><a href="/lkml/2021/8/26/148">Nikolay Borisov</a></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Thu, 22 Jul 2021 09:40:43 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH] lib/string: Bring optimized memcmp from glibc</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Thu, Jul 22, 2021 at 4:28 AM Nikolay Borisov<br />&lt;n.borisov.lkml&#64;gmail.com&gt; wrote:<br />&gt;<br />&gt; This one also works, tested only on x86-64. Looking at the perf diff:<br />&gt;<br />&gt;     30.44%    -28.66%  [kernel.vmlinux]         [k] memcmp<br /><br />Ok.<br /><br />So the one that doesn't even bother to align is<br /><br />    30.44%    -29.38%  [kernel.vmlinux]         [k] memcmp<br /><br />and the one that aligns the first one is<br /><br />    30.44%    -28.66%  [kernel.vmlinux]         [k] memcmp<br /><br />and the difference between the two is basically in the noise:<br /><br />     1.05%     +0.72%  [kernel.vmlinux]     [k] memcmp<br /><br />but the first one does seem to be slightly better.<br /><br />&gt; Now on a more practical note, IIUC your 2nd version makes sense if the<br />&gt; cost of doing a one unaligned access in the loop body is offset by the<br />&gt; fact we are doing a native word-sized comparison, right?<br /><br />So honestly, the reason the first one seems to beat the second one is<br />that the cost of unaligned accesses on modern x86 is basically<br />epsilon.<br /><br />For all normal unaligned accesses there simply is no cost at all.<br />There is a _tiny_ cost when the unaligned access crosses a cacheline<br />access boundary (which on older CPU's is every 32 bytes, on modern<br />ones it's 64 bytes). And then there is another slightly bigger cost<br />when the unaligned access actually crosses a page boundary.<br /><br />But even those non-zero cost cases are basically in the noise, because<br />under most circumstances they will be hidden by any out-of-order<br />engine, and completely dwarfed by the _real_ costs which are branch<br />mispredicts and cache misses.<br /><br />So on the whole, unaligned accesses are basically no cost at all. You<br />almost have to have unusual code sequences for them to be even<br />measurable.<br /><br />So that second patch that aligns one of the sources is basically only<br />extra overhead for no real advantage. The cost of it is probably one<br />more branch mispredict, and possibly a cycle or two for the extra<br />instructions.<br /><br />Which is why the first one wins: it's simpler, and the extra work the<br />second one does is basically not worth it on x86. Plus I suspect your<br />test-case was all aligned anyway to begin with, so the extra work is<br />_doubly_ pointless.<br /><br />I suspect the second patch would be worthwhile if<br /><br /> (a) there really were a lot of strings that weren't aligned (likelihood: low)<br /><br /> (b) other microarchitectures that do worse on unaligned accesses -<br />some microarchitectures spend extra cycles on _any_ unaligned accesses<br />even if they don't cross cache access boundaries etc.<br /><br />and I can see (b) happening quite easily. You just won't see it on a<br />modern x86-64 setup.<br /><br />I suspect we should start with the first version. It's not only better<br />on x86, but it's simpler, and it's guarded by that<br /><br />    #ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS<br /><br />so it's fundamentally "safer" on architectures that are just horrible<br />about unaligned accesses.<br /><br />Now, admittedly I don't personally even care about such architectures,<br />and because we use "get_unaligned()", the compiler should always<br />generate code that doesn't absolutely suck for bad architectures, but<br />considering how long we've gone with the completely brainlessly simple<br />"byte at a time" version without anybody even noticing, I think a<br />minimal change is a better change.<br /><br />That said, I'm not convinced I want to apply even that minimal first<br />patch outside of the merge window.<br /><br />So would you mind reminding me about this patch the next merge window?<br />Unless there was some big extrernal reason why the performance of<br />memcmp() mattered to you so much (ie some user that actually noticed<br />and complained) and we really want to prioritize this..<br /><br />              Linus<br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
