    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2021/7/21/524">First message in thread</a></li><li><a href="/lkml/2021/7/21/799">Nikolay Borisov</a><ul><li><a href="/lkml/2021/7/21/846">Linus Torvalds</a><ul><li><a href="/lkml/2021/7/21/849">Linus Torvalds</a></li><li class="origin"><a href="/lkml/2021/7/22/517">Linus Torvalds</a><ul><li><a href="/lkml/2021/7/22/517">Nikolay Borisov</a><ul><li><a href="/lkml/2021/7/22/974">Linus Torvalds</a></li></ul></li></ul></li><li><a href="/lkml/2021/7/22/180">Nikolay Borisov</a></li><li><a href="/lkml/2021/7/23/492">David Laight</a></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2021/7/21/871/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><script type="text/javascript" src="//pagead2.googlesyndication.com/pagead/show_ads.js"></script><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Wed, 21 Jul 2021 12:26:29 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH] lib/string: Bring optimized memcmp from glibc</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Wed, Jul 21, 2021 at 11:45 AM Linus Torvalds<br />&lt;torvalds&#64;linux-foundation.org&gt; wrote:<br />&gt;<br />&gt; I can do the mutual alignment too, but I'd actually prefer to do it as<br />&gt; a separate patch, for when there are numbers for that.<br />&gt;<br />&gt; And I wouldn't do it as a byte-by-byte case, because that's just stupid.<br /><br />Here's that "try to align one of the pointers in order to avoid the<br />lots-of-unaligned case" patch.<br /><br />It's not quite as simple, and the generated assembly isn't quite as<br />obvious. But it still generates code that looks good, it's just that<br />the code to align the first pointer ends up being a bit harder to<br />read.<br /><br />And since it's a bit less obvious, the "this is probably buggy because<br />I didn't actually _test_ it" warning holds even more. But you can see<br />how much simpler the code still is than the horrendous glibc mess is.<br /><br />And I removed the "CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS" checking,<br />because now it should be at least *somewhat* reasonable even on<br />machines that have a complicated "get_unaligned()".<br /><br />But maybe I should have kept it. Only testing will tell.<br /><br />Again: UNTESTED GARBAGE ATTACHED. Be careful. But it migth work, and<br />ti generates something that looks superficially reasonable.<br /><br />Gcc again:<br /><br />        memcmp:<br />                cmpq    $7, %rdx<br />                jbe     .L56<br />                movq    (%rsi), %rax<br />                cmpq    %rax, (%rdi)<br />                je      .L61<br />        .L55:<br />                xorl    %ecx, %ecx<br />                jmp     .L58<br />        .L62:<br />                addq    $1, %rcx<br />                cmpq    %rcx, %rdx<br />                je      .L51<br />        .L58:<br />                movzbl  (%rdi,%rcx), %eax<br />                movzbl  (%rsi,%rcx), %r8d<br />                subl    %r8d, %eax<br />                je      .L62<br />        .L51:<br />                ret<br />        .L56:<br />                testq   %rdx, %rdx<br />                jne     .L55<br />                xorl    %eax, %eax<br />                ret<br />        .L61:<br />                movq    %rdi, %rcx<br />                movl    $8, %eax<br />                andl    $7, %ecx<br />                subq    %rcx, %rax<br />                leaq    -8(%rcx,%rdx), %rdx<br />                addq    %rax, %rdi<br />                addq    %rax, %rsi<br />                cmpq    $7, %rdx<br />                ja      .L57<br />                jmp     .L56<br />        .L63:<br />                subq    $8, %rdx<br />                addq    $8, %rdi<br />                addq    $8, %rsi<br />                cmpq    $7, %rdx<br />                jbe     .L56<br />        .L57:<br />                movq    (%rsi), %rax<br />                cmpq    %rax, (%rdi)<br />                je      .L63<br />                jmp     .L55<br /><br />but clang is similar (except clang isn't as eager to move basic blocks<br />around, so it's visually very different).<br /><br />Note no spills, no odd shifts for unaligned accesses, no garbage.<br /><br />Again: untested, so consider this a starting point rather than<br />anything good and proper.<br /><br />                   Linus<br /> lib/string.c | 26 ++++++++++++++++++++++++++<br /> 1 file changed, 26 insertions(+)<br /><br />diff --git a/lib/string.c b/lib/string.c<br />index 77bd0b1d3296..3eb390fc4f73 100644<br />--- a/lib/string.c<br />+++ b/lib/string.c<br />&#64;&#64; -29,6 +29,7 &#64;&#64;<br /> #include &lt;linux/errno.h&gt;<br /> #include &lt;linux/slab.h&gt;<br /> <br />+#include &lt;asm/unaligned.h&gt;<br /> #include &lt;asm/byteorder.h&gt;<br /> #include &lt;asm/word-at-a-time.h&gt;<br /> #include &lt;asm/page.h&gt;<br />&#64;&#64; -935,6 +936,31 &#64;&#64; __visible int memcmp(const void *cs, const void *ct, size_t count)<br /> 	const unsigned char *su1, *su2;<br /> 	int res = 0;<br /> <br />+	if (count &gt;= sizeof(unsigned long)) {<br />+		const unsigned long *u1 = cs;<br />+		const unsigned long *u2 = ct;<br />+		unsigned long offset;<br />+<br />+		if (get_unaligned(u1) != get_unaligned(u2))<br />+			goto bytewise;<br />+<br />+		/* Align 'u1' up */<br />+		offset = sizeof(*u1) - ((sizeof(*u1)-1) &amp; (unsigned long)(u1));<br />+		u1 = cs + offset;<br />+		u2 = ct + offset;<br />+		count -= offset;<br />+<br />+		while (count &gt;= sizeof(unsigned long)) {<br />+			if (*u1 != get_unaligned(u2))<br />+				break;<br />+			u1++;<br />+			u2++;<br />+			count -= sizeof(unsigned long);<br />+		}<br />+		cs = u1;<br />+		ct = u2;<br />+	}<br />+bytewise:<br /> 	for (su1 = cs, su2 = ct; 0 &lt; count; ++su1, ++su2, count--)<br /> 		if ((res = *su1 - *su2) != 0)<br /> 			break;</pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
