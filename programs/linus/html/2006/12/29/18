    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2006/12/16/164">First message in thread</a></li><li><a href="/lkml/2006/12/28/214">David Miller</a><ul><li><a href="/lkml/2006/12/28/251">Segher Boessenkool</a><ul><li class="origin"><a href="/lkml/2006/12/29/26">Linus Torvalds</a><ul><li><a href="/lkml/2006/12/29/26">Linus Torvalds</a><ul><li><a href="/lkml/2006/12/29/44">Linus Torvalds</a></li><li><a href="/lkml/2006/12/29/85">Theodore Tso</a></li></ul></li><li><a href="/lkml/2006/12/29/52">Ingo Molnar</a><ul><li><a href="/lkml/2007/1/2/52">Christoph Hellwig</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><script type="text/javascript" src="//pagead2.googlesyndication.com/pagead/show_ads.js"></script><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Thu, 28 Dec 2006 22:48:51 -0800 (PST)</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH] mm: fix page_mkclean_one</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody"><br /><br />On Fri, 29 Dec 2006, Segher Boessenkool wrote:<br />&gt;<br />&gt; &gt; I think what might be happening is that pdflush writes them out fine,<br />&gt; &gt; however we don't trap writes by the application _during_ that writeout.<br />&gt; <br />&gt; Yeah.  I believe that more exactly it happens if the very last<br />&gt; write to the page causes a writeback (due to dirty balancing)<br />&gt; while another writeback for the page is already happening.<br />&gt; <br />&gt; As usual in these cases, I have zero proof.<br /><br />I actually have proof to the contrary, ie I have traces that say "the <br />write was started" after the last write.<br /><br />And the VM layer in this area is actually fairly sane and civilized. It <br />has a bit that says "writeback in progress", and if that bit is set, it <br />simply _will_not_ start a new write. It even has various BUG_ON()'s to <br />that effect.<br /><br />So everything I have ever seen says that the VM layer is actually doing <br />everything right.<br /><br />&gt; It's the do_wp_page -&gt; balance_dirty_pages -&gt; generic_writepages<br />&gt; path for sure.  Maybe it's enough to change<br />&gt; <br />&gt;                         if (wbc-&gt;sync_mode != WB_SYNC_NONE)<br />&gt;                                 wait_on_page_writeback(page);<br />&gt; <br />&gt;                         if (PageWriteback(page) ||<br />&gt;                             !clear_page_dirty_for_io(page)) {<br />&gt;                                 unlock_page(page);<br />&gt;                                 continue;<br />&gt;                         }<br /><br />Notive how this one basically says:<br /><br /> - if it's under writeback, don't even clear the page dirty flag.<br /><br />Your suggested change:<br /><br />&gt;                         if (wbc-&gt;sync_mode != WB_SYNC_NONE)<br />&gt;                                 wait_on_page_writeback(page);<br />&gt; <br />&gt;                         if (PageWriteback(page)) {<br />&gt;                         	    redirty_page_for_writepage(wbc, page);<br /><br />makes no sense, because we simply never _did_ the "clear_page_dirty()" if <br />the thing was under writeback in the first place. That's how C <br />conditionals work.  So there's no reason to "redirty" it, because it <br />wasn't cleaned in the first place.<br /><br />I've double- and triple-checked the dirty bits, including having traces <br />that actually say that the IO was started (from a VM perspective) _after_ <br />the last write was done. The IO just didn't hit the disk.<br /><br />I'm personally fairly convinced that it's not a VM issue, but a "IO <br />issue". Either in a low-level filesystem or in some of the fs/buffer.c <br />helper routines.<br /><br />But I'd love to be proven wrong. <br /><br />I do have a few interesting details from the trace I haven't really <br />analyzed yet. Here's the trace for events on one of the pages that was <br />corrupted. Note how the events are numbered (there were 171640 events <br />total), so the thing you see is just a small set of events from the whole <br />big trace, but it's the ones that talk about _that_ particular page.<br /><br />I've grouped them so hat "consecutive" events group together. That just <br />means that no events on any other pages happened in between those events, <br />and it is usually a sign that it's really one single call-chain that <br />causes all the events.<br /><br />For example, for the first group of three events (44366-44368), it's the <br />page fault that brings in the page, and since it's a write-fault, it will <br />not only map the page, it will mark the page itself dirty and then also <br />set the TAG_DIRTY on the mapping. So the "group" is just really a result <br />of one single event happening, which causes several things to happen to <br />that page. That's exactly what you'd expect.<br /><br />Anyway, here is the list of events that page went through:<br /><br />   44366  PG 00000f6d: mm/memory.c:2254 mapping at b789fc54 (write)<br />   44367  PG 00000f6d: mm/page-writeback.c:817 setting dirty<br />   44368  PG 00000f6d: fs/buffer.c:738 setting TAG_DIRTY<br /><br />   64231  PG 00000f6d: mm/page-writeback.c:872 clean_for_io<br />   64232  PG 00000f6d: mm/rmap.c:451 cleaning PTE b789f000<br />   64233  PG 00000f6d: mm/page-writeback.c:914 set writeback<br />   64234  PG 00000f6d: mm/page-writeback.c:916 setting TAG_WRITEBACK<br />   64235  PG 00000f6d: mm/page-writeback.c:922 clearing TAG_DIRTY<br /><br />   67570  PG 00000f6d: mm/page-writeback.c:891 end writeback<br />   67571  PG 00000f6d: mm/page-writeback.c:893 clearing TAG_WRITEBACK<br /><br />   76705  PG 00000f6d: mm/page-writeback.c:817 setting dirty<br />   76706  PG 00000f6d: fs/buffer.c:725 dirtied buffers<br />   76707  PG 00000f6d: fs/buffer.c:738 setting TAG_DIRTY<br /><br />  105267  PG 00000f6d: mm/page-writeback.c:872 clean_for_io<br />  105268  PG 00000f6d: mm/rmap.c:451 cleaning PTE b789f000<br />  105269  PG 00000f6d: mm/page-writeback.c:914 set writeback<br />  105270  PG 00000f6d: mm/page-writeback.c:916 setting TAG_WRITEBACK<br />  105271  PG 00000f6d: mm/page-writeback.c:922 clearing TAG_DIRTY<br />  105272  PG 00000f6d: mm/page-writeback.c:891 end writeback<br />  105273  PG 00000f6d: mm/page-writeback.c:893 clearing TAG_WRITEBACK<br /><br />  128032  PG 00000f6d: mm/memory.c:670 unmapped at b789f000<br /><br />  132662  PG 00000f6d: mm/filemap.c:119 Removing page cache<br /><br />  148278  PG 00000f6d: mm/memory.c:2254 mapping at b789f000 (read)<br /><br />  166326  PG 00000f6d: mm/memory.c:670 unmapped at b789f000<br /><br />  171958  PG 00000f6d: mm/filemap.c:119 Removing page cache<br /><br />And notice that big grouping of seven events (105267-105273). The five <br />first events really _do_ make sense together: it's our page cleaning that <br />happens. But notice how the "end writeback" happens _immediately_.<br /><br />Here's another page cleaning event for the page that preceded that page, <br />and did _not_ get corrupted:<br /><br />  105262  PG 00000f6c: mm/page-writeback.c:872 clean_for_io<br />  105263  PG 00000f6c: mm/rmap.c:451 cleaning PTE b789e000<br />  105264  PG 00000f6c: mm/page-writeback.c:914 set writeback<br />  105265  PG 00000f6c: mm/page-writeback.c:916 setting TAG_WRITEBACK<br />  105266  PG 00000f6c: mm/page-writeback.c:922 clearing TAG_DIRTY<br /><br />  108437  PG 00000f6c: mm/page-writeback.c:891 end writeback<br />  108438  PG 00000f6c: mm/page-writeback.c:893 clearing TAG_WRITEBACK<br /><br />and this looks a lot more like what you'd expect: other thngs happened in <br />between the "clear dirty, set writeback" stage and the "end writeback" <br />stage. That's what you'd expect to see if there was actually overlapping <br />IO and/or work. <br /><br />(And notice that that _was_ what you saw even for the corrupted page for <br />the _first_ writeback: you saw the group-of-five that indicated a page <br />cleaning event had started, and then a group-of-two to indicate that the <br />writeback finished).<br /><br />So I find this kind of pattern really suspicious. We have a missing <br />writeout, and my traces show (I didn't analyze this _particular_ one <br />closely, but I did the previous trace for another page that I posted) that <br />the writeback was actually started after the write that went missing was <br />done. AND I have this trace that seems to show that the writeback <br />basically completed immediately, with no other work in between.<br /><br />That to me says: "somebody didn't actually write out out". The VM layer <br />asked the filesystem to do the write, but the filesystem just didn't do <br />it. I personally think it's because some buffer-head BH_dirty bit got <br />scrogged, but it could be some event that makes the filesystem simply not <br />do the IO because it thinks the "disk queues are too full", so it just <br />says "IO completed", without actually doing anything at all.<br /><br />Now, the fact that it apparently happens for all of ext2, ext3 <br />and reiserfs (but NOT apparently with "data=writeback"), makes me suspect <br />that there is some common interaction, and that it's somehow BH-related <br />(they all share much of the buffer head infrastructure). So it doesn't <br />look like it's just a bug in one random filesystem, I think it's a bug in <br />some buffer-head infrastructure/helper function.<br /><br />So I don't think it's "core VM". I don't think it's the "page cache". I <br />think we handle the dirty state correctly at that level.<br /><br />It looks more like "buffer cache" or "filesystem" to me by now.<br /><br />(Btw, don't get me wrong - the above sequence numbers are in no way <br />*proof* of anything. You could get big groups for one page just because <br />something ended up being synchronous. I'll add some timestamps to my <br />traces to make it easier to see where there was real IO going on and where <br />there wasn't).<br /><br />		Linus<br />-<br />To unsubscribe from this list: send the line "unsubscribe linux-kernel" in<br />the body of a message to majordomo&#64;vger.kernel.org<br />More majordomo info at  <a href="http://vger.kernel.org/majordomo-info.html">http://vger.kernel.org/majordomo-info.html</a><br />Please read the FAQ at  <a href="http://www.tux.org/lkml/">http://www.tux.org/lkml/</a><br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
