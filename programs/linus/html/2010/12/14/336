    </div></td><td width="32"> </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2010/12/14/51">First message in thread</a></li><li><a href="/lkml/2010/12/14/51">Borislav Petkov</a><ul><li class="origin"><a href="/lkml/2010/12/14/440">Linus Torvalds</a><ul><li><a href="/lkml/2010/12/14/440">Borislav Petkov</a></li><li><a href="/lkml/2010/12/15/64">Peter Zijlstra</a></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Tue, 14 Dec 2010 12:29:52 -0800</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: BUG: sleeping function called from invalid context at arch/x86/mm/fault.c:1081</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Tue, Dec 14, 2010 at 1:40 AM, Borislav Petkov &lt;bp&#64;alien8.de&gt; wrote:<br />&gt;<br />&gt; The oops below happened today with v2.6.37-rc5-62-g6313e3c with the<br />&gt; following hpet oneline fix which should be unrelated (tglx has an<br />&gt; equivalent one in -tip/urgent already):<br /><br />The "sleeping function called from invalid context" part is<br />uninteresting - it's just a result of the real bug, which is the oops.<br />The oops gets printed out later, because the might_sleep() test just<br />happens to trigger first in the page faulting path.<br /><br />So ignore the early part of the dmesg, the only thing relevant is the<br />kernel paging request in __lock_acquire.<br /><br />Of course, that one is then followed rather closely by a GP fault in<br />the radeon driver, which I don't know is real or not. It could easily<br />be some bad interaction with the kmsg_dump() thing (a result of the<br />first page fault) interacting badly with the X server getting killed<br />by that bogus page fault. So it's hard to tell whether the GP fault is<br />real and maybe relevant for the discussion, or whether it's a result<br />of the earlier oops.<br /><br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144509] BUG: unable to handle kernel paging request at 00000110850fc085<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144512] IP: [&lt;ffffffff8106d1da&gt;] __lock_acquire+0x19d/0x173f<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144516] PGD 0<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144518] Oops: 0000 [#1] PREEMPT SMP<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144521] last sysfs file: /sys/devices/system/cpu/cpu3/cpufreq/scaling_cur_freq<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144524] CPU 1<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144526] Modules linked in: tun ip6table_filter ip6_tables iptable_filter ip_tables x_tables powernow_k8 mperf cpufreq_ondemand cpufreq_powersave cpufreq_userspace cpufreq_conservative cpufreq_stats freq_table binfmt_misc kvm_amd kvm fuse ipv6 vfat fat 8250_pnp 8250 serial_core ohci_hcd edac_core k10temp<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144548]<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144551] Pid: 1930, comm: Xorg Not tainted 2.6.37-rc5-00063-g779049f #1 M3A78 PRO/System Product Name<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144553] RIP: 0010:[&lt;ffffffff8106d1da&gt;]  [&lt;ffffffff8106d1da&gt;] __lock_acquire+0x19d/0x173f<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144557] RSP: 0000:ffff88022d1b3cb8  EFLAGS: 00013087<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144559] RAX: ffffffff819bb000 RBX: ffff88022d09da00 RCX: ffffffff821c6b40<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144561] RDX: 00000110850fc085 RSI: 0000000000000000 RDI: 8000000000000000<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144563] RBP: ffff88022d1b3d88 R08: 0000000000000002 R09: 0000000000000000<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144564] R10: ffffffff8124f16c R11: 0000000000001128 R12: ffff88022f445b90<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144566] R13: 0000000000000000 R14: 00000110850fc085 R15: 0000000000000002<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144569] FS:  00007f2fe2941700(0000) GS:ffff8800cf000000(0000) knlGS:00000000f6a4c740<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144571] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144572] CR2: 00000110850fc085 CR3: 000000022b965000 CR4: 00000000000006e0<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144574] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144576] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144578] Process Xorg (pid: 1930, threadinfo ffff88022d1b2000, task ffff88022f445b90)<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144580] Stack:<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144581]  0000000000000046 0000000000000001 ffff88022d1b3ce8 0000000081032b75<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144585]  ffff880200000001 0000000000000001 ffff88022d1b3cf8 ffffffff81069d4a<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144589]  ffff88022d1b3d18 ffff88022f445b90 ffff88022f445b90 ffffffff8144c1ea<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144593] Call Trace:<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144596]  [&lt;ffffffff81069d4a&gt;] ? put_lock_stats+0xe/0x29<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144599]  [&lt;ffffffff8144c1ea&gt;] ? __mutex_unlock_slowpath+0x105/0x129<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144602]  [&lt;ffffffff8106b29d&gt;] ? trace_hardirqs_on+0xd/0xf<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144605]  [&lt;ffffffff81021e19&gt;] ? do_page_fault+0x170/0x3a7<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144608]  [&lt;ffffffff8106e8c9&gt;] lock_acquire+0x14d/0x192<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144610]  [&lt;ffffffff81021e19&gt;] ? do_page_fault+0x170/0x3a7<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144613]  [&lt;ffffffff8124f16c&gt;] ? radeon_cp_idle+0x0/0xa6<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144616]  [&lt;ffffffff8105ea37&gt;] down_read_trylock+0x56/0x5f<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144619]  [&lt;ffffffff81021e19&gt;] ? do_page_fault+0x170/0x3a7<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144621]  [&lt;ffffffff81021e19&gt;] do_page_fault+0x170/0x3a7<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144624]  [&lt;ffffffff810f6159&gt;] ? do_vfs_ioctl+0x4d0/0x51f<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144627]  [&lt;ffffffff8144f403&gt;] ? error_sti+0x5/0x6<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144630]  [&lt;ffffffff8144dbfb&gt;] ? trace_hardirqs_off_thunk+0x3a/0x3c<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144632]  [&lt;ffffffff8144f21f&gt;] page_fault+0x1f/0x30<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144634] Code: 01 00 00 74 33 80 3d 25 1b c7 00 01 74 2a be a6 02 00 00 48 c7 c7 d1 8b 5e 81 e8 af dc fc ff c6 05 0b 1b c7 00 01 eb 10 49 89 d6 &lt;49&gt; 8b 16 49 39 ce 0f 18 0a 75 bc eb 09 4d 85 f6 0f 85 2e 03 00<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144671] RIP  [&lt;ffffffff8106d1da&gt;] __lock_acquire+0x19d/0x173f<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144674]  RSP &lt;ffff88022d1b3cb8&gt;<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144676] CR2: 00000110850fc085<br />&gt; Dec 14 10:17:26 liondog kernel: [12312.144678] ---[ end trace b215ad3559f4bac0 ]---<br /><br />That code disassembles to<br /><br /><br />   0:	74 33                	je     0x35<br />   2:	80 3d 25 1b c7 00 01 	cmpb   $0x1,0xc71b25(%rip)        # 0xc71b2e<br />   9:	74 2a                	je     0x35<br />   b:	be a6 02 00 00       	mov    $0x2a6,%esi<br />  10:	48 c7 c7 d1 8b 5e 81 	mov    $0xffffffff815e8bd1,%rdi<br />  17:	e8 af dc fc ff       	callq  0xfffffffffffcdccb<br />  1c:	c6 05 0b 1b c7 00 01 	movb   $0x1,0xc71b0b(%rip)        # 0xc71b2e<br />  23:	eb 10                	jmp    0x35<br />  25:	49 89 d6             	mov    %rdx,%r14<br />  28:*	49 8b 16             	mov    (%r14),%rdx     &lt;-- trapping instruction<br />  2b:	49 39 ce             	cmp    %rcx,%r14<br />  2e:	0f 18 0a             	prefetcht0 (%rdx)<br />  31:	75 bc                	jne    0xffffffffffffffef<br />  33:	eb 09                	jmp    0x3e<br /><br />which I think matches up with this:<br /><br />        movq    (%rdx), %rsi    # class_293-&gt;hash_entry.next, D.31652<br />        cmpq    %rcx, %rdx      # hash_head, class<br />        prefetcht0      (%rsi)  # D.31652<br />        jne     .L449   #,<br />        jmp     .L560   #<br /><br />which looks like one of the "list_for_each_entry()" uses of that<br />hash_head. In fact, building lockdep.lst and matching against that<br />0x2a6 constant (it's a warn_slowpath_null thing and is the line<br />number), we can pinpoint exactly which one it seems to be. Afaik, it<br />seems to be this code in look_up_lock_class:<br /><br />        /*<br />         * We can walk the hash lockfree, because the hash only<br />         * grows, and we are careful when adding entries to the end:<br />         */<br />        list_for_each_entry(class, hash_head, hash_entry) {<br />                if (class-&gt;key == key) {<br />                        WARN_ON_ONCE(class-&gt;name != lock-&gt;name);<br />                        return class;<br />                }<br />        }<br /><br />Maybe the RCU list use in lockdep isn't entirely safe?<br /><br />Added Peter and Ingo to the cc. It does look like some lockdep race.<br /><br />Or then it's some general memory corruption, of course. That just<br />happened to hit the lockdep data structures. That would explain your<br />subsequent radeon GP fault too.<br /><br />                                Linus<br />--<br />To unsubscribe from this list: send the line "unsubscribe linux-kernel" in<br />the body of a message to majordomo&#64;vger.kernel.org<br />More majordomo info at  <a href="http://vger.kernel.org/majordomo-info.html">http://vger.kernel.org/majordomo-info.html</a><br />Please read the FAQ at  <a href="http://www.tux.org/lkml/">http://www.tux.org/lkml/</a><br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
