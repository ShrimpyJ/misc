    </div></td><td width="32"> </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2010/7/14/204">First message in thread</a></li><li><a href="/lkml/2010/7/14/465">Linus Torvalds</a><ul><li><a href="/lkml/2010/7/14/472">Andi Kleen</a><ul><li><a href="/lkml/2010/7/14/478">Linus Torvalds</a></li></ul></li><li><a href="/lkml/2010/7/15/119">Frederic Weisbecker</a><ul><li><a href="/lkml/2010/7/15/129">Andi Kleen</a><ul><li><a href="/lkml/2010/7/16/146">Frederic Weisbecker</a></li></ul></li><li><a href="/lkml/2010/7/15/130">Steven Rostedt</a><ul><li><a href="/lkml/2010/7/16/128">Frederic Weisbecker</a><ul><li><a href="/lkml/2010/7/16/148">Steven Rostedt</a></li></ul></li></ul></li><li class="origin"><a href="/lkml/2010/7/15/162">Linus Torvalds</a><ul><li><a href="/lkml/2010/7/15/162">Linus Torvalds</a></li><li><a href="/lkml/2010/7/16/150">Frederic Weisbecker</a><ul><li><a href="/lkml/2010/7/16/188">Steven Rostedt</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Thu, 15 Jul 2010 07:51:55 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [patch 1/2] x86_64 page fault NMI-safe</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Thu, Jul 15, 2010 at 7:11 AM, Frederic Weisbecker &lt;fweisbec&#64;gmail.com&gt; wrote:<br />&gt; On Wed, Jul 14, 2010 at 03:56:43PM -0700, Linus Torvalds wrote:<br />&gt;&gt; You can:<br />&gt;&gt;<br />&gt;&gt;  - make sure that you only ever use _one_ single top-level entry for<br />&gt;&gt; all vmalloc issues, and can make sure that all processes are created<br />&gt;&gt; with that static entry filled in. This is optimal, but it just doesn't<br />&gt;&gt; work on all architectures (eg on 32-bit x86, it would limit the<br />&gt;&gt; vmalloc space to 4MB in non-PAE, whatever)<br />&gt;<br />&gt; But then, even if you ensure that, don't we need to also fill lower level<br />&gt; entries for the new mapping.<br /><br />Yes, but now they are all mapped by the one *shared* top-level entry.<br /><br />Think about it.<br /><br />[ Time passes ]<br /><br />End result: if you can map the whole vmalloc area with a single<br />top-level entry that is shared by all processes, and can then just<br />fill in the lower levels when doing actual allocations, it means that<br />all processes will automatically get the entries added, and do not<br />need any fixups.<br /><br />In other words, the page tables will be automatically correct and<br />filled in for everybody - without having to traverse any lists,<br />without any extra locking, and without any races. So this is efficient<br />and simple, and never needs any faulting to fill in page tables later<br />on.<br /><br />(Side note: "single top-level entry" could equally well be "multiple<br />preallocated entries covering the whole region": the important part is<br />not really the "single entry", but the "preallocated and filled into<br />every page directory from the start" part)<br /><br />&gt; Also, why is this a worry for vmalloc but not for kmalloc? Don't we also<br />&gt; risk to add a new memory mapping for new memory allocated with kmalloc?<br /><br />No. The kmalloc space is all in the 1:1 kernel mapping, and is always<br />mapped. Even with PAGEALLOC_DEBUG, it's always mapped at the top<br />level, and even if a particular page is unmapped/remapped for<br />debugging, it is done so in the shared kernel page tables (which ends<br />up being the above trivial case - there is just a single set of page<br />directory entries that are shared by everybody).<br /><br />&gt;&gt;  - at vmalloc time, when adding a new page directory entry, walk all<br />&gt;&gt; the tens of thousands of existing page tables under a lock that<br />&gt;&gt; guarantees that we don't add any new ones (ie it will lock out fork())<br />&gt;&gt; and add the required pgd entry to them.<br />&gt;&gt;<br />&gt;&gt;  - or just take the fault and do the "fill the page tables" on demand.<br />&gt;&gt;<br />&gt;&gt; Quite frankly, most of the time it's probably better to make that last<br />&gt;&gt; choice (unless your hardware makes it easy to make the first choice,<br />&gt;&gt; which is obviously simplest for everybody). It makes it _much_ cheaper<br />&gt;&gt; to do vmalloc. It also avoids that nasty latency issue. And it's just<br />&gt;&gt; simpler too, and has no interesting locking issues with how/when you<br />&gt;&gt; expose the page tables in fork() etc.<br />&gt;&gt;<br />&gt;&gt; So the only downside is that you do end up taking a fault in the<br />&gt;&gt; (rare) case where you have a newly created task that didn't get an<br />&gt;&gt; even newer vmalloc entry.<br />&gt;<br />&gt; But then how did the previous tasks get this new mapping? You said<br />&gt; we don't walk through every process page tables for vmalloc.<br /><br />We always add the mapping to the "init_mm" page tables when it is<br />created (just a single mm), and when fork creates a new page table, it<br />will always copy the kernel mapping parts from the old one. So the<br />_common_ case is that all normal mappings are already set up in page<br />tables, including newly created page tables.<br /><br />The uncommon case is when there is a new page table created _and_ a<br />new vmalloc mapping, and the race that happens between those events.<br />Whent hat new page table is then later used (and it can be _much_<br />later, of course: we're now talking process scheduling, so IO delays<br />etc are relevant), it won't necessarily have the page table entries<br />for vmalloc stuff that was created since the page tables were created.<br />So we fill _those_ in dynamically.<br /><br />But vmalloc mappings should be reasonably rare, and the actual "fill<br />them in" cases are much rarer still (since we fill them in page<br />directory entries at a time: so even if you make a lot of vmalloc()<br />calls, we only _fill_ at most once per page directory entry, which is<br />usually a pretty big chunk). On 32-bit x86, for example, we'd fill<br />once every 4MB (or 2MB if PAE), and you cannot have a lot of vmalloc<br />mappings that large (since the VM space is limited).<br /><br />So the cost of filling things in is basically zero, because it happens<br />so seldom. And by _allowing_ things to be done lazily, we avoid all<br />the locking costs, and all the costs of traversing every single<br />possible mm (and there can be many many thousands of those).<br /><br />&gt; I would understand this race if we were to walk on every processes page<br />&gt; tables and add the new mapping on them, but we missed one new task that<br />&gt; forked or so, because we didn't lock (or just rcu).<br /><br />.. and how do you keep track of which tasks you missed? And no, it's<br />not just the new tasks - you have old tasks that have their page<br />tables built up too, but need to be updated. They may never need the<br />mapping since they may be sleeping and never using the driver or data<br />structures that created it (in fact, that's a common case), so filling<br />them would be pointless. But if we don't do the lazy fill, we'd have<br />to fill them all, because WE DO NOT KNOW.<br /><br />&gt; So the parts of the problem I don't understand are:<br />&gt;<br />&gt; - why don't we have this problem with kmalloc() ?<br /><br />Hopefully clarified.<br /><br />&gt; - did I understand well the race that makes the fault necessary,<br />&gt;  ie: we walk the tasklist lockless, add the new mapping if<br />&gt;  not present, but we might miss a task lately forked, but<br />&gt;  the fault will fix that.<br /><br />But the _fundamental_ issue is that we do not want to walk the<br />tasklist (or the mm_list) AT ALL. It's a f*cking waste of time. It's a<br />long list, and nobody cares. In many cases it won't be needed.<br /><br />The lazy algorithm is _better_. It's way simpler (we take nested<br />faults all the time in the kernel, and it's a particularly _easy_ page<br />fault to handle with no IO or no locking needed), and it does less<br />work. It really boils down to that.<br /><br />So it's not the lazy page table fill that is the problem. Never has<br />been. We've been doing the lazy fill for a long time, and it was<br />simple and useful way back when.<br /><br />The problem has always been NMI, and nothing else. NMI's are nasty,<br />and the x86 NMI blocking is insane and crazy.<br /><br />Which is why I'm so adamant that this should be fixed in the NMI code,<br />and we should _not_ talk about trying to screw up other, totally<br />unrelated, code. The lazy fill really was never the problem.<br /><br />                        Linus<br />--<br />To unsubscribe from this list: send the line "unsubscribe linux-kernel" in<br />the body of a message to majordomo&#64;vger.kernel.org<br />More majordomo info at  <a href="http://vger.kernel.org/majordomo-info.html">http://vger.kernel.org/majordomo-info.html</a><br />Please read the FAQ at  <a href="http://www.tux.org/lkml/">http://www.tux.org/lkml/</a><br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
