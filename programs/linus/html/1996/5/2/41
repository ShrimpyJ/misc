    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/1996/5/1/44">First message in thread</a></li><li><a href="/lkml/1996/5/1/44">Paul Gortmaker</a><ul><li><a href="/lkml/1996/5/1/121">Alan Cox</a><ul><li class="origin"><a href="/lkml/1996/5/2/112">Linus Torvalds</a><ul><li><a href="/lkml/1996/5/2/112">Paul Gortmaker</a><ul><li><a href="/lkml/1996/5/2/115">Alan Cox</a></li><li><a href="/lkml/1996/5/3/59"> nelson&#64;crynwr ...</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><script type="text/javascript" src="//pagead2.googlesyndication.com/pagead/show_ads.js"></script><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Thu, 2 May 1996 07:06:42 +0300 (EET DST)</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: Tx TCP rates down &gt; 20% - A report.</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody"><br /><br />On Wed, 1 May 1996, Alan Cox wrote:<br />&gt; <br />&gt; &gt; Tx speeds are way down in 1.3.97 as compared to v1.2.13 -- in fact there<br />&gt; &gt; appears to be a "glass-ceiling" effect, where v1.3.97 can't Tx any <br />&gt; &gt; better than sending 820 -&gt; 830kB/s even with decent hardware. However <br />&gt; <br />&gt; Its probably generating some stupid retransmits like the current<br />&gt; hacked around code does on PPP links.  The other thing that may do it<br />&gt; is windows filling because of ack problems.<br /><br />I don't think it's unnecessary packets - I've been tcpdumping linux on <br />ethernet, and it looks generally clean (there seems to be some silly <br />problem with zero-window probing, but that only shows up if we end up <br />doign a probe in the first place, and if that happens it means that the <br />receiver is so slow that the whole performance thing doesn't really enter <br />the picture in the first place ;-)<br /><br />&gt; &gt; Rx with v1.3.97 is good, as I can consistently jam &gt;1100kB/s into a <br />&gt; &gt; wd8013 and about 1040kB/s into a soft-config Winbond based ne2000 when<br />&gt; &gt; Tx'ing from a 1.2.13 kernel.<br />&gt; <br />&gt; Thats nice to know.<br /><br />I get 900+kB with a 3c509, and that's over a bridge to a Sun machine <br />(only early in the morning - it goes down to 500-600kB during normal <br />working hours when the network fills up).<br /><br />There _does_ seem to be some bad effects with the drivers under some<br />circumstances, though. Notably, the "tbusy" handling in the ethernet driver<br />interface looks like it's pretty broken - it's used for two things: (a)<br />serializing the ethernet driver (which was the original reason for it, but is<br />unnecessary these days when the network layer makes sure it's all serialized<br />anyway) and (b) as a send throttle to tell the network layer that the <br />card is busy.<br /><br />The (b) case is the only thing it does any more, and I suspect it is also <br />the thing that makes you see bad performance. The TCP side is much faster <br />in the later 1.3.x kernels, and the network cards can no longer keep up <br />so the throttle is essentially in effect _all_ the time. What you see is <br />probably due to:<br /><br /> - TCP layer has a few packets queued up, sends one to the network driver<br /> - network driver puts out the packet, sets tbusy<br /> - TCP layer sees tbusy, and doesn't send any more<br /> - network driver gets a "tx complete interrupt" and does a callback to <br />   net layer with mark_bh(NET_BH), and the cycle starts up again..<br /><br />Essentially, the tbusy thing may result in a _single_ packet being sent <br />and then we go away and come back only next time around. Broken, broken, <br />broken. I haven't touched it because I don't know the network drivers <br />well enough.<br /><br />In short, the problem is _not_ in the network layer. <br /><br />The reason 1.2.13 does better is probably two-fold<br /> - the TCP layer wasn't very fast, so it was entirely possible that the <br />   driver got the packet send out quickly enough that there wasn't much <br />   of a throttling effect.<br /> - the "net_bh()" handler used to do multiple calls to "dev_transmit()". <br />   You still see that in net/core/dev.t - look at the things that are <br />   #ifdef'ed with "XMIT_EVERY" and "XMIT_AFTER".<br /><br />You can probably get better performance by enablign XMIT_EVERY and <br />XMIT_AFTER, but that is not the fix to the problem - they are probably <br />there exactly because somebody noticed that they improved performance and <br />that was the "easy fix" instead of the _real_ fix which is to make the <br />network drivers a bit more streamlined and maybe have a one-packet <br />send-queue inside the driver or whatever.<br /><br />&gt; &gt; It is also worth noting that similar Tx rates were obtained by ftp'ing<br />&gt; &gt; a large file (cached) to /dev/null which indicates that the TTCP <br />&gt; &gt; measurements are not subjet to some huge systematic error.<br />&gt; <br />&gt; Does changing the large windows option affect it. Can a third box log the<br />&gt; link and see what kind of mess you see in duplicates. How does 1.3.59 compare.<br />&gt; <br />&gt; We need to pin this down - its I think got to be extra frames or window/ack<br />&gt; problems. The base performance goes well over 40Mbit/second on 100baseT<br /><br />As I said, I'm more-or-less certain that the problem is _not_ the packets <br />on the wire, but the drivers. They need to be updated a bit - the 3c509 <br />driver gets reasonable performance because it has a internal packet queue <br />on the card, so the tbusy thing works correctly (instead of throttling <br />every packet it throttles maybe every ten packets, which is roughly what <br />we'd want.<br /><br />		Linus<br /><br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
