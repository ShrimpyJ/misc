    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2020/12/9/769">First message in thread</a></li><li><a href="/lkml/2020/12/26/130">Hugh Dickins</a><ul><li><a href="/lkml/2020/12/26/138">Hugh Dickins</a><ul><li class="origin"><a href="/lkml/2020/12/27/140">Linus Torvalds</a><ul><li><a href="/lkml/2020/12/27/140">Damian Tometzki</a><ul><li><a href="/lkml/2020/12/27/173">Hugh Dickins</a></li></ul></li><li><a href="/lkml/2020/12/27/178">"Kirill A. Shutemov"</a><ul><li><a href="/lkml/2020/12/27/193">Linus Torvalds</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2020/12/27/130/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Sun, 27 Dec 2020 11:38:22 -0800</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH 1/2] mm: Allow architectures to request 'old' entries when prefaulting</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Sat, Dec 26, 2020 at 6:38 PM Hugh Dickins &lt;hughd&#64;google.com&gt; wrote:<br />&gt;<br />&gt; This patch (like its antecedents) moves the pte_unmap_unlock() from<br />&gt; after do_fault_around()'s "check if the page fault is solved" into<br />&gt; filemap_map_pages() itself (which apparently does not NULLify vmf-&gt;pte<br />&gt; after unmapping it, which is poor, but good for revealing this issue).<br />&gt; That looks cleaner, but of course there was a very good reason for its<br />&gt; original positioning.<br /><br />Good catch.<br /><br />&gt; Maybe you want to change the -&gt;map_pages prototype, to pass down the<br />&gt; requested address too, so that it can report whether the requested<br />&gt; address was resolved or not.  Or it could be left to __do_fault(),<br />&gt; or even to a repeated fault; but those would be less efficient.<br /><br />Let's keep the old really odd "let's unlock in the caller" for now,<br />and minimize the changes.<br /><br />Adding a big big comment at the end of filemap_map_pages() to note the<br />odd delayed page table unlocking.<br /><br />Here's an updated patch that combines Kirill's original patch, his<br />additional incremental patch, and the fix for the pte lock oddity into<br />one thing.<br /><br />Does this finally pass your testing?<br /><br />               Linus<br />From 4d221d934d112aa40c3f4978460be098fc9ce831 Mon Sep 17 00:00:00 2001<br />From: "Kirill A. Shutemov" &lt;kirill.shutemov&#64;linux.intel.com&gt;<br />Date: Sat, 19 Dec 2020 15:19:23 +0300<br />Subject: [PATCH] mm: Cleanup faultaround and finish_fault() codepaths<br /><br />alloc_set_pte() has two users with different requirements: in the<br />faultaround code, it called from an atomic context and PTE page table<br />has to be preallocated. finish_fault() can sleep and allocate page table<br />as needed.<br /><br />PTL locking rules are also strange, hard to follow and overkill for<br />finish_fault().<br /><br />Let's untangle the mess. alloc_set_pte() has gone now. All locking is<br />explicit.<br /><br />The price is some code duplication to handle huge pages in faultaround<br />path, but it should be fine, having overall improvement in readability.<br /><br />Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov&#64;linux.intel.com&gt;<br />Signed-off-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />---<br /> include/linux/mm.h      |   8 +-<br /> include/linux/pgtable.h |  11 +++<br /> mm/filemap.c            | 168 ++++++++++++++++++++++++++++++----------<br /> mm/memory.c             | 161 +++++++++++---------------------------<br /> 4 files changed, 192 insertions(+), 156 deletions(-)<br /><br />diff --git a/include/linux/mm.h b/include/linux/mm.h<br />index 5299b90a6c40..c0643a0ad5ff 100644<br />--- a/include/linux/mm.h<br />+++ b/include/linux/mm.h<br />&#64;&#64; -535,8 +535,8 &#64;&#64; struct vm_fault {<br /> 					 * is not NULL, otherwise pmd.<br /> 					 */<br /> 	pgtable_t prealloc_pte;		/* Pre-allocated pte page table.<br />-					 * vm_ops-&gt;map_pages() calls<br />-					 * alloc_set_pte() from atomic context.<br />+					 * vm_ops-&gt;map_pages() sets up a page<br />+					 * table from from atomic context.<br /> 					 * do_fault_around() pre-allocates<br /> 					 * page table to avoid allocation from<br /> 					 * atomic context.<br />&#64;&#64; -981,7 +981,9 &#64;&#64; static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)<br /> 	return pte;<br /> }<br /> <br />-vm_fault_t alloc_set_pte(struct vm_fault *vmf, struct page *page);<br />+vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page);<br />+void do_set_pte(struct vm_fault *vmf, struct page *page);<br />+<br /> vm_fault_t finish_fault(struct vm_fault *vmf);<br /> vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);<br /> #endif<br />diff --git a/include/linux/pgtable.h b/include/linux/pgtable.h<br />index 8fcdfa52eb4b..36eb748f3c97 100644<br />--- a/include/linux/pgtable.h<br />+++ b/include/linux/pgtable.h<br />&#64;&#64; -1314,6 +1314,17 &#64;&#64; static inline int pmd_trans_unstable(pmd_t *pmd)<br /> #endif<br /> }<br /> <br />+/*<br />+ * the ordering of these checks is important for pmds with _page_devmap set.<br />+ * if we check pmd_trans_unstable() first we will trip the bad_pmd() check<br />+ * inside of pmd_none_or_trans_huge_or_clear_bad(). this will end up correctly<br />+ * returning 1 but not before it spams dmesg with the pmd_clear_bad() output.<br />+ */<br />+static inline int pmd_devmap_trans_unstable(pmd_t *pmd)<br />+{<br />+	return pmd_devmap(*pmd) || pmd_trans_unstable(pmd);<br />+}<br />+<br /> #ifndef CONFIG_NUMA_BALANCING<br /> /*<br />  * Technically a PTE can be PROTNONE even when not doing NUMA balancing but<br />diff --git a/mm/filemap.c b/mm/filemap.c<br />index 5c9d564317a5..dbc2eda92a53 100644<br />--- a/mm/filemap.c<br />+++ b/mm/filemap.c<br />&#64;&#64; -42,6 +42,7 &#64;&#64;<br /> #include &lt;linux/psi.h&gt;<br /> #include &lt;linux/ramfs.h&gt;<br /> #include &lt;linux/page_idle.h&gt;<br />+#include &lt;asm/pgalloc.h&gt;<br /> #include "internal.h"<br /> <br /> #define CREATE_TRACE_POINTS<br />&#64;&#64; -2911,50 +2912,133 &#64;&#64; vm_fault_t filemap_fault(struct vm_fault *vmf)<br /> }<br /> EXPORT_SYMBOL(filemap_fault);<br /> <br />+static bool filemap_map_pmd(struct vm_fault *vmf, struct page *page)<br />+{<br />+	struct mm_struct *mm = vmf-&gt;vma-&gt;vm_mm;<br />+<br />+	/* Huge page is mapped? No need to proceed. */<br />+	if (pmd_trans_huge(*vmf-&gt;pmd)) {<br />+		unlock_page(page);<br />+		put_page(page);<br />+		return true;<br />+	}<br />+<br />+	if (pmd_none(*vmf-&gt;pmd) &amp;&amp; PageTransHuge(page)) {<br />+	    vm_fault_t ret = do_set_pmd(vmf, page);<br />+	    if (!ret) {<br />+		    /* The page is mapped successfully, reference consumed. */<br />+		    unlock_page(page);<br />+		    return true;<br />+	    }<br />+	}<br />+<br />+	if (pmd_none(*vmf-&gt;pmd)) {<br />+		vmf-&gt;ptl = pmd_lock(mm, vmf-&gt;pmd);<br />+		if (likely(pmd_none(*vmf-&gt;pmd))) {<br />+			mm_inc_nr_ptes(mm);<br />+			pmd_populate(mm, vmf-&gt;pmd, vmf-&gt;prealloc_pte);<br />+			vmf-&gt;prealloc_pte = NULL;<br />+		}<br />+		spin_unlock(vmf-&gt;ptl);<br />+	}<br />+<br />+	/* See comment in handle_pte_fault() */<br />+	if (pmd_devmap_trans_unstable(vmf-&gt;pmd)) {<br />+		unlock_page(page);<br />+		put_page(page);<br />+		return true;<br />+	}<br />+<br />+	return false;<br />+}<br />+<br />+static struct page *next_uptodate_page(struct page *page, struct vm_fault *vmf,<br />+				     struct xa_state *xas, pgoff_t end_pgoff)<br />+{<br />+	struct address_space *mapping = vmf-&gt;vma-&gt;vm_file-&gt;f_mapping;<br />+	unsigned long max_idx;<br />+<br />+	do {<br />+		if (!page)<br />+			return NULL;<br />+		if (xas_retry(xas, page))<br />+			continue;<br />+		if (xa_is_value(page))<br />+			continue;<br />+		if (PageLocked(page))<br />+			continue;<br />+		if (!page_cache_get_speculative(page))<br />+			continue;<br />+		/* Has the page moved or been split? */<br />+		if (unlikely(page != xas_reload(xas)))<br />+			goto skip;<br />+		if (!PageUptodate(page) || PageReadahead(page))<br />+			goto skip;<br />+		if (PageHWPoison(page))<br />+			goto skip;<br />+		if (!trylock_page(page))<br />+			goto skip;<br />+		if (page-&gt;mapping != mapping)<br />+			goto unlock;<br />+		if (!PageUptodate(page))<br />+			goto unlock;<br />+		max_idx = DIV_ROUND_UP(i_size_read(mapping-&gt;host), PAGE_SIZE);<br />+		if (xas-&gt;xa_index &gt;= max_idx)<br />+			goto unlock;<br />+		return page;<br />+unlock:<br />+		unlock_page(page);<br />+skip:<br />+		put_page(page);<br />+	} while ((page = xas_next_entry(xas, end_pgoff)) != NULL);<br />+<br />+	return NULL;<br />+}<br />+<br />+static inline struct page *first_map_page(struct vm_fault *vmf,<br />+					  struct xa_state *xas,<br />+					  pgoff_t end_pgoff)<br />+{<br />+	return next_uptodate_page(xas_find(xas, end_pgoff),<br />+				  vmf, xas, end_pgoff);<br />+}<br />+<br />+static inline struct page *next_map_page(struct vm_fault *vmf,<br />+					  struct xa_state *xas,<br />+					  pgoff_t end_pgoff)<br />+{<br />+	return next_uptodate_page(xas_next_entry(xas, end_pgoff),<br />+				  vmf, xas, end_pgoff);<br />+}<br />+<br /> void filemap_map_pages(struct vm_fault *vmf,<br /> 		pgoff_t start_pgoff, pgoff_t end_pgoff)<br /> {<br />-	struct file *file = vmf-&gt;vma-&gt;vm_file;<br />+	struct vm_area_struct *vma = vmf-&gt;vma;<br />+	struct file *file = vma-&gt;vm_file;<br /> 	struct address_space *mapping = file-&gt;f_mapping;<br /> 	pgoff_t last_pgoff = start_pgoff;<br />-	unsigned long max_idx;<br /> 	XA_STATE(xas, &amp;mapping-&gt;i_pages, start_pgoff);<br /> 	struct page *head, *page;<br /> 	unsigned int mmap_miss = READ_ONCE(file-&gt;f_ra.mmap_miss);<br /> <br /> 	rcu_read_lock();<br />-	xas_for_each(&amp;xas, head, end_pgoff) {<br />-		if (xas_retry(&amp;xas, head))<br />-			continue;<br />-		if (xa_is_value(head))<br />-			goto next;<br />+	head = first_map_page(vmf, &amp;xas, end_pgoff);<br />+	if (!head) {<br />+		rcu_read_unlock();<br />+		return;<br />+	}<br /> <br />-		/*<br />-		 * Check for a locked page first, as a speculative<br />-		 * reference may adversely influence page migration.<br />-		 */<br />-		if (PageLocked(head))<br />-			goto next;<br />-		if (!page_cache_get_speculative(head))<br />-			goto next;<br />+	if (filemap_map_pmd(vmf, head)) {<br />+		rcu_read_unlock();<br />+		return;<br />+	}<br /> <br />-		/* Has the page moved or been split? */<br />-		if (unlikely(head != xas_reload(&amp;xas)))<br />-			goto skip;<br />+	vmf-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, vmf-&gt;pmd,<br />+				       vmf-&gt;address, &amp;vmf-&gt;ptl);<br />+	do {<br /> 		page = find_subpage(head, xas.xa_index);<br />-<br />-		if (!PageUptodate(head) ||<br />-				PageReadahead(page) ||<br />-				PageHWPoison(page))<br />-			goto skip;<br />-		if (!trylock_page(head))<br />-			goto skip;<br />-<br />-		if (head-&gt;mapping != mapping || !PageUptodate(head))<br />-			goto unlock;<br />-<br />-		max_idx = DIV_ROUND_UP(i_size_read(mapping-&gt;host), PAGE_SIZE);<br />-		if (xas.xa_index &gt;= max_idx)<br />+		if (PageHWPoison(page))<br /> 			goto unlock;<br /> <br /> 		if (mmap_miss &gt; 0)<br />&#64;&#64; -2964,19 +3048,25 &#64;&#64; void filemap_map_pages(struct vm_fault *vmf,<br /> 		if (vmf-&gt;pte)<br /> 			vmf-&gt;pte += xas.xa_index - last_pgoff;<br /> 		last_pgoff = xas.xa_index;<br />-		if (alloc_set_pte(vmf, page))<br />+<br />+		if (!pte_none(*vmf-&gt;pte))<br /> 			goto unlock;<br />+<br />+		do_set_pte(vmf, page);<br />+		/* no need to invalidate: a not-present page won't be cached */<br />+		update_mmu_cache(vma, vmf-&gt;address, vmf-&gt;pte);<br /> 		unlock_page(head);<br />-		goto next;<br />+		continue;<br /> unlock:<br /> 		unlock_page(head);<br />-skip:<br /> 		put_page(head);<br />-next:<br />-		/* Huge page is mapped? No need to proceed. */<br />-		if (pmd_trans_huge(*vmf-&gt;pmd))<br />-			break;<br />-	}<br />+	} while ((head = next_map_page(vmf, &amp;xas, end_pgoff)) != NULL);<br />+<br />+	/*<br />+	 * NOTE! We return with the pte still locked! It is unlocked<br />+	 * by do_fault_around() after it has tested whether the target<br />+	 * address got filled in.<br />+	 */<br /> 	rcu_read_unlock();<br /> 	WRITE_ONCE(file-&gt;f_ra.mmap_miss, mmap_miss);<br /> }<br />diff --git a/mm/memory.c b/mm/memory.c<br />index 7d608765932b..07a408c7d38b 100644<br />--- a/mm/memory.c<br />+++ b/mm/memory.c<br />&#64;&#64; -3501,7 +3501,7 &#64;&#64; static vm_fault_t do_anonymous_page(struct vm_fault *vmf)<br /> 	if (pte_alloc(vma-&gt;vm_mm, vmf-&gt;pmd))<br /> 		return VM_FAULT_OOM;<br /> <br />-	/* See the comment in pte_alloc_one_map() */<br />+	/* See comment in handle_pte_fault() */<br /> 	if (unlikely(pmd_trans_unstable(vmf-&gt;pmd)))<br /> 		return 0;<br /> <br />&#64;&#64; -3641,66 +3641,6 &#64;&#64; static vm_fault_t __do_fault(struct vm_fault *vmf)<br /> 	return ret;<br /> }<br /> <br />-/*<br />- * The ordering of these checks is important for pmds with _PAGE_DEVMAP set.<br />- * If we check pmd_trans_unstable() first we will trip the bad_pmd() check<br />- * inside of pmd_none_or_trans_huge_or_clear_bad(). This will end up correctly<br />- * returning 1 but not before it spams dmesg with the pmd_clear_bad() output.<br />- */<br />-static int pmd_devmap_trans_unstable(pmd_t *pmd)<br />-{<br />-	return pmd_devmap(*pmd) || pmd_trans_unstable(pmd);<br />-}<br />-<br />-static vm_fault_t pte_alloc_one_map(struct vm_fault *vmf)<br />-{<br />-	struct vm_area_struct *vma = vmf-&gt;vma;<br />-<br />-	if (!pmd_none(*vmf-&gt;pmd))<br />-		goto map_pte;<br />-	if (vmf-&gt;prealloc_pte) {<br />-		vmf-&gt;ptl = pmd_lock(vma-&gt;vm_mm, vmf-&gt;pmd);<br />-		if (unlikely(!pmd_none(*vmf-&gt;pmd))) {<br />-			spin_unlock(vmf-&gt;ptl);<br />-			goto map_pte;<br />-		}<br />-<br />-		mm_inc_nr_ptes(vma-&gt;vm_mm);<br />-		pmd_populate(vma-&gt;vm_mm, vmf-&gt;pmd, vmf-&gt;prealloc_pte);<br />-		spin_unlock(vmf-&gt;ptl);<br />-		vmf-&gt;prealloc_pte = NULL;<br />-	} else if (unlikely(pte_alloc(vma-&gt;vm_mm, vmf-&gt;pmd))) {<br />-		return VM_FAULT_OOM;<br />-	}<br />-map_pte:<br />-	/*<br />-	 * If a huge pmd materialized under us just retry later.  Use<br />-	 * pmd_trans_unstable() via pmd_devmap_trans_unstable() instead of<br />-	 * pmd_trans_huge() to ensure the pmd didn't become pmd_trans_huge<br />-	 * under us and then back to pmd_none, as a result of MADV_DONTNEED<br />-	 * running immediately after a huge pmd fault in a different thread of<br />-	 * this mm, in turn leading to a misleading pmd_trans_huge() retval.<br />-	 * All we have to ensure is that it is a regular pmd that we can walk<br />-	 * with pte_offset_map() and we can do that through an atomic read in<br />-	 * C, which is what pmd_trans_unstable() provides.<br />-	 */<br />-	if (pmd_devmap_trans_unstable(vmf-&gt;pmd))<br />-		return VM_FAULT_NOPAGE;<br />-<br />-	/*<br />-	 * At this point we know that our vmf-&gt;pmd points to a page of ptes<br />-	 * and it cannot become pmd_none(), pmd_devmap() or pmd_trans_huge()<br />-	 * for the duration of the fault.  If a racing MADV_DONTNEED runs and<br />-	 * we zap the ptes pointed to by our vmf-&gt;pmd, the vmf-&gt;ptl will still<br />-	 * be valid and we will re-check to make sure the vmf-&gt;pte isn't<br />-	 * pte_none() under vmf-&gt;ptl protection when we return to<br />-	 * alloc_set_pte().<br />-	 */<br />-	vmf-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, vmf-&gt;pmd, vmf-&gt;address,<br />-			&amp;vmf-&gt;ptl);<br />-	return 0;<br />-}<br />-<br /> #ifdef CONFIG_TRANSPARENT_HUGEPAGE<br /> static void deposit_prealloc_pte(struct vm_fault *vmf)<br /> {<br />&#64;&#64; -3715,7 +3655,7 &#64;&#64; static void deposit_prealloc_pte(struct vm_fault *vmf)<br /> 	vmf-&gt;prealloc_pte = NULL;<br /> }<br /> <br />-static vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)<br />+vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)<br /> {<br /> 	struct vm_area_struct *vma = vmf-&gt;vma;<br /> 	bool write = vmf-&gt;flags &amp; FAULT_FLAG_WRITE;<br />&#64;&#64; -3780,45 +3720,11 &#64;&#64; static vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)<br /> }<br /> #endif<br /> <br />-/**<br />- * alloc_set_pte - setup new PTE entry for given page and add reverse page<br />- * mapping. If needed, the function allocates page table or use pre-allocated.<br />- *<br />- * &#64;vmf: fault environment<br />- * &#64;page: page to map<br />- *<br />- * Caller must take care of unlocking vmf-&gt;ptl, if vmf-&gt;pte is non-NULL on<br />- * return.<br />- *<br />- * Target users are page handler itself and implementations of<br />- * vm_ops-&gt;map_pages.<br />- *<br />- * Return: %0 on success, %VM_FAULT_ code in case of error.<br />- */<br />-vm_fault_t alloc_set_pte(struct vm_fault *vmf, struct page *page)<br />+void do_set_pte(struct vm_fault *vmf, struct page *page)<br /> {<br /> 	struct vm_area_struct *vma = vmf-&gt;vma;<br /> 	bool write = vmf-&gt;flags &amp; FAULT_FLAG_WRITE;<br /> 	pte_t entry;<br />-	vm_fault_t ret;<br />-<br />-	if (pmd_none(*vmf-&gt;pmd) &amp;&amp; PageTransCompound(page)) {<br />-		ret = do_set_pmd(vmf, page);<br />-		if (ret != VM_FAULT_FALLBACK)<br />-			return ret;<br />-	}<br />-<br />-	if (!vmf-&gt;pte) {<br />-		ret = pte_alloc_one_map(vmf);<br />-		if (ret)<br />-			return ret;<br />-	}<br />-<br />-	/* Re-check under ptl */<br />-	if (unlikely(!pte_none(*vmf-&gt;pte))) {<br />-		update_mmu_tlb(vma, vmf-&gt;address, vmf-&gt;pte);<br />-		return VM_FAULT_NOPAGE;<br />-	}<br /> <br /> 	flush_icache_page(vma, page);<br /> 	entry = mk_pte(page, vma-&gt;vm_page_prot);<br />&#64;&#64; -3835,14 +3741,8 &#64;&#64; vm_fault_t alloc_set_pte(struct vm_fault *vmf, struct page *page)<br /> 		page_add_file_rmap(page, false);<br /> 	}<br /> 	set_pte_at(vma-&gt;vm_mm, vmf-&gt;address, vmf-&gt;pte, entry);<br />-<br />-	/* no need to invalidate: a not-present page won't be cached */<br />-	update_mmu_cache(vma, vmf-&gt;address, vmf-&gt;pte);<br />-<br />-	return 0;<br /> }<br /> <br />-<br /> /**<br />  * finish_fault - finish page fault once we have prepared the page to fault<br />  *<br />&#64;&#64; -3860,12 +3760,12 &#64;&#64; vm_fault_t alloc_set_pte(struct vm_fault *vmf, struct page *page)<br />  */<br /> vm_fault_t finish_fault(struct vm_fault *vmf)<br /> {<br />+	struct vm_area_struct *vma = vmf-&gt;vma;<br /> 	struct page *page;<br />-	vm_fault_t ret = 0;<br />+	vm_fault_t ret;<br /> <br /> 	/* Did we COW the page? */<br />-	if ((vmf-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp;<br />-	    !(vmf-&gt;vma-&gt;vm_flags &amp; VM_SHARED))<br />+	if ((vmf-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED))<br /> 		page = vmf-&gt;cow_page;<br /> 	else<br /> 		page = vmf-&gt;page;<br />&#64;&#64; -3874,13 +3774,35 &#64;&#64; vm_fault_t finish_fault(struct vm_fault *vmf)<br /> 	 * check even for read faults because we might have lost our CoWed<br /> 	 * page<br /> 	 */<br />-	if (!(vmf-&gt;vma-&gt;vm_flags &amp; VM_SHARED))<br />-		ret = check_stable_address_space(vmf-&gt;vma-&gt;vm_mm);<br />-	if (!ret)<br />-		ret = alloc_set_pte(vmf, page);<br />-	if (vmf-&gt;pte)<br />-		pte_unmap_unlock(vmf-&gt;pte, vmf-&gt;ptl);<br />-	return ret;<br />+	if (!(vma-&gt;vm_flags &amp; VM_SHARED))<br />+		ret = check_stable_address_space(vma-&gt;vm_mm);<br />+	if (ret)<br />+		return ret;<br />+<br />+	if (pmd_none(*vmf-&gt;pmd)) {<br />+		if (PageTransCompound(page)) {<br />+			ret = do_set_pmd(vmf, page);<br />+			if (ret != VM_FAULT_FALLBACK)<br />+				return ret;<br />+		}<br />+<br />+		if (unlikely(pte_alloc(vma-&gt;vm_mm, vmf-&gt;pmd)))<br />+			return VM_FAULT_OOM;<br />+	}<br />+<br />+	/* See comment in handle_pte_fault() */<br />+	if (pmd_devmap_trans_unstable(vmf-&gt;pmd))<br />+		return 0;<br />+<br />+	vmf-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, vmf-&gt;pmd,<br />+				      vmf-&gt;address, &amp;vmf-&gt;ptl);<br />+	/* Re-check under ptl */<br />+	if (likely(pte_none(*vmf-&gt;pte)))<br />+		do_set_pte(vmf, page);<br />+<br />+	update_mmu_tlb(vma, vmf-&gt;address, vmf-&gt;pte);<br />+	pte_unmap_unlock(vmf-&gt;pte, vmf-&gt;ptl);<br />+	return 0;<br /> }<br /> <br /> static unsigned long fault_around_bytes __read_mostly =<br />&#64;&#64; -4351,7 +4273,18 &#64;&#64; static vm_fault_t handle_pte_fault(struct vm_fault *vmf)<br /> 		 */<br /> 		vmf-&gt;pte = NULL;<br /> 	} else {<br />-		/* See comment in pte_alloc_one_map() */<br />+		/*<br />+		 * If a huge pmd materialized under us just retry later.  Use<br />+		 * pmd_trans_unstable() via pmd_devmap_trans_unstable() instead<br />+		 * of pmd_trans_huge() to ensure the pmd didn't become<br />+		 * pmd_trans_huge under us and then back to pmd_none, as a<br />+		 * result of MADV_DONTNEED running immediately after a huge pmd<br />+		 * fault in a different thread of this mm, in turn leading to a<br />+		 * misleading pmd_trans_huge() retval. All we have to ensure is<br />+		 * that it is a regular pmd that we can walk with<br />+		 * pte_offset_map() and we can do that through an atomic read<br />+		 * in C, which is what pmd_trans_unstable() provides.<br />+		 */<br /> 		if (pmd_devmap_trans_unstable(vmf-&gt;pmd))<br /> 			return 0;<br /> 		/*<br />-- <br />2.29.2.157.g1d47791a39<br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
