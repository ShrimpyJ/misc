    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2020/7/21/76">First message in thread</a></li><li><a href="/lkml/2020/7/23/1246">Linus Torvalds</a><ul><li><a href="/lkml/2020/7/23/1268">Hugh Dickins</a><ul><li><a href="/lkml/2020/7/23/1283">Linus Torvalds</a><ul><li><a href="/lkml/2020/7/23/1385">Hugh Dickins</a></li></ul></li></ul></li><li><a href="/lkml/2020/7/24/786">Oleg Nesterov</a><ul><li class="origin"><a href="/lkml/2020/7/24/1262">Linus Torvalds</a><ul><li><a href="/lkml/2020/7/24/1262">Linus Torvalds</a><ul><li><a href="/lkml/2020/7/24/1360">Hugh Dickins</a></li><li><a href="/lkml/2020/7/25/133">Oleg Nesterov</a></li><li><a href="/lkml/2020/8/3/755">Michal Hocko</a></li></ul></li><li><a href="/lkml/2020/7/25/125">Oleg Nesterov</a></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patches in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2020/7/24/969/1">Get diff 1</a></li><li><a href="/lkml/diff/2020/7/24/969/2">Get diff 2</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Fri, 24 Jul 2020 10:32:51 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [RFC PATCH] mm: silence soft lockups from unlock_page</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Fri, Jul 24, 2020 at 8:24 AM Oleg Nesterov &lt;oleg&#64;redhat.com&gt; wrote:<br />&gt;<br />&gt; not sure this makes any sense, but this looks like another user of<br />&gt; trylock_page_bit_common(), see the patch below on top of 1/2.<br /><br />Ok, that makes sense. Except you did it on top of the original patch<br />without the fix to set WQ_FLAG_WOKEN for the non-wakeup case.<br /><br />And in fact, once you do it on top of that, it becomes obvious that we<br />can share even more code: move the WQ_FLAG_WOKEN logic _into_ the<br />trylock_page_bit_common() function.<br /><br />Then the whole thing becomes something like the attached.<br /><br />I added your reviewed-by, but maybe you should double-check my changes.<br /><br />                Linus<br />From 7dd29b015a8f259ea1bbad954cf888e7fb2fe65f Mon Sep 17 00:00:00 2001<br />From: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />Date: Thu, 23 Jul 2020 10:16:49 -0700<br />Subject: [PATCH 1/2] mm: rewrite wait_on_page_bit_common() logic<br /><br />It turns out that wait_on_page_bit_common() had several problems,<br />ranging from just unfair behavioe due to re-queueing at the end of the<br />wait queue when re-trying, and an outright bug that could result in<br />missed wakeups (but probably never happened in practice).<br /><br />This rewrites the whole logic to avoid both issues, by simply moving the<br />logic to check (and possibly take) the bit lock into the wakeup path<br />instead.<br /><br />That makes everything much more straightforward, and means that we never<br />need to re-queue the wait entry: if we get woken up, we'll be notified<br />through WQ_FLAG_WOKEN, and the wait queue entry will have been removed,<br />and everything will have been done for us.<br /><br />Link: <a href="https://lore.kernel.org/lkml/CAHk-=wjJA2Z3kUFb-5s=6">https://lore.kernel.org/lkml/CAHk-=wjJA2Z3kUFb-5s=6</a>+n0qbTs8ELqKFt9B3pH85a8fGD73w&#64;mail.gmail.com/<br />Link: <a href="https://lore.kernel.org/lkml/alpine.LSU.2.11.2007221359450.1017&#64;eggly.anvils/">https://lore.kernel.org/lkml/alpine.LSU.2.11.2007221359450.1017&#64;eggly.anvils/</a><br />Reported-by: Oleg Nesterov &lt;oleg&#64;redhat.com&gt;<br />Reported-by: Hugh Dickins &lt;hughd&#64;google.com&gt;<br />Cc: Michal Hocko &lt;mhocko&#64;suse.com&gt;<br />Reviewed-by: Oleg Nesterov &lt;oleg&#64;redhat.com&gt;<br />Signed-off-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />---<br /> mm/filemap.c | 131 ++++++++++++++++++++++++++++++++-------------------<br /> 1 file changed, 82 insertions(+), 49 deletions(-)<br /><br />diff --git a/mm/filemap.c b/mm/filemap.c<br />index 385759c4ce4b..c71c5e5c8cdc 100644<br />--- a/mm/filemap.c<br />+++ b/mm/filemap.c<br />&#64;&#64; -1000,8 +1000,33 &#64;&#64; struct wait_page_queue {<br /> 	wait_queue_entry_t wait;<br /> };<br /> <br />+/*<br />+ * Attempt to check (or get) the page bit, and mark the<br />+ * waiter woken if successful.<br />+ *<br />+ * Return negative on failure, and the "number of exclusive<br />+ * wakeups" (for the wait function exclusive counting logic)<br />+ * if successful.<br />+ */<br />+static inline int trylock_page_bit_common(struct page *page, int bit_nr,<br />+					struct wait_queue_entry *wait)<br />+{<br />+	if (wait-&gt;flags &amp; WQ_FLAG_EXCLUSIVE) {<br />+		if (test_and_set_bit(bit_nr, &amp;page-&gt;flags))<br />+			return -1;<br />+		wait-&gt;flags |= WQ_FLAG_WOKEN;<br />+		return 1;<br />+	}<br />+<br />+	if (test_bit(bit_nr, &amp;page-&gt;flags))<br />+		return -1;<br />+	wait-&gt;flags |= WQ_FLAG_WOKEN;<br />+	return 0;<br />+}<br />+<br /> static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)<br /> {<br />+	int ret;<br /> 	struct wait_page_key *key = arg;<br /> 	struct wait_page_queue *wait_page<br /> 		= container_of(wait, struct wait_page_queue, wait);<br />&#64;&#64; -1013,18 +1038,29 &#64;&#64; static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync,<br /> 	if (wait_page-&gt;bit_nr != key-&gt;bit_nr)<br /> 		return 0;<br /> <br />+	/* Stop walking if it's locked */<br />+	ret = trylock_page_bit_common(key-&gt;page, key-&gt;bit_nr, wait);<br />+	if (ret &lt; 0)<br />+		return ret;<br />+<br />+	wake_up_state(wait-&gt;private, mode);<br />+<br /> 	/*<br />-	 * Stop walking if it's locked.<br />-	 * Is this safe if put_and_wait_on_page_locked() is in use?<br />-	 * Yes: the waker must hold a reference to this page, and if PG_locked<br />-	 * has now already been set by another task, that task must also hold<br />-	 * a reference to the *same usage* of this page; so there is no need<br />-	 * to walk on to wake even the put_and_wait_on_page_locked() callers.<br />+	 * Ok, we have successfully done what we're waiting for,<br />+	 * and we can unconditionally remove the wait entry.<br />+	 *<br />+	 * Note that this has to be the absolute last thing we do,<br />+	 * since after list_del_init(&amp;wait-&gt;entry) the wait entry<br />+	 * might be de-allocated and the process might even have<br />+	 * exited.<br />+	 *<br />+	 * We _really_ should have a "list_del_init_careful()" to<br />+	 * properly pair with the unlocked "list_empty_careful()"<br />+	 * in finish_wait().<br /> 	 */<br />-	if (test_bit(key-&gt;bit_nr, &amp;key-&gt;page-&gt;flags))<br />-		return -1;<br />-<br />-	return autoremove_wake_function(wait, mode, sync, key);<br />+	smp_mb();<br />+	list_del_init(&amp;wait-&gt;entry);<br />+	return ret;<br /> }<br /> <br /> static void wake_up_page_bit(struct page *page, int bit_nr)<br />&#64;&#64; -1108,11 +1144,9 &#64;&#64; static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br /> {<br /> 	struct wait_page_queue wait_page;<br /> 	wait_queue_entry_t *wait = &amp;wait_page.wait;<br />-	bool bit_is_set;<br /> 	bool thrashing = false;<br /> 	bool delayacct = false;<br /> 	unsigned long pflags;<br />-	int ret = 0;<br /> <br /> 	if (bit_nr == PG_locked &amp;&amp;<br /> 	    !PageUptodate(page) &amp;&amp; PageWorkingset(page)) {<br />&#64;&#64; -1130,48 +1164,47 &#64;&#64; static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br /> 	wait_page.page = page;<br /> 	wait_page.bit_nr = bit_nr;<br /> <br />+	/*<br />+	 * Do one last check whether we can get the<br />+	 * page bit synchronously.<br />+	 *<br />+	 * Do the SetPageWaiters() marking before that<br />+	 * to let any waker we _just_ missed know they<br />+	 * need to wake us up (otherwise they'll never<br />+	 * even go to the slow case that looks at the<br />+	 * page queue), and add ourselves to the wait<br />+	 * queue if we need to sleep.<br />+	 *<br />+	 * This part needs to be done under the queue<br />+	 * lock to avoid races.<br />+	 */<br />+	spin_lock_irq(&amp;q-&gt;lock);<br />+	SetPageWaiters(page);<br />+	if (trylock_page_bit_common(page, bit_nr, wait) &lt; 0)<br />+		__add_wait_queue_entry_tail(q, wait);<br />+	spin_unlock_irq(&amp;q-&gt;lock);<br />+<br />+	/*<br />+	 * From now on, all the logic will be based on<br />+	 * whether the wait entry is on the queue or not,<br />+	 * and the page bit testing (and setting) will be<br />+	 * done by the wake function, not us.<br />+	 *<br />+	 * We can drop our reference to the page.<br />+	 */<br />+	if (behavior == DROP)<br />+		put_page(page);<br />+<br /> 	for (;;) {<br />-		spin_lock_irq(&amp;q-&gt;lock);<br />-<br />-		if (likely(list_empty(&amp;wait-&gt;entry))) {<br />-			__add_wait_queue_entry_tail(q, wait);<br />-			SetPageWaiters(page);<br />-		}<br />-<br /> 		set_current_state(state);<br /> <br />-		spin_unlock_irq(&amp;q-&gt;lock);<br />-<br />-		bit_is_set = test_bit(bit_nr, &amp;page-&gt;flags);<br />-		if (behavior == DROP)<br />-			put_page(page);<br />-<br />-		if (likely(bit_is_set))<br />-			io_schedule();<br />-<br />-		if (behavior == EXCLUSIVE) {<br />-			if (!test_and_set_bit_lock(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />-		} else if (behavior == SHARED) {<br />-			if (!test_bit(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />-		}<br />-<br />-		if (signal_pending_state(state, current)) {<br />-			ret = -EINTR;<br />+		if (signal_pending_state(state, current))<br /> 			break;<br />-		}<br /> <br />-		if (behavior == DROP) {<br />-			/*<br />-			 * We can no longer safely access page-&gt;flags:<br />-			 * even if CONFIG_MEMORY_HOTREMOVE is not enabled,<br />-			 * there is a risk of waiting forever on a page reused<br />-			 * for something that keeps it locked indefinitely.<br />-			 * But best check for -EINTR above before breaking.<br />-			 */<br />+		if (wait-&gt;flags &amp; WQ_FLAG_WOKEN)<br /> 			break;<br />-		}<br />+<br />+		io_schedule();<br /> 	}<br /> <br /> 	finish_wait(q, wait);<br />&#64;&#64; -1190,7 +1223,7 &#64;&#64; static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br /> 	 * bother with signals either.<br /> 	 */<br /> <br />-	return ret;<br />+	return wait-&gt;flags &amp; WQ_FLAG_WOKEN ? 0 : -EINTR;<br /> }<br /> <br /> void wait_on_page_bit(struct page *page, int bit_nr)<br />-- <br />2.28.0.rc0.3.g1e25d3a62f<br />From 46ac2dda623666034b1b1a0b41bbc1934ed396d2 Mon Sep 17 00:00:00 2001<br />From: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />Date: Thu, 23 Jul 2020 12:33:41 -0700<br />Subject: [PATCH 2/2] list: add "list_del_init_careful()" to go with<br /> "list_empty_careful()"<br /><br />That gives us ordering guarantees around the pair.<br /><br />Signed-off-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />---<br /> include/linux/list.h | 20 +++++++++++++++++++-<br /> kernel/sched/wait.c  |  2 +-<br /> mm/filemap.c         |  7 +------<br /> 3 files changed, 21 insertions(+), 8 deletions(-)<br /><br />diff --git a/include/linux/list.h b/include/linux/list.h<br />index aff44d34f4e4..0d0d17a10d25 100644<br />--- a/include/linux/list.h<br />+++ b/include/linux/list.h<br />&#64;&#64; -282,6 +282,24 &#64;&#64; static inline int list_empty(const struct list_head *head)<br /> 	return READ_ONCE(head-&gt;next) == head;<br /> }<br /> <br />+/**<br />+ * list_del_init_careful - deletes entry from list and reinitialize it.<br />+ * &#64;entry: the element to delete from the list.<br />+ *<br />+ * This is the same as list_del_init(), except designed to be used<br />+ * together with list_empty_careful() in a way to guarantee ordering<br />+ * of other memory operations.<br />+ *<br />+ * Any memory operations done before a list_del_init_careful() are<br />+ * guaranteed to be visible after a list_empty_careful() test.<br />+ */<br />+static inline void list_del_init_careful(struct list_head *entry)<br />+{<br />+	__list_del_entry(entry);<br />+	entry-&gt;prev = entry;<br />+	smp_store_release(&amp;entry-&gt;next, entry);<br />+}<br />+<br /> /**<br />  * list_empty_careful - tests whether a list is empty and not being modified<br />  * &#64;head: the list to test<br />&#64;&#64; -297,7 +315,7 &#64;&#64; static inline int list_empty(const struct list_head *head)<br />  */<br /> static inline int list_empty_careful(const struct list_head *head)<br /> {<br />-	struct list_head *next = head-&gt;next;<br />+	struct list_head *next = smp_load_acquire(&amp;head-&gt;next);<br /> 	return (next == head) &amp;&amp; (next == head-&gt;prev);<br /> }<br /> <br />diff --git a/kernel/sched/wait.c b/kernel/sched/wait.c<br />index ba059fbfc53a..01f5d3020589 100644<br />--- a/kernel/sched/wait.c<br />+++ b/kernel/sched/wait.c<br />&#64;&#64; -389,7 +389,7 &#64;&#64; int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, i<br /> 	int ret = default_wake_function(wq_entry, mode, sync, key);<br /> <br /> 	if (ret)<br />-		list_del_init(&amp;wq_entry-&gt;entry);<br />+		list_del_init_careful(&amp;wq_entry-&gt;entry);<br /> <br /> 	return ret;<br /> }<br />diff --git a/mm/filemap.c b/mm/filemap.c<br />index c71c5e5c8cdc..0987eef6f9bc 100644<br />--- a/mm/filemap.c<br />+++ b/mm/filemap.c<br />&#64;&#64; -1053,13 +1053,8 &#64;&#64; static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync,<br /> 	 * since after list_del_init(&amp;wait-&gt;entry) the wait entry<br /> 	 * might be de-allocated and the process might even have<br /> 	 * exited.<br />-	 *<br />-	 * We _really_ should have a "list_del_init_careful()" to<br />-	 * properly pair with the unlocked "list_empty_careful()"<br />-	 * in finish_wait().<br /> 	 */<br />-	smp_mb();<br />-	list_del_init(&amp;wait-&gt;entry);<br />+	list_del_init_careful(&amp;wait-&gt;entry);<br /> 	return ret;<br /> }<br /> <br />-- <br />2.28.0.rc0.3.g1e25d3a62f<br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
