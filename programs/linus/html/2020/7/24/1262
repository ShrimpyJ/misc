    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2020/7/21/76">First message in thread</a></li><li><a href="/lkml/2020/7/24/786">Oleg Nesterov</a><ul><li><a href="/lkml/2020/7/24/969">Linus Torvalds</a><ul><li class="origin"><a href="/lkml/2020/7/24/1360">Linus Torvalds</a><ul><li><a href="/lkml/2020/7/24/1360">Hugh Dickins</a><ul><li><a href="/lkml/2020/7/24/1366">Linus Torvalds</a></li></ul></li><li><a href="/lkml/2020/7/25/133">Oleg Nesterov</a><ul><li><a href="/lkml/2020/7/25/309">Linus Torvalds</a></li></ul></li><li><a href="/lkml/2020/8/3/755">Michal Hocko</a><ul><li><a href="/lkml/2020/8/3/1052">Linus Torvalds</a></li></ul></li></ul></li><li><a href="/lkml/2020/7/25/125">Oleg Nesterov</a></li></ul></li></ul></li></ul><div class="threadlist">Patches in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2020/7/24/1262/1">Get diff 1</a></li><li><a href="/lkml/diff/2020/7/24/1262/2">Get diff 2</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Fri, 24 Jul 2020 16:25:56 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [RFC PATCH] mm: silence soft lockups from unlock_page</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Fri, Jul 24, 2020 at 10:32 AM Linus Torvalds<br />&lt;torvalds&#64;linux-foundation.org&gt; wrote:<br />&gt; Ok, that makes sense. Except you did it on top of the original patch<br />&gt; without the fix to set WQ_FLAG_WOKEN for the non-wakeup case.<br /><br />Hmm.<br /><br />I just realized that one thing we could do is to not even test the<br />page bit for the shared case in the wakeup path.<br /><br />Because anybody who uses the non-exclusive "wait_on_page_locked()" or<br />"wait_on_page_writeback()" isn't actually interested in the bit state<br />any more at that point. All they care about is that somebody cleared<br />it - not whether it was then re-taken again.<br /><br />So instead of keeping them on the list - or stopping the waitqueue<br />walk because somebody else got the bit - we could just mark them<br />successfully done, wake them up, and remove those entries from the<br />list.<br /><br />That would be better for everybody - less pointless waiting for a new<br />lock or writeback event, but also fewer entries on the wait queues as<br />we get rid of them more quickly instead of walking them over and over<br />just because somebody else re-took the page lock.<br /><br />Generally "wait_on_page_locked()" is used for two things<br /><br /> - either wait for the IO to then check if it's now uptodate<br /><br /> - throttle things that can't afford to lock the page (eg page faults<br />that dropped the mm lock, and as such need to go through the whole<br />restart logic, but that don't want to lock the page because it's now<br />too late, but also the page migration things)<br /><br />In the first case, waiting to actually seeing the locked bit clear is<br />pointless - the code only cared about the "wait for IO in progress"<br />not about the lock bit itself.<br /><br />And that second case generally might want to retry, but doesn't want<br />to busy-loop.<br /><br />And "wait_on_page_writeback()" is basically used for similar reasons<br />(ie check if there were IO errors, but also possibly to throttle<br />writeback traffic).<br /><br />Saying "stop walking, keep it on the list" seems wrong. It makes IO<br />error handling and retries much worse, for example.<br /><br />So it turns out that the wakeup logic and the initial wait logic don't<br />have so much in common after all, and there is a fundamental<br />conceptual difference between that "check bit one last time" case, and<br />the "we got woken up, now what" case..<br /><br />End result: one final (yes, hopefully - I think I'm done) version of<br />this patch-series.<br /><br />This not only makes the code cleaner (the generated code for<br />wake_up_page() is really quite nice now), but getting rid of extra<br />waiting might help the load that Michal reported.<br /><br />Because a lot of page waiting might be that non-exclusive<br />"wait_on_page_locked()" kind, particularly in the thundering herd kind<br />of situation where one process starts IO, and then other processes<br />wait for it to finish.<br /><br />Those users don't even care if somebody else then did a "lock_page()"<br />for some other reason (maybe for writeback). They are generally<br />perfectly happy with a locked page, as long as it's now up-to-date.<br /><br />So this not only simplifies the code, it really might avoid some problems too.<br /><br />               Linus<br />From 0bccb60841cc52a9aa6e9cc6b7eff59d1983e8fa Mon Sep 17 00:00:00 2001<br />From: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />Date: Thu, 23 Jul 2020 10:16:49 -0700<br />Subject: [PATCH 1/2] mm: rewrite wait_on_page_bit_common() logic<br /><br />It turns out that wait_on_page_bit_common() had several problems,<br />ranging from just unfair behavioe due to re-queueing at the end of the<br />wait queue when re-trying, and an outright bug that could result in<br />missed wakeups (but probably never happened in practice).<br /><br />This rewrites the whole logic to avoid both issues, by simply moving the<br />logic to check (and possibly take) the bit lock into the wakeup path<br />instead.<br /><br />That makes everything much more straightforward, and means that we never<br />need to re-queue the wait entry: if we get woken up, we'll be notified<br />through WQ_FLAG_WOKEN, and the wait queue entry will have been removed,<br />and everything will have been done for us.<br /><br />Link: <a href="https://lore.kernel.org/lkml/CAHk-=wjJA2Z3kUFb-5s=6">https://lore.kernel.org/lkml/CAHk-=wjJA2Z3kUFb-5s=6</a>+n0qbTs8ELqKFt9B3pH85a8fGD73w&#64;mail.gmail.com/<br />Link: <a href="https://lore.kernel.org/lkml/alpine.LSU.2.11.2007221359450.1017&#64;eggly.anvils/">https://lore.kernel.org/lkml/alpine.LSU.2.11.2007221359450.1017&#64;eggly.anvils/</a><br />Reported-by: Oleg Nesterov &lt;oleg&#64;redhat.com&gt;<br />Reported-by: Hugh Dickins &lt;hughd&#64;google.com&gt;<br />Cc: Michal Hocko &lt;mhocko&#64;suse.com&gt;<br />Reviewed-by: Oleg Nesterov &lt;oleg&#64;redhat.com&gt;<br />Signed-off-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />---<br /> mm/filemap.c | 132 +++++++++++++++++++++++++++++++++------------------<br /> 1 file changed, 85 insertions(+), 47 deletions(-)<br /><br />diff --git a/mm/filemap.c b/mm/filemap.c<br />index 385759c4ce4b..8c3d3e233d37 100644<br />--- a/mm/filemap.c<br />+++ b/mm/filemap.c<br />&#64;&#64; -1002,6 +1002,7 &#64;&#64; struct wait_page_queue {<br /> <br /> static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)<br /> {<br />+	int ret;<br /> 	struct wait_page_key *key = arg;<br /> 	struct wait_page_queue *wait_page<br /> 		= container_of(wait, struct wait_page_queue, wait);<br />&#64;&#64; -1014,17 +1015,40 &#64;&#64; static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync,<br /> 		return 0;<br /> <br /> 	/*<br />-	 * Stop walking if it's locked.<br />-	 * Is this safe if put_and_wait_on_page_locked() is in use?<br />-	 * Yes: the waker must hold a reference to this page, and if PG_locked<br />-	 * has now already been set by another task, that task must also hold<br />-	 * a reference to the *same usage* of this page; so there is no need<br />-	 * to walk on to wake even the put_and_wait_on_page_locked() callers.<br />+	 * If it's an exclusive wait, we get the bit for it, and<br />+	 * stop walking if we can't.<br />+	 *<br />+	 * If it's a non-exclusive wait, then the fact that this<br />+	 * wake function was called means that the bit already<br />+	 * was cleared, and we don't care if somebody then<br />+	 * re-took it.<br /> 	 */<br />-	if (test_bit(key-&gt;bit_nr, &amp;key-&gt;page-&gt;flags))<br />-		return -1;<br />+	ret = 0;<br />+	if (wait-&gt;flags &amp; WQ_FLAG_EXCLUSIVE) {<br />+		if (test_and_set_bit(key-&gt;bit_nr, &amp;key-&gt;page-&gt;flags))<br />+			return -1;<br />+		ret = 1;<br />+	}<br />+	wait-&gt;flags |= WQ_FLAG_WOKEN;<br /> <br />-	return autoremove_wake_function(wait, mode, sync, key);<br />+	wake_up_state(wait-&gt;private, mode);<br />+<br />+	/*<br />+	 * Ok, we have successfully done what we're waiting for,<br />+	 * and we can unconditionally remove the wait entry.<br />+	 *<br />+	 * Note that this has to be the absolute last thing we do,<br />+	 * since after list_del_init(&amp;wait-&gt;entry) the wait entry<br />+	 * might be de-allocated and the process might even have<br />+	 * exited.<br />+	 *<br />+	 * We _really_ should have a "list_del_init_careful()" to<br />+	 * properly pair with the unlocked "list_empty_careful()"<br />+	 * in finish_wait().<br />+	 */<br />+	smp_mb();<br />+	list_del_init(&amp;wait-&gt;entry);<br />+	return ret;<br /> }<br /> <br /> static void wake_up_page_bit(struct page *page, int bit_nr)<br />&#64;&#64; -1103,16 +1127,31 &#64;&#64; enum behavior {<br /> 			 */<br /> };<br /> <br />+/*<br />+ * Attempt to check (or get) the page bit, and mark the<br />+ * waiter woken if successful.<br />+ */<br />+static inline bool trylock_page_bit_common(struct page *page, int bit_nr,<br />+					struct wait_queue_entry *wait)<br />+{<br />+	if (wait-&gt;flags &amp; WQ_FLAG_EXCLUSIVE) {<br />+		if (test_and_set_bit(bit_nr, &amp;page-&gt;flags))<br />+			return false;<br />+	} else if (test_bit(bit_nr, &amp;page-&gt;flags))<br />+		return false;<br />+<br />+	wait-&gt;flags |= WQ_FLAG_WOKEN;<br />+	return true;<br />+}<br />+<br /> static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br /> 	struct page *page, int bit_nr, int state, enum behavior behavior)<br /> {<br /> 	struct wait_page_queue wait_page;<br /> 	wait_queue_entry_t *wait = &amp;wait_page.wait;<br />-	bool bit_is_set;<br /> 	bool thrashing = false;<br /> 	bool delayacct = false;<br /> 	unsigned long pflags;<br />-	int ret = 0;<br /> <br /> 	if (bit_nr == PG_locked &amp;&amp;<br /> 	    !PageUptodate(page) &amp;&amp; PageWorkingset(page)) {<br />&#64;&#64; -1130,48 +1169,47 &#64;&#64; static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br /> 	wait_page.page = page;<br /> 	wait_page.bit_nr = bit_nr;<br /> <br />-	for (;;) {<br />-		spin_lock_irq(&amp;q-&gt;lock);<br />+	/*<br />+	 * Do one last check whether we can get the<br />+	 * page bit synchronously.<br />+	 *<br />+	 * Do the SetPageWaiters() marking before that<br />+	 * to let any waker we _just_ missed know they<br />+	 * need to wake us up (otherwise they'll never<br />+	 * even go to the slow case that looks at the<br />+	 * page queue), and add ourselves to the wait<br />+	 * queue if we need to sleep.<br />+	 *<br />+	 * This part needs to be done under the queue<br />+	 * lock to avoid races.<br />+	 */<br />+	spin_lock_irq(&amp;q-&gt;lock);<br />+	SetPageWaiters(page);<br />+	if (!trylock_page_bit_common(page, bit_nr, wait))<br />+		__add_wait_queue_entry_tail(q, wait);<br />+	spin_unlock_irq(&amp;q-&gt;lock);<br /> <br />-		if (likely(list_empty(&amp;wait-&gt;entry))) {<br />-			__add_wait_queue_entry_tail(q, wait);<br />-			SetPageWaiters(page);<br />-		}<br />+	/*<br />+	 * From now on, all the logic will be based on<br />+	 * the WQ_FLAG_WOKEN flag, and the and the page<br />+	 * bit testing (and setting) will be - or has<br />+	 * already been - done by the wake function.<br />+	 *<br />+	 * We can drop our reference to the page.<br />+	 */<br />+	if (behavior == DROP)<br />+		put_page(page);<br /> <br />+	for (;;) {<br /> 		set_current_state(state);<br /> <br />-		spin_unlock_irq(&amp;q-&gt;lock);<br />-<br />-		bit_is_set = test_bit(bit_nr, &amp;page-&gt;flags);<br />-		if (behavior == DROP)<br />-			put_page(page);<br />-<br />-		if (likely(bit_is_set))<br />-			io_schedule();<br />-<br />-		if (behavior == EXCLUSIVE) {<br />-			if (!test_and_set_bit_lock(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />-		} else if (behavior == SHARED) {<br />-			if (!test_bit(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />-		}<br />-<br />-		if (signal_pending_state(state, current)) {<br />-			ret = -EINTR;<br />+		if (signal_pending_state(state, current))<br /> 			break;<br />-		}<br /> <br />-		if (behavior == DROP) {<br />-			/*<br />-			 * We can no longer safely access page-&gt;flags:<br />-			 * even if CONFIG_MEMORY_HOTREMOVE is not enabled,<br />-			 * there is a risk of waiting forever on a page reused<br />-			 * for something that keeps it locked indefinitely.<br />-			 * But best check for -EINTR above before breaking.<br />-			 */<br />+		if (wait-&gt;flags &amp; WQ_FLAG_WOKEN)<br /> 			break;<br />-		}<br />+<br />+		io_schedule();<br /> 	}<br /> <br /> 	finish_wait(q, wait);<br />&#64;&#64; -1190,7 +1228,7 &#64;&#64; static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br /> 	 * bother with signals either.<br /> 	 */<br /> <br />-	return ret;<br />+	return wait-&gt;flags &amp; WQ_FLAG_WOKEN ? 0 : -EINTR;<br /> }<br /> <br /> void wait_on_page_bit(struct page *page, int bit_nr)<br />-- <br />2.28.0.rc0.3.g1e25d3a62f<br />From 93f0263b9b952a1c449cec56a6aadf6320e821f9 Mon Sep 17 00:00:00 2001<br />From: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />Date: Thu, 23 Jul 2020 12:33:41 -0700<br />Subject: [PATCH 2/2] list: add "list_del_init_careful()" to go with<br /> "list_empty_careful()"<br /><br />That gives us ordering guarantees around the pair.<br /><br />Signed-off-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />---<br /> include/linux/list.h | 20 +++++++++++++++++++-<br /> kernel/sched/wait.c  |  2 +-<br /> mm/filemap.c         |  7 +------<br /> 3 files changed, 21 insertions(+), 8 deletions(-)<br /><br />diff --git a/include/linux/list.h b/include/linux/list.h<br />index aff44d34f4e4..0d0d17a10d25 100644<br />--- a/include/linux/list.h<br />+++ b/include/linux/list.h<br />&#64;&#64; -282,6 +282,24 &#64;&#64; static inline int list_empty(const struct list_head *head)<br /> 	return READ_ONCE(head-&gt;next) == head;<br /> }<br /> <br />+/**<br />+ * list_del_init_careful - deletes entry from list and reinitialize it.<br />+ * &#64;entry: the element to delete from the list.<br />+ *<br />+ * This is the same as list_del_init(), except designed to be used<br />+ * together with list_empty_careful() in a way to guarantee ordering<br />+ * of other memory operations.<br />+ *<br />+ * Any memory operations done before a list_del_init_careful() are<br />+ * guaranteed to be visible after a list_empty_careful() test.<br />+ */<br />+static inline void list_del_init_careful(struct list_head *entry)<br />+{<br />+	__list_del_entry(entry);<br />+	entry-&gt;prev = entry;<br />+	smp_store_release(&amp;entry-&gt;next, entry);<br />+}<br />+<br /> /**<br />  * list_empty_careful - tests whether a list is empty and not being modified<br />  * &#64;head: the list to test<br />&#64;&#64; -297,7 +315,7 &#64;&#64; static inline int list_empty(const struct list_head *head)<br />  */<br /> static inline int list_empty_careful(const struct list_head *head)<br /> {<br />-	struct list_head *next = head-&gt;next;<br />+	struct list_head *next = smp_load_acquire(&amp;head-&gt;next);<br /> 	return (next == head) &amp;&amp; (next == head-&gt;prev);<br /> }<br /> <br />diff --git a/kernel/sched/wait.c b/kernel/sched/wait.c<br />index ba059fbfc53a..01f5d3020589 100644<br />--- a/kernel/sched/wait.c<br />+++ b/kernel/sched/wait.c<br />&#64;&#64; -389,7 +389,7 &#64;&#64; int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, i<br /> 	int ret = default_wake_function(wq_entry, mode, sync, key);<br /> <br /> 	if (ret)<br />-		list_del_init(&amp;wq_entry-&gt;entry);<br />+		list_del_init_careful(&amp;wq_entry-&gt;entry);<br /> <br /> 	return ret;<br /> }<br />diff --git a/mm/filemap.c b/mm/filemap.c<br />index 8c3d3e233d37..991503bbf922 100644<br />--- a/mm/filemap.c<br />+++ b/mm/filemap.c<br />&#64;&#64; -1041,13 +1041,8 &#64;&#64; static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync,<br /> 	 * since after list_del_init(&amp;wait-&gt;entry) the wait entry<br /> 	 * might be de-allocated and the process might even have<br /> 	 * exited.<br />-	 *<br />-	 * We _really_ should have a "list_del_init_careful()" to<br />-	 * properly pair with the unlocked "list_empty_careful()"<br />-	 * in finish_wait().<br /> 	 */<br />-	smp_mb();<br />-	list_del_init(&amp;wait-&gt;entry);<br />+	list_del_init_careful(&amp;wait-&gt;entry);<br /> 	return ret;<br /> }<br /> <br />-- <br />2.28.0.rc0.3.g1e25d3a62f<br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
