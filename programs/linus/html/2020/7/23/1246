    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2020/7/21/76">First message in thread</a></li><li><a href="/lkml/2020/7/23/1039">Linus Torvalds</a><ul><li><a href="/lkml/2020/7/23/1228">Hugh Dickins</a><ul><li class="origin"><a href="/lkml/2020/7/23/1268">Linus Torvalds</a><ul><li><a href="/lkml/2020/7/23/1268">Hugh Dickins</a><ul><li><a href="/lkml/2020/7/23/1283">Linus Torvalds</a></li></ul></li><li><a href="/lkml/2020/7/24/786">Oleg Nesterov</a><ul><li><a href="/lkml/2020/7/24/969">Linus Torvalds</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patches in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2020/7/23/1246/1">Get diff 1</a></li><li><a href="/lkml/diff/2020/7/23/1246/2">Get diff 2</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Thu, 23 Jul 2020 16:43:08 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [RFC PATCH] mm: silence soft lockups from unlock_page</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Thu, Jul 23, 2020 at 4:11 PM Hugh Dickins &lt;hughd&#64;google.com&gt; wrote:<br />&gt;<br />&gt; On Thu, 23 Jul 2020, Linus Torvalds wrote:<br />&gt; &gt;<br />&gt; &gt; I'll send a new version after I actually test it.<br />&gt;<br />&gt; I'll give it a try when you're happy with it.<br /><br />Ok, what I described is what I've been running for a while now. But I<br />don't put much stress on my system with my normal workload, so..<br /><br />&gt; I did try yesterday's<br />&gt; with my swapping loads on home machines (3 of 4 survived 16 hours),<br />&gt; and with some google stresstests on work machines (0 of 10 survived).<br />&gt;<br />&gt; I've not spent long analyzing the crashes, all of them in or below<br />&gt; __wake_up_common() called from __wake_up_locked_key_bookmark():<br />&gt; sometimes gets to run the curr-&gt;func() and crashes on something<br />&gt; inside there (often list_del's lib/list_debug.c:53!), sometimes<br />&gt; cannot get that far. Looks like the wait queue entries on the list<br />&gt; were not entirely safe with that patch.<br /><br />Hmm. The bug Oleg pointed out should be pretty theoretical. But I<br />think the new approach with WQ_FLAG_WOKEN was much better anyway,<br />despite me missing that one spot in the first version of the patch.<br /><br />So here's two patches - the first one does that wake_page_function()<br />conversion, and the second one just does the memory ordering cleanup I<br />mentioned.<br /><br />I don't think the second one shouldn't matter on x86, but who knows.<br /><br />I don't enable list debugging, but I find list corruption surprising.<br />All of _that_ should be inside the page waiqueue lock, the only<br />unlocked part was the "list_empty_careful()" part.<br /><br />But I'll walk over my patch mentally one more time. Here's the current<br />version, anyway.<br /><br />                Linus<br />From cf6db0b8554723f0308fd9299e642898e36c9c8c Mon Sep 17 00:00:00 2001<br />From: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />Date: Thu, 23 Jul 2020 10:16:49 -0700<br />Subject: [PATCH 1/2] mm: rewrite wait_on_page_bit_common() logic<br /><br />It turns out that wait_on_page_bit_common() had several problems,<br />ranging from just unfair behavioe due to re-queueing at the end of the<br />wait queue when re-trying, and an outright bug that could result in<br />missed wakeups (but probably never happened in practice).<br /><br />This rewrites the whole logic to avoid both issues, by simply moving the<br />logic to check (and possibly take) the bit lock into the wakeup path<br />instead.<br /><br />That makes everything much more straightforward, and means that we never<br />need to re-queue the wait entry: if we get woken up, we'll be notified<br />through WQ_FLAG_WOKEN, and the wait queue entry will have been removed,<br />and everything will have been done for us.<br /><br />Link: <a href="https://lore.kernel.org/lkml/CAHk-=wjJA2Z3kUFb-5s=6">https://lore.kernel.org/lkml/CAHk-=wjJA2Z3kUFb-5s=6</a>+n0qbTs8ELqKFt9B3pH85a8fGD73w&#64;mail.gmail.com/<br />Link: <a href="https://lore.kernel.org/lkml/alpine.LSU.2.11.2007221359450.1017&#64;eggly.anvils/">https://lore.kernel.org/lkml/alpine.LSU.2.11.2007221359450.1017&#64;eggly.anvils/</a><br />Reported-by: Oleg Nesterov &lt;oleg&#64;redhat.com&gt;<br />Reported-by: Hugh Dickins &lt;hughd&#64;google.com&gt;<br />Cc: Michal Hocko &lt;mhocko&#64;suse.com&gt;<br />Signed-off-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />---<br /> mm/filemap.c | 121 +++++++++++++++++++++++++++++++--------------------<br /> 1 file changed, 73 insertions(+), 48 deletions(-)<br /><br />diff --git a/mm/filemap.c b/mm/filemap.c<br />index 385759c4ce4b..1143c0652d81 100644<br />--- a/mm/filemap.c<br />+++ b/mm/filemap.c<br />&#64;&#64; -1002,6 +1002,7 &#64;&#64; struct wait_page_queue {<br /> <br /> static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)<br /> {<br />+	int ret;<br /> 	struct wait_page_key *key = arg;<br /> 	struct wait_page_queue *wait_page<br /> 		= container_of(wait, struct wait_page_queue, wait);<br />&#64;&#64; -1013,18 +1014,40 &#64;&#64; static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync,<br /> 	if (wait_page-&gt;bit_nr != key-&gt;bit_nr)<br /> 		return 0;<br /> <br />+	/* Stop walking if it's locked */<br />+	if (wait-&gt;flags &amp; WQ_FLAG_EXCLUSIVE) {<br />+		if (test_and_set_bit(key-&gt;bit_nr, &amp;key-&gt;page-&gt;flags))<br />+			return -1;<br />+	} else {<br />+		if (test_bit(key-&gt;bit_nr, &amp;key-&gt;page-&gt;flags))<br />+			return -1;<br />+	}<br />+<br /> 	/*<br />-	 * Stop walking if it's locked.<br />-	 * Is this safe if put_and_wait_on_page_locked() is in use?<br />-	 * Yes: the waker must hold a reference to this page, and if PG_locked<br />-	 * has now already been set by another task, that task must also hold<br />-	 * a reference to the *same usage* of this page; so there is no need<br />-	 * to walk on to wake even the put_and_wait_on_page_locked() callers.<br />+	 * Let the waiter know we have done the page flag<br />+	 * handling for it (and the return value lets the<br />+	 * wakeup logic count exclusive wakeup events).<br /> 	 */<br />-	if (test_bit(key-&gt;bit_nr, &amp;key-&gt;page-&gt;flags))<br />-		return -1;<br />+	ret = (wait-&gt;flags &amp; WQ_FLAG_EXCLUSIVE) != 0;<br />+	wait-&gt;flags |= WQ_FLAG_WOKEN;<br />+	wake_up_state(wait-&gt;private, mode);<br /> <br />-	return autoremove_wake_function(wait, mode, sync, key);<br />+	/*<br />+	 * Ok, we have successfully done what we're waiting for,<br />+	 * and we can unconditionally remove the wait entry.<br />+	 *<br />+	 * Note that this has to be the absolute last thing we do,<br />+	 * since after list_del_init(&amp;wait-&gt;entry) the wait entry<br />+	 * might be de-allocated and the process might even have<br />+	 * exited.<br />+	 *<br />+	 * We _really_ should have a "list_del_init_careful()" to<br />+	 * properly pair with the unlocked "list_empty_careful()"<br />+	 * in finish_wait().<br />+	 */<br />+	smp_mb();<br />+	list_del_init(&amp;wait-&gt;entry);<br />+	return ret;<br /> }<br /> <br /> static void wake_up_page_bit(struct page *page, int bit_nr)<br />&#64;&#64; -1103,16 +1126,22 &#64;&#64; enum behavior {<br /> 			 */<br /> };<br /> <br />+static inline int trylock_page_bit_common(struct page *page, int bit_nr,<br />+	enum behavior behavior)<br />+{<br />+	return behavior == EXCLUSIVE ?<br />+		!test_and_set_bit(bit_nr, &amp;page-&gt;flags) :<br />+		!test_bit(bit_nr, &amp;page-&gt;flags);<br />+}<br />+<br /> static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br /> 	struct page *page, int bit_nr, int state, enum behavior behavior)<br /> {<br /> 	struct wait_page_queue wait_page;<br /> 	wait_queue_entry_t *wait = &amp;wait_page.wait;<br />-	bool bit_is_set;<br /> 	bool thrashing = false;<br /> 	bool delayacct = false;<br /> 	unsigned long pflags;<br />-	int ret = 0;<br /> <br /> 	if (bit_nr == PG_locked &amp;&amp;<br /> 	    !PageUptodate(page) &amp;&amp; PageWorkingset(page)) {<br />&#64;&#64; -1130,48 +1159,44 &#64;&#64; static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br /> 	wait_page.page = page;<br /> 	wait_page.bit_nr = bit_nr;<br /> <br />+	/*<br />+	 * Add ourselves to the wait queue.<br />+	 *<br />+	 * NOTE! This is where we also check the page<br />+	 * state synchronously the last time to see that<br />+	 * somebody didn't just clear the bit. Do the<br />+	 * SetPageWaiters() before that to let anybody<br />+	 * we just miss know they need to wake us up.<br />+	 */<br />+	spin_lock_irq(&amp;q-&gt;lock);<br />+	SetPageWaiters(page);<br />+	if (!trylock_page_bit_common(page, bit_nr, behavior))<br />+		__add_wait_queue_entry_tail(q, wait);<br />+	else<br />+		wait-&gt;flags |= WQ_FLAG_WOKEN;<br />+	spin_unlock_irq(&amp;q-&gt;lock);<br />+<br />+	/*<br />+	 * From now on, all the logic will be based on<br />+	 * whether the wait entry is on the queue or not,<br />+	 * and the page bit testing (and setting) will be<br />+	 * done by the wake function, not us.<br />+	 *<br />+	 * We can drop our reference to the page.<br />+	 */<br />+	if (behavior == DROP)<br />+		put_page(page);<br />+<br /> 	for (;;) {<br />-		spin_lock_irq(&amp;q-&gt;lock);<br />-<br />-		if (likely(list_empty(&amp;wait-&gt;entry))) {<br />-			__add_wait_queue_entry_tail(q, wait);<br />-			SetPageWaiters(page);<br />-		}<br />-<br /> 		set_current_state(state);<br /> <br />-		spin_unlock_irq(&amp;q-&gt;lock);<br />-<br />-		bit_is_set = test_bit(bit_nr, &amp;page-&gt;flags);<br />-		if (behavior == DROP)<br />-			put_page(page);<br />-<br />-		if (likely(bit_is_set))<br />-			io_schedule();<br />-<br />-		if (behavior == EXCLUSIVE) {<br />-			if (!test_and_set_bit_lock(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />-		} else if (behavior == SHARED) {<br />-			if (!test_bit(bit_nr, &amp;page-&gt;flags))<br />-				break;<br />-		}<br />-<br />-		if (signal_pending_state(state, current)) {<br />-			ret = -EINTR;<br />+		if (signal_pending_state(state, current))<br /> 			break;<br />-		}<br /> <br />-		if (behavior == DROP) {<br />-			/*<br />-			 * We can no longer safely access page-&gt;flags:<br />-			 * even if CONFIG_MEMORY_HOTREMOVE is not enabled,<br />-			 * there is a risk of waiting forever on a page reused<br />-			 * for something that keeps it locked indefinitely.<br />-			 * But best check for -EINTR above before breaking.<br />-			 */<br />+		if (wait-&gt;flags &amp; WQ_FLAG_WOKEN)<br /> 			break;<br />-		}<br />+<br />+		io_schedule();<br /> 	}<br /> <br /> 	finish_wait(q, wait);<br />&#64;&#64; -1190,7 +1215,7 &#64;&#64; static inline int wait_on_page_bit_common(wait_queue_head_t *q,<br /> 	 * bother with signals either.<br /> 	 */<br /> <br />-	return ret;<br />+	return wait-&gt;flags &amp; WQ_FLAG_WOKEN ? 0 : -EINTR;<br /> }<br /> <br /> void wait_on_page_bit(struct page *page, int bit_nr)<br />-- <br />2.28.0.rc0.3.g1e25d3a62f<br />From ddc00aaf8e020bab630ec641b37564634454634c Mon Sep 17 00:00:00 2001<br />From: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />Date: Thu, 23 Jul 2020 12:33:41 -0700<br />Subject: [PATCH 2/2] list: add "list_del_init_careful()" to go with<br /> "list_empty_careful()"<br /><br />That gives us ordering guarantees around the pair.<br /><br />Signed-off-by: Linus Torvalds &lt;torvalds&#64;linux-foundation.org&gt;<br />---<br /> include/linux/list.h | 20 +++++++++++++++++++-<br /> kernel/sched/wait.c  |  2 +-<br /> mm/filemap.c         |  7 +------<br /> 3 files changed, 21 insertions(+), 8 deletions(-)<br /><br />diff --git a/include/linux/list.h b/include/linux/list.h<br />index aff44d34f4e4..0d0d17a10d25 100644<br />--- a/include/linux/list.h<br />+++ b/include/linux/list.h<br />&#64;&#64; -282,6 +282,24 &#64;&#64; static inline int list_empty(const struct list_head *head)<br /> 	return READ_ONCE(head-&gt;next) == head;<br /> }<br /> <br />+/**<br />+ * list_del_init_careful - deletes entry from list and reinitialize it.<br />+ * &#64;entry: the element to delete from the list.<br />+ *<br />+ * This is the same as list_del_init(), except designed to be used<br />+ * together with list_empty_careful() in a way to guarantee ordering<br />+ * of other memory operations.<br />+ *<br />+ * Any memory operations done before a list_del_init_careful() are<br />+ * guaranteed to be visible after a list_empty_careful() test.<br />+ */<br />+static inline void list_del_init_careful(struct list_head *entry)<br />+{<br />+	__list_del_entry(entry);<br />+	entry-&gt;prev = entry;<br />+	smp_store_release(&amp;entry-&gt;next, entry);<br />+}<br />+<br /> /**<br />  * list_empty_careful - tests whether a list is empty and not being modified<br />  * &#64;head: the list to test<br />&#64;&#64; -297,7 +315,7 &#64;&#64; static inline int list_empty(const struct list_head *head)<br />  */<br /> static inline int list_empty_careful(const struct list_head *head)<br /> {<br />-	struct list_head *next = head-&gt;next;<br />+	struct list_head *next = smp_load_acquire(&amp;head-&gt;next);<br /> 	return (next == head) &amp;&amp; (next == head-&gt;prev);<br /> }<br /> <br />diff --git a/kernel/sched/wait.c b/kernel/sched/wait.c<br />index ba059fbfc53a..01f5d3020589 100644<br />--- a/kernel/sched/wait.c<br />+++ b/kernel/sched/wait.c<br />&#64;&#64; -389,7 +389,7 &#64;&#64; int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, i<br /> 	int ret = default_wake_function(wq_entry, mode, sync, key);<br /> <br /> 	if (ret)<br />-		list_del_init(&amp;wq_entry-&gt;entry);<br />+		list_del_init_careful(&amp;wq_entry-&gt;entry);<br /> <br /> 	return ret;<br /> }<br />diff --git a/mm/filemap.c b/mm/filemap.c<br />index 1143c0652d81..239d156a38ea 100644<br />--- a/mm/filemap.c<br />+++ b/mm/filemap.c<br />&#64;&#64; -1040,13 +1040,8 &#64;&#64; static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync,<br /> 	 * since after list_del_init(&amp;wait-&gt;entry) the wait entry<br /> 	 * might be de-allocated and the process might even have<br /> 	 * exited.<br />-	 *<br />-	 * We _really_ should have a "list_del_init_careful()" to<br />-	 * properly pair with the unlocked "list_empty_careful()"<br />-	 * in finish_wait().<br /> 	 */<br />-	smp_mb();<br />-	list_del_init(&amp;wait-&gt;entry);<br />+	list_del_init_careful(&amp;wait-&gt;entry);<br /> 	return ret;<br /> }<br /> <br />-- <br />2.28.0.rc0.3.g1e25d3a62f<br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
