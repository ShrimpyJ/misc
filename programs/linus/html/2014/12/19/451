    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2014/11/14/656">First message in thread</a></li><li><a href="/lkml/2014/12/19/417">Linus Torvalds</a><ul><li><a href="/lkml/2014/12/19/439">Thomas Gleixner</a><ul><li class="origin"><a href="/lkml/2014/12/20/130">Linus Torvalds</a><ul><li><a href="/lkml/2014/12/20/130">Linus Torvalds</a><ul><li><a href="/lkml/2014/12/20/169">Linus Torvalds</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Fri, 19 Dec 2014 17:57:05 -0800</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: frequent lockups in 3.18rc4</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Fri, Dec 19, 2014 at 5:00 PM, Thomas Gleixner &lt;tglx&#64;linutronix.de&gt; wrote:<br />&gt;<br />&gt; The watchdog timer runs on a fully periodic schedule. It's self<br />&gt; rearming via<br />&gt;<br />&gt;          hrtimer_forward_now(hrtimer, ns_to_ktime(sample_period));<br />&gt;<br />&gt; So if that aligns with the equally periodic tick interrupt on the<br />&gt; other CPU then you might get into that situation due to the fully<br />&gt; synchronizing and serializing nature of HPET reads.<br /><br />No. Really. No.<br /><br />There is no way in hell that a modern system ends up being that<br />synchronized. Not to the point that the interrupt also happens at the<br />*exact* same point.<br /><br />Thomas, you're in denial. It's not just the exact same instruction in<br />the timer interrupt handler ("timer interrupts and NMI's are in<br />sync"), the call chain *past* the timer interrupt is the exact same<br />thing too.  The stack has the same contents. So not only must the two<br />CPU's be perfectly in sync so that the NMI always triggers on exactly<br />the same instruction, the CPU that takes the timer interrupt (in your<br />schenario, over and over again) must be magically filling the stack<br />with the exact same thing every time.<br /><br />That CPU isn't making progress that just happens to be "synchronized".<br /><br />Admit it, you've not seen a busy system that is *so* synchronized that<br />you get timer interrupts and NMI's that hit on the same instruction<br />over a sequence of minutes.<br /><br />&gt; So lets assume there is that hrtimer_remove race (I'm certainly going<br />&gt; to stare at that tomorrow with fully awake brain. It's past beer<br />&gt; o'clock here). How do you explain that:<br />&gt;<br />&gt; 1) the watchdog always triggers on CPU1?<br />&gt;<br />&gt; 2) the race only ever happens on CPU0?<br /><br />I'm claiming that the race happened *once*. And it then corrupted some<br />data structure or similar sufficiently that CPU0 keeps looping.<br /><br />Perhaps something keeps re-adding itself to the head of the timerqueue<br />due to the race.<br /><br />The watchdog doesn't trigger on CPU0, because this is the software<br />watchdog, and interrupts are disabled on CPU0, and CPU0 isn't making<br />any progress. Because it's looping in a fairly tight loop.<br /><br />The watchdog triggers on CPU1 (over and over again) because CPU1 is<br />waiting for the TLB shootdown to complete. And it doesn't, because<br />interrupts are disabled on CPU0 that it's trying to shoot down the TLB<br />on.<br /><br />That theory at least fits the data. So CPU1 isn't doing anything odd at all.<br /><br />In a way that "magic happens so that everything is so synchronized<br />that you cross-syncornize two CPU's making real progress over many<br />minutes". THAT sounds like just a fairy tale to me.<br /><br />Your argument "it has a question mark in front of it" objection is<br />bogus. We got an *interrupt* in the middle of the call chain. Of<br />*course* the call chain is unreliable. It doesn't matter. What matters<br />is that the stack under the interrupt DOES NOT CHANGE. It doesn't even<br />matter if it's a real honest-to-god callchain or not, what matters is<br />that the kernel stack under the interrupt is unchanged. No way does<br />that happen if it's making progress at the same time.<br /><br />&gt; 3) the hrtimer interrupt took too long message never appears?<br /><br />At a guess, it's looping (for a long long time) on<br />timerqueue_getnext() in hrtimer_interrupt(), and then returns. Never<br />gets to the retry or the "interrupt took %llu" messages.<br /><br />And we know what timer entry it is looping on:  tick_sched_timer.<br />Which does a HRTIMER_RESTART, so the re-enqueueing isn't exactly<br />unlikely.<br /><br />All it needs is that the re-enqueueing has gotten confused enough that<br />it re-enqueues it on the same queue over and over again.  Which would<br />run tick_sched_timer over and over again. No? Until the re-enqueuing<br />magically stops (and we do actually have a HPET counter overflow<br />there. Just look at the RAX values:<br /><br /> RAX: 00000000fb9d303f<br /> RAX: 000000002b67efe4<br /><br />that 00000000fb9d303f is the last time we see that particular<br />callchain. The next time we see hpet_read(), it's that<br />000000002b67efe4 thing.<br /><br />So my "maybe it has something to do with HPET overflow" wasn't just a<br />random throw-away comment. We actually have real data saying that the<br />HPET *did* overflow, and it in fact happened somewhere around the time<br />when the lockup went away.<br /><br />Are they related? Maybe not. But dammit, there's a lot of<br />"coincidences" here. Not just the "oh, it always takes the NMI on the<br />exact same instruction".<br /><br />&gt; 4) the RT throttler hit?<br />&gt; 5) that the system makes progress afterwards?<br /><br />.. something eventually clears the problem, maybe because of the HPET<br />overflow. I dunno. I'm just saying that your arguments to ignore CPU0<br />are pretty damn weak.<br /><br />So I think the CPU1 behavior is 100% consistent with CPU0 just having<br />interrupts disabled.<br /><br />So I think CPU1 is _trivial_ to explain if you accept that CPU0 is<br />doing something weird.<br /><br />Which is why I think your "look at CPU1" sounds so strange. I don't<br />think CPU1 is all that interesting. I can *trivially* explain it with<br />a single sentence, and did exactly that above.<br /><br />Can you trivially explain CPU0? Without the "it's just a big<br />coincidence that we take  the NMI on the same instruction for several<br />minutes, and the stack under the timer interrupt hasn't changed at all<br />in that same time".<br /><br />                     Linus<br /><br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
