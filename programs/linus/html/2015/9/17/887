    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2015/9/11/570">First message in thread</a></li><li><a href="/lkml/2015/9/17/851">Chris Mason</a><ul><li><a href="/lkml/2015/9/17/865">Dave Chinner</a><ul><li class="origin"><a href="/lkml/2015/9/18/30">Linus Torvalds</a><ul><li><a href="/lkml/2015/9/18/30">Dave Chinner</a><ul><li><a href="/lkml/2015/9/18/44">Linus Torvalds</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2015/9/17/887/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Thu, 17 Sep 2015 18:50:29 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH] fs-writeback: drop wb-&gt;list_lock during blk_finish_plug()</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Thu, Sep 17, 2015 at 5:37 PM, Dave Chinner &lt;david&#64;fromorbit.com&gt; wrote:<br />&gt;&gt; &gt;<br />&gt;&gt; &gt; I'm not seeing why that should be an issue. Sure, there's some CPU<br />&gt;&gt; &gt; overhead to context switching, but I don't see that it should be that<br />&gt;&gt; &gt; big of a deal.<br />&gt;<br />&gt; It may well change the dispatch order of enough IOs for it to be<br />&gt; significant on an IO bound device.<br /><br />Hmm. Maybe. We obviously try to order the IO's a bit by inode, and I<br />could see the use of a workqueue maybe changing that sufficiently. But<br />it still sounds a bit unlikely.<br /><br />And in fact, I think I have a better explanation.<br /><br />&gt; In outright performance on my test machine, the difference in<br />&gt; files/s is noise. However, the consistency looks to be substantially<br />&gt; improved and the context switch rate is now running at under<br />&gt; 3,000/sec.<br /><br />Hmm. I don't recall seeing you mention how many context switches per<br />second you had before. What is it down from?<br /><br />However, I think I may have found something more interesting here.<br /><br />The fact is that a *normal* schedule will trigger that whole<br />blk_schedule_flush_plug(), but a cond_sched() or a cond_sched_lock()<br />doesn't actually do a normal schedule at all. Those trigger a<br />*preemption* schedule.<br /><br />And a preemption schedule does not trigger that unplugging at all.<br />Why? A kernel "preemption" very much tries to avoid touching thread<br />state, because the whole point is that normally we may be preempting<br />threads in random places, so we don't run things like<br />sched_submit_work(), because the thread may be in the middle of<br />*creating* that work, and we don't want to introduce races. The<br />preemption scheduling can also be done with "task-&gt;state" set to<br />sleeping, and it won't actually sleep.<br /><br />Now, for the explicit schedules like "cond_resched()" and<br />"cond_resched_lock()", those races with obviously don't exist, but<br />they happen to share the same preemption scheduling logic.<br /><br />So it turns out that as far as I can see, the whole "cond_resched()"<br />will not start any IO at all, and it will just be left on the thread<br />plug until we schedule back to the thread.<br /><br />So I don't think this has anything to do with kblockd_workqueue. I<br />don't think it even gets to that point.<br /><br />I may be missing something, but just to humor me, can you test the<br />attached patch *without* Chris's patch to do explicit plugging? This<br />should make cond_resched() and cond_resched_lock() run the unplugging.<br /><br />It may be entirely broken, I haven't thought this entirely through.<br /><br />                   Linus<br />diff --git a/kernel/sched/core.c b/kernel/sched/core.c<br />index 97d276ff1edb..388ea9e7ab8a 100644<br />--- a/kernel/sched/core.c<br />+++ b/kernel/sched/core.c<br />&#64;&#64; -4548,6 +4548,7 &#64;&#64; SYSCALL_DEFINE0(sched_yield)<br /> int __sched _cond_resched(void)<br /> {<br /> 	if (should_resched(0)) {<br />+		sched_submit_work(current);<br /> 		preempt_schedule_common();<br /> 		return 1;<br /> 	}<br />&#64;&#64; -4572,9 +4573,10 &#64;&#64; int __cond_resched_lock(spinlock_t *lock)<br /> <br /> 	if (spin_needbreak(lock) || resched) {<br /> 		spin_unlock(lock);<br />-		if (resched)<br />+		if (resched) {<br />+			sched_submit_work(current);<br /> 			preempt_schedule_common();<br />-		else<br />+		} else<br /> 			cpu_relax();<br /> 		ret = 1;<br /> 		spin_lock(lock);</pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
