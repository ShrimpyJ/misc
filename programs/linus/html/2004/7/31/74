    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2004/7/26/94">First message in thread</a></li><li><a href="/lkml/2004/7/30/100">Marcelo Tosatti</a><ul><li><a href="/lkml/2004/7/30/152">Andrew Morton</a><ul><li><a href="/lkml/2004/7/30/230">Marcelo Tosatti</a></li><li class="origin"><a href="/lkml/2004/7/31/115">Linus Torvalds</a><ul><li><a href="/lkml/2004/7/31/115">Andrew Morton</a><ul><li><a href="/lkml/2004/8/3/18">Nick Piggin</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2004/7/31/74/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Sat, 31 Jul 2004 10:23:10 -0700 (PDT)</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: dentry cache leak? Re: rsync out of memory 2.6.8-rc2</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody"><br /><br />On Fri, 30 Jul 2004, Andrew Morton wrote:<br />&gt; <br />&gt; Seems that we reach a state where lowmem pagecache get reclaimed faster<br />&gt; than dcache/icache.  This causes the number of pages scanned for lowmem<br />&gt; allocations to fall.  This causes less scanning of the slab and the whole<br />&gt; thing repeats.  I expect changing nr_used_zone_pages() to ignore highmem<br />&gt; will fix it, and might be the long-term fix, too.<br /><br />Ouch. Indeed, looking at the usage of nr_used_zone_pages(), it looks <br />totally broken. However, I don't think the right thing to do is to ignore <br />the HIGHMEM zone altogether, I think it should take the list of zones <br />being shrunk into account.<br /><br />Look at usage: one caller of shrink_slab() is try_to_free_pages(), and if <br />try_to_free_pages() is called for a HIGHMEM zone, then it is _fine_ to <br />count the HIGHMEM zone as pages, and shrink the slab memory much less <br />aggressively. The _problem_ is when slab competes against itself (lowmem) <br />or some other thing (inodes) that want lowmem-only memory, and then the <br />slab shrinking really should ignore the fact that there are tons of <br />highmem pages left.<br /><br />Something like this (totally untested, may not compile, you get the idea) <br />might work. Or not. Since the _rest_ of "shrink_slab()" doesn't know about <br />zonelists, just making the "how many pages does this zone have free" take <br />the zonelist into account might cause other problems.<br /><br />		Linus<br />-----<br />===== include/linux/mm.h 1.179 vs edited =====<br />--- 1.179/include/linux/mm.h	2004-06-27 00:19:35 -07:00<br />+++ edited/include/linux/mm.h	2004-07-31 10:19:25 -07:00<br />&#64;&#64; -706,7 +706,7 &#64;&#64;<br /> <br /> extern struct vm_area_struct *find_extend_vma(struct mm_struct *mm, unsigned long addr);<br /> <br />-extern unsigned int nr_used_zone_pages(void);<br />+extern unsigned int nr_used_zone_pages(struct zone **);<br /> <br /> extern struct page * vmalloc_to_page(void *addr);<br /> extern struct page * follow_page(struct mm_struct *mm, unsigned long address,<br />===== mm/page_alloc.c 1.221 vs edited =====<br />--- 1.221/mm/page_alloc.c	2004-07-19 08:44:36 -07:00<br />+++ edited/mm/page_alloc.c	2004-07-31 10:18:37 -07:00<br />&#64;&#64; -825,13 +825,15 &#64;&#64;<br /> <br /> EXPORT_SYMBOL(nr_free_pages);<br /> <br />-unsigned int nr_used_zone_pages(void)<br />+unsigned int nr_used_zone_pages(struct zone **zones)<br /> {<br /> 	unsigned int pages = 0;<br />-	struct zone *zone;<br /> <br />-	for_each_zone(zone)<br />+	while (*zones) {<br />+		struct zone * zone = *zones;<br />+		zones++;<br /> 		pages += zone-&gt;nr_active + zone-&gt;nr_inactive;<br />+	}<br /> <br /> 	return pages;<br /> }<br />===== mm/vmscan.c 1.224 vs edited =====<br />--- 1.224/mm/vmscan.c	2004-06-24 01:56:14 -07:00<br />+++ edited/mm/vmscan.c	2004-07-31 10:20:17 -07:00<br />&#64;&#64; -170,7 +170,7 &#64;&#64;<br />  *<br />  * We do weird things to avoid (scanned*seeks*entries) overflowing 32 bits.<br />  */<br />-static int shrink_slab(unsigned long scanned, unsigned int gfp_mask)<br />+static int shrink_slab(unsigned long scanned, unsigned int gfp_mask, struct zone **zones)<br /> {<br /> 	struct shrinker *shrinker;<br /> 	long pages;<br />&#64;&#64; -178,7 +178,7 &#64;&#64;<br /> 	if (down_trylock(&amp;shrinker_sem))<br /> 		return 0;<br /> <br />-	pages = nr_used_zone_pages();<br />+	pages = nr_used_zone_pages(zones);<br /> 	list_for_each_entry(shrinker, &amp;shrinker_list, list) {<br /> 		unsigned long long delta;<br /> <br />&#64;&#64; -912,7 +912,7 &#64;&#64;<br /> 		sc.nr_reclaimed = 0;<br /> 		sc.priority = priority;<br /> 		shrink_caches(zones, &amp;sc);<br />-		shrink_slab(sc.nr_scanned, gfp_mask);<br />+		shrink_slab(sc.nr_scanned, gfp_mask, zones);<br /> 		if (reclaim_state) {<br /> 			sc.nr_reclaimed += reclaim_state-&gt;reclaimed_slab;<br /> 			reclaim_state-&gt;reclaimed_slab = 0;<br />&#64;&#64; -1032,6 +1032,7 &#64;&#64;<br /> 		 */<br /> 		for (i = 0; i &lt;= end_zone; i++) {<br /> 			struct zone *zone = pgdat-&gt;node_zones + i;<br />+			struct zone * zones[2] = { zone, NULL };<br /> <br /> 			if (zone-&gt;all_unreclaimable &amp;&amp; priority != DEF_PRIORITY)<br /> 				continue;<br />&#64;&#64; -1048,7 +1049,7 &#64;&#64;<br /> 			sc.priority = priority;<br /> 			shrink_zone(zone, &amp;sc);<br /> 			reclaim_state-&gt;reclaimed_slab = 0;<br />-			shrink_slab(sc.nr_scanned, GFP_KERNEL);<br />+			shrink_slab(sc.nr_scanned, GFP_KERNEL, zones);<br /> 			sc.nr_reclaimed += reclaim_state-&gt;reclaimed_slab;<br /> 			total_reclaimed += sc.nr_reclaimed;<br /> 			if (zone-&gt;all_unreclaimable)<br />-<br />To unsubscribe from this list: send the line "unsubscribe linux-kernel" in<br />the body of a message to majordomo&#64;vger.kernel.org<br />More majordomo info at  <a href="http://vger.kernel.org/majordomo-info.html">http://vger.kernel.org/majordomo-info.html</a><br />Please read the FAQ at  <a href="http://www.tux.org/lkml/">http://www.tux.org/lkml/</a><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
