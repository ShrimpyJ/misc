    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2000/10/23/157">First message in thread</a></li><li><a href="/lkml/2000/10/23/157">Jeff Garzik</a><ul><li class="origin"><a href="">(Linus Torvalds)</a></li><li><a href="/lkml/2000/10/24/49">"Dan Maas"</a></li><li><a href="/lkml/2000/10/24/75">"David S. Miller"</a></li><li><a href="/lkml/2000/10/23/105">Chris Evans</a></li><li><a href="/lkml/2000/10/23/165">Jeff Garzik</a></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">(Linus Torvalds)</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: LMbench 2.4.0-test10pre-SMP vs. 2.2.18pre-SMP</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">24 Oct 2000 10:21:52 -0700</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">In article &lt;Pine.LNX.4.21.0010232338440.1762-100000&#64;ferret.lmh.ox.ac.uk&gt;,<br />Chris Evans  &lt;chris&#64;scary.beasts.org&gt; wrote:<br />&gt;<br />&gt;On Mon, 23 Oct 2000, Jeff Garzik wrote:<br />&gt;<br />&gt;&gt; First test was with 2.4.0-test10-pre3.<br />&gt;&gt; Next four tests were with 2.4.0-test10-pre4.<br />&gt;&gt; Final four tests were with 2.2.18-pre17.<br />&gt;&gt; <br />&gt;&gt; All are 'virgin' kernels, without any patches.<br />&gt;<br />&gt;[...]<br />&gt;<br />&gt;I'll take the liberty of highlighting some big changes, v2.2 vs v2.4<br />&gt;<br />&gt;*Local* Communication latencies in microseconds - smaller is better<br />&gt;-------------------------------------------------------------------<br />&gt;Host                 OS 2p/0K  Pipe AF     UDP  RPC/   TCP  RPC/ TCP<br />&gt;                        ctxsw       UNIX         UDP         TCP conn<br />&gt;--------- ------------- ----- ----- ---- ----- ----- ----- ----- ----<br />&gt;rum.normn Linux 2.4.0-t     6    20   45    63   106    81   157  146<br />&gt;rum.normn Linux 2.2.18p     2    12   18    56   123   106   159  237<br />&gt;<br />&gt;- So we broke pipe/AF UNIX latencies<br /><br />Not really.<br /><br />The issue is that under 2.4.x, the Linux scheduler is very actively<br />trying to spread out the load across multiple CPU's. It was a hard<br />decision to make, exactly because it tends to make the lmbench context<br />switch numbers higher - the way lmbench does context switch testing is<br />to pass a simple packet back and forth between pipes, and it's actually<br />entirely synchronous. So you get the best lmbench numbers when both the<br />receiver and the sender stay on the same CPU's (mainly for cache<br />reasons: everything stays on the same CPU, no spinlock movement, and no<br />pipe data movement from one CPU to another).<br /><br />This was something people (mainly Ingo) were very conscious of when<br />doing scheduler changes: lmbench was something that we all ran all the<br />time for this, and it was not pretty to see the numbers go up when you<br />schedule on another CPU.<br /><br />But in real life, the advantage of spreading out is actually noticeable.<br />In real life you don't tend to have quite as synchronous a data passing:<br />you have the pipe writer continue to generate data, while the pipe<br />reader actually _does_ something with the data, and spreading out on<br />multiple CPU's means that this work can be done in parallell.<br /><br />Oh, well. <br /><br />(I don't actually know why AF UNIX went up, but it might be the same<br />issue. The new networking code is fairly asynchronous, which _really_<br />improves performance because it is able to actually make good use of<br />multiple CPU's unlike the old code, but probably for similar reasons as<br />the pipe latency thing this is bad for AF UNIX. I don't think anybody<br />has bothered looking that much: a lot more work has been put into TCP<br />than into AF UNIX).<br /><br />&gt;File &amp; VM system latencies in microseconds - smaller is better<br />&gt;--------------------------------------------------------------<br />&gt;Host                 OS   0K File      10K File      Mmap    Prot    Page       <br />&gt;                        Create Delete Create Delete  Latency Fault   Fault <br />&gt;--------- ------------- ------ ------ ------ ------  ------- -----   ----- <br />&gt;rum.normn Linux 2.4.0-t     15      1     28      3     1016     1    0.0K<br />&gt;rum.normn Linux 2.2.18p     16      1     29      2     7658     2    0.6K<br />&gt;<br />&gt;- But gave steroids to mmap latencies<br /><br />This is due to all the VM layer changes. That mmap latency thing is<br />basically due to the new page-cache stuff that made us kick ass on all<br />the benchmarks that 2.2.x was bad at (ie mindcraft etc).<br /><br />&gt;*Local* Communication bandwidths in MB/s - bigger is better<br />&gt;-----------------------------------------------------------<br />&gt;Host                OS  Pipe AF    TCP  File   Mmap  Bcopy  Bcopy  Mem Mem<br />&gt;                             UNIX      reread reread (libc) (hand) read write<br />&gt;--------- ------------- ---- ---- ---- ------ ------ ------ ------ ---- -----<br />&gt;rum.normn Linux 2.4.0-t  152  105   98    151    326    138    144  326 171<br />&gt;rum.normn Linux 2.2.18p  264  106   55    152    326    137    142  326 180<br />&gt;<br />&gt;- Mixed fortunes here. A serious boost to TCP bandwidth but pipe bandwidth<br />&gt;dies a bit<br /><br />The pipe bandwidth is intimately related to pipe latency.  Linux pipes<br />are fairly small (only 4kB worth of data buffer), so they need good<br />latency for good performance.  Many of the same arguments apply (ie<br />bandwidth if you actually _use_ the data as opposed to just benchmark<br />with it is probably better on 2.4.x)<br /><br />The pipe bandwidth could be fairly easily improved by just doubling the<br />buffer size (or by using VM tricks), but it's not been something that<br />anybody has felt was all that important in real life.<br /><br />Yeah, I'm probably making excuses. Some things in 2.4.x are slower<br />simply because it has more locks than 2.2.x. The big kernel lock in<br />2.2.x is actually the best for performance on many single-threaded<br />things, and going to more fine-grained locking has sometimes cost us.<br />But some of the changes (the page cache in particular) has caused 2.4.x<br />to win BIG.<br /><br />The biggest advantage of 2.4.x as far as I'm concerned is that even on a<br />pretty regular run-of-the-mill dual CPU machine, 2.4.x just _feels_<br />better under any CPU load. Much smoother behaviour, mainly because 2.2.x<br />sometimes holds the kernel lock enough to make scheduling jerky.<br /><br />But I bet you can find work-loads where the reverse is true also.<br /><br />			Linus<br />-<br />To unsubscribe from this list: send the line "unsubscribe linux-kernel" in<br />the body of a message to majordomo&#64;vger.kernel.org<br />Please read the FAQ at <a href="http://www.tux.org/lkml/">http://www.tux.org/lkml/</a><br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
