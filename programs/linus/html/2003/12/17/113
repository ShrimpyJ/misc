    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2003/12/15/70">First message in thread</a></li><li><a href="/lkml/2003/12/16/57">Linus Torvalds</a><ul><li><a href="/lkml/2003/12/16/102">Mike Fedyk</a><ul><li><a href="/lkml/2003/12/16/104">Linus Torvalds</a><ul><li><a href="/lkml/2003/12/17/26">=?iso-8859-1?Q?J=F6rn?= Engel</a></li><li><a href="/lkml/2003/12/17/36">Peter Zaitsev</a><ul><li><a href="/lkml/2003/12/17/67">Linus Torvalds</a></li><li><a href="/lkml/2003/12/17/82">(bill davidsen)</a></li></ul></li></ul></li></ul></li><li><a href="/lkml/2003/12/17/106">Jamie Lokier</a><ul><li class="origin"><a href="/lkml/2003/12/17/146">Linus Torvalds</a><ul><li><a href="/lkml/2003/12/17/146">(bill davidsen)</a></li></ul></li><li><a href="/lkml/2003/12/17/173">jw schultz</a></li></ul></li><li><a href="/lkml/2003/12/17/142">(bill davidsen)</a><ul><li><a href="/lkml/2003/12/17/171">jw schultz</a></li></ul></li><li><a href="/lkml/2004/1/7/360">Greg Stark</a></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Wed, 17 Dec 2003 11:40:33 -0800 (PST)</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: raid0 slower than devices it is assembled of?</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody"><br /><br />On Wed, 17 Dec 2003, Jamie Lokier wrote:<br />&gt; <br />&gt; If a large fs-level I/O transaction is split into lots of 32k<br />&gt; transactions by the RAID layer, many of those 32k transactions will be<br />&gt; contiguous on the disks.<br /><br />Yes.<br /><br />&gt; That doesn't mean they're contiguous from the fs point of view, but<br />&gt; given that all modern hardware does scatter-gather, shouldn't the<br />&gt; contiguous transactions be merged before being sent to the disk?<br /><br />Yes, as long as the RAID layer (or lowlevel disk) doesn't try to avoid the <br />elevator.<br /><br />BUT (and this is a big but) - apart from wasting a lot of CPU time by<br />splitting and re-merging, the problem is more fundamental than that.<br /><br />Let's say that you are striping four disks, with 32kB blocking. Not <br />an unreasonable setup.<br /><br />Now, let's say that the contiguous block IO from high up is 256kB in size. <br />Again, this is not unreasonable, although it is actually larger than a lot <br />of IO actually is (it is smaller than _some_ IO patterns, but on the whole <br />I'm willing to bet that it's in the "high 1%" of the IO done).<br /><br />Now, we can split that up in 32kB blocks (8 of them), and then merge it<br />back into 4 64kB blocks sent to disk. We can even avoid a lot of the CPU<br />overhead by not merging in the first place (and I think we largely do,<br />actually), and just generate 4 64kB requests in the first place.<br /><br />But did you notice something?<br /><br />In one schenario, the disk got a 256kB request, in the other it got a 64kB <br />requests.<br /><br />And guess what? The bigger request is likely to be more efficient.  <br />Normal disks these days have 8MB+ of cache on the disk, and do partial <br />track buffering etc, and the bigger the requests are, the better.<br /><br />&gt; It may strain the CPU (splitting and merging in a different order lots<br />&gt; of requests), but I don't see why it should kill I/O access patterns,<br />&gt; as they can be as large as if you had large stripes in the first place.<br /><br />But you _did_ kill the IO access patterns. You started out with a 256kB <br />IO, and you ended up splitting it in four. You lose.<br /><br />The thing is, in real life you do NOT have "infinite IO blocks" to start <br />with. If that were true, splitting it up across the disks wouldn't cost <br />you anything: infinite divided by four is still infinite. But in real life <br />you have something that is already of a finite length and a few hundred kB <br />is "good" in most normal loads - and splitting it in four is a BAD IDEA!<br /><br />In contrast, imagine that you had a 1MB stripe. Most of the time the 256kB <br />request wouldn't be split at all, and even in the worst case it would get <br />split into just 2 requests.<br /><br />Yes, there are some loads where you can get largely "infinite" request <br />sizes. But I'd claim that they are quite rare.<br /><br />			Linus<br />-<br />To unsubscribe from this list: send the line "unsubscribe linux-kernel" in<br />the body of a message to majordomo&#64;vger.kernel.org<br />More majordomo info at  <a href="http://vger.kernel.org/majordomo-info.html">http://vger.kernel.org/majordomo-info.html</a><br />Please read the FAQ at  <a href="http://www.tux.org/lkml/">http://www.tux.org/lkml/</a><br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
