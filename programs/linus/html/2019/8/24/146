    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2019/8/20/1203">First message in thread</a></li><li><a href="/lkml/2019/8/23/683">Linus Torvalds</a><ul><li><a href="/lkml/2019/8/24/134">Ingo Molnar</a><ul><li class="origin"><a href="/lkml/2019/8/24/186">Linus Torvalds</a><ul><li><a href="/lkml/2019/8/24/186">Ingo Molnar</a><ul><li><a href="/lkml/2019/8/24/202">Linus Torvalds</a></li><li><a href="/lkml/2019/8/25/10">Tetsuo Handa</a></li></ul></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Sat, 24 Aug 2019 10:40:47 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH] /dev/mem: Bail out upon SIGKILL when reading memory.</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Sat, Aug 24, 2019 at 9:14 AM Ingo Molnar &lt;mingo&#64;kernel.org&gt; wrote:<br />&gt;<br />&gt; What I noticed is that while reading regular RAM is reasonably fast even<br />&gt; in very large chunks, accesses become very slow once they hit iomem - and<br />&gt; delays even longer than the reported 145 seconds become possible if<br />&gt; bs=100M is increased to say bs=1000M.<br /><br />Ok, that makes sense.<br /><br />So for IOMEM, we actually have two independent issues:<br /><br /> (a) for normal unmapped IOMEM (ie no device), which is probably the<br />case your test triggers anyway, it quite possibly ends up having ISA<br />timings (ie around 1us per read)<br /><br /> (b) we use "probe_kernel_read()" to a temporary buffer avoid page<br />faults, which uses __copy_from_user_inatomic(). So far so good. But on<br />modern x86, because we treat it as just a regular memcpy, we probably<br />have ERMS and do "rep movsb".<br /><br />So (a) means that each access is slow to begin with, and then (b)<br />means that we don't use "copy_from_io()" but a slow byte-by-byte<br />access.<br /><br />&gt; With Tetsuo's approach the delays are fixed, because the fatal signal is<br />&gt; noticed within the 4K chunks of read_mem() immediately:<br /><br />Yeah. that's the size of the temp buffer.<br /><br />&gt; Note how the first 100MB chunk took 17 seconds, the second chunk was<br />&gt; interrupted within ~2 seconds after I sent a SIGKILL.<br /><br />It looks closer to 1 second from that trace, but that actually is<br />still within the basic noise above: a 4kB buffer being copied one byte<br />at a time would take about 4s, but maybe it's not *quite* as bad as<br />traditional ISA PIO timings.<br /><br />&gt; Except that I think the regular pattern here is not 'break' but 'goto<br />&gt; failed' - 'break' would just result in a partial read and 'err' gets<br />&gt; ignored.<br /><br />That's actually the correct signal handling pattern: either a partial<br />read, or -EINTR.<br /><br />In the case of SIGKILL, the return value doesn't matter, of course,<br />but *if* we were to decide that we can make it interruptible, then it<br />would.<br /><br />&gt; I also agree that we should probably allow regular interrupts to<br />&gt; interrupt /dev/mem accesses - while the 'dd' process is killable, it's<br />&gt; not Ctrl-C-able.<br /><br />I'm a bit nervous about it, but there probably aren't all that many<br />actual /dev/mem users.<br /><br />The main ones I can see are things like "dmidecode" etc.<br /><br />&gt; If I change the fatal_signal_pending() to signal_pending() it all behaves<br />&gt; much better IMHO - assuming that no utility learned rely on<br />&gt; non-interruptibility ...<br /><br />So I cloned the dmidecode git tree, and it does "myread()" on the<br />/dev/mem file as far as I can tell. And that one correctly hanles<br />partial reads.<br /><br />It still makes me a bit nervous, but just plain "signal_pending()" is<br />not just nicer, it's technically the right thing to do (it's not a<br />regular file, so partial reads are permissible, and other files like<br />it - eg /dev/zero - already does exactly that).<br /><br />I also wonder if we should just not use that crazy<br />"probe_kernel_read()" to begin with. The /dev/mem code uses it to<br />avoid faults - and that's the intent of it - but it's not meant for<br />IO-memory at all.<br /><br />But "read()" on /dev/mem does that off "xlate_dev_mem_ptr()", which is<br />a bastardized "kernel address or ioremap" thing. It works, but it's<br />all kinds of stupid. We'd be a *lot* better off using kmap(), I think.<br /><br />So my gut feel is that this is something worth trying to do bigger and<br />more fundamental changes to.<br /><br />But changing it to use "signal_pending()" at least as a trial looks<br />ok. The only user *should* be things like dmidecode that apparently<br />already do the right thing.<br /><br />And if we changed the bounce buffering to do things at least a 32-bit<br />word at a time, that would help latency for IO by a factor of 4.<br /><br />And if the signal_pending() is at the end of the copy, then we'd<br />guarantee that at least _small_ reads still work the way they did<br />before, so people who do things like "lspci()" with direct hardware<br />accesses wouldn't be impacted - if they exist.<br /><br />So I'd be willing to try that (and then if somebody reports a<br />regression we can make it use "fatal_signal_pending()" instead)<br /><br />                    Linus<br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
