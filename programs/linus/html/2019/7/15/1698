    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2019/7/15/42">First message in thread</a></li><li><a href="/lkml/2019/7/15/1514">=?UTF-8?Q?Thomas_Hellstr=c3=b6m_=28VMware=29?=</a><ul><li><a href="/lkml/2019/7/15/1574">Linus Torvalds</a><ul><li class="origin"><a href="/lkml/2019/8/6/185">Linus Torvalds</a><ul><li><a href="/lkml/2019/8/6/185">Christoph Hellwig</a><ul><li><a href="/lkml/2019/8/6/188">Christoph Hellwig</a></li><li><a href="/lkml/2019/8/6/859">Linus Torvalds</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2019/7/15/1698/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Mon, 15 Jul 2019 15:17:42 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: drm pull for v5.3-rc1</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Mon, Jul 15, 2019 at 1:07 PM Linus Torvalds<br />&lt;torvalds&#64;linux-foundation.org&gt; wrote:<br />&gt;<br />&gt; The mm_walk struct is indeed a bit similar, and is in fact a bit<br />&gt; problematic exactly because it mixes function pointers with non-const<br />&gt; data.<br /><br />This made me look at how nasty that would be to fix.<br /><br />Not too bad.<br /><br />The attached patch does add more lines than it removes, but in most<br />cases it's actually a clear improvement.<br /><br />It results in:<br /><br /> - smaller stackframes and less runtime initialization: the bulk of<br />the 'mm_walk' structure was the ops pointers, and if we split them out<br />and make them const, we can just initialize them statically, and the<br />stack footprint now becomes just a single word.<br /><br /> - the function pointers are now nicely in a const data section<br /><br />in addition to the whole "don't mix variable data with constants, and<br />don't put function pointers on the stack" thing.<br /><br />Of course, I haven't _tested_ the end result, but since it compiles it<br />must be perfect, right? Not that I tested all of the build either,<br />since several of the mm_walk users were for other architectures.<br /><br />I'm not sure this is really worth it, but I'm throwing the patch out<br />there in case somebody wants to look.<br /><br />Andrew, comments? I don't think we have anybody who is in charge of<br />mm_walk outside of you...<br /><br />                  Linus<br /> arch/openrisc/kernel/dma.c              | 10 ++++--<br /> arch/powerpc/mm/book3s64/subpage_prot.c |  5 ++-<br /> arch/s390/mm/gmap.c                     | 25 ++++++++++----<br /> fs/proc/task_mmu.c                      | 40 +++++++++++++++-------<br /> include/linux/mm.h                      | 23 +++++++++----<br /> mm/hmm.c                                | 34 ++++++++++---------<br /> mm/madvise.c                            | 10 ++++--<br /> mm/memcontrol.c                         | 11 ++++--<br /> mm/mempolicy.c                          |  5 ++-<br /> mm/migrate.c                            | 20 +++++------<br /> mm/mincore.c                            |  5 ++-<br /> mm/mprotect.c                           |  5 ++-<br /> mm/pagewalk.c                           | 60 +++++++++++++++++++--------------<br /> 13 files changed, 165 insertions(+), 88 deletions(-)<br /><br />diff --git a/arch/openrisc/kernel/dma.c b/arch/openrisc/kernel/dma.c<br />index b41a79fcdbd9..1a69a66fe257 100644<br />--- a/arch/openrisc/kernel/dma.c<br />+++ b/arch/openrisc/kernel/dma.c<br />&#64;&#64; -80,8 +80,11 &#64;&#64; arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,<br /> {<br /> 	unsigned long va;<br /> 	void *page;<br />-	struct mm_walk walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.pte_entry = page_set_nocache,<br />+	};<br />+	struct mm_walk walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = &amp;init_mm<br /> 	};<br /> <br />&#64;&#64; -111,8 +114,11 &#64;&#64; arch_dma_free(struct device *dev, size_t size, void *vaddr,<br /> 		dma_addr_t dma_handle, unsigned long attrs)<br /> {<br /> 	unsigned long va = (unsigned long)vaddr;<br />-	struct mm_walk walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.pte_entry = page_clear_nocache,<br />+	};<br />+	struct mm_walk walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = &amp;init_mm<br /> 	};<br /> <br />diff --git a/arch/powerpc/mm/book3s64/subpage_prot.c b/arch/powerpc/mm/book3s64/subpage_prot.c<br />index 9ba07e55c489..7876b316138b 100644<br />--- a/arch/powerpc/mm/book3s64/subpage_prot.c<br />+++ b/arch/powerpc/mm/book3s64/subpage_prot.c<br />&#64;&#64; -143,9 +143,12 &#64;&#64; static void subpage_mark_vma_nohuge(struct mm_struct *mm, unsigned long addr,<br /> 				    unsigned long len)<br /> {<br /> 	struct vm_area_struct *vma;<br />+	static const struct mm_walk_ops ops = {<br />+		.pmd_entry = subpage_walk_pmd_entry,<br />+	};<br /> 	struct mm_walk subpage_proto_walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = mm,<br />-		.pmd_entry = subpage_walk_pmd_entry,<br /> 	};<br /> <br /> 	/*<br />diff --git a/arch/s390/mm/gmap.c b/arch/s390/mm/gmap.c<br />index 1e668b95e0c6..9e0feeb469c2 100644<br />--- a/arch/s390/mm/gmap.c<br />+++ b/arch/s390/mm/gmap.c<br />&#64;&#64; -2523,9 +2523,14 &#64;&#64; static int __zap_zero_pages(pmd_t *pmd, unsigned long start,<br /> <br /> static inline void zap_zero_pages(struct mm_struct *mm)<br /> {<br />-	struct mm_walk walk = { .pmd_entry = __zap_zero_pages };<br />+	static const struct mm_walk_ops ops = {<br />+		.pmd_entry = __zap_zero_pages,<br />+	};<br />+	struct mm_walk walk = {<br />+		.ops = &amp;ops,<br />+		.mm = mm,<br />+	};<br /> <br />-	walk.mm = mm;<br /> 	walk_page_range(0, TASK_SIZE, &amp;walk);<br /> }<br /> <br />&#64;&#64; -2591,11 +2596,15 &#64;&#64; static int __s390_enable_skey_hugetlb(pte_t *pte, unsigned long addr,<br /> <br /> int s390_enable_skey(void)<br /> {<br />-	struct mm_walk walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.hugetlb_entry = __s390_enable_skey_hugetlb,<br /> 		.pte_entry = __s390_enable_skey_pte,<br /> 	};<br /> 	struct mm_struct *mm = current-&gt;mm;<br />+	struct mm_walk walk = {<br />+		.ops = &amp;ops,<br />+		.mm = mm,<br />+	};<br /> 	struct vm_area_struct *vma;<br /> 	int rc = 0;<br /> <br />&#64;&#64; -2614,7 +2623,6 &#64;&#64; int s390_enable_skey(void)<br /> 	}<br /> 	mm-&gt;def_flags &amp;= ~VM_MERGEABLE;<br /> <br />-	walk.mm = mm;<br /> 	walk_page_range(0, TASK_SIZE, &amp;walk);<br /> <br /> out_up:<br />&#64;&#64; -2635,10 +2643,15 &#64;&#64; static int __s390_reset_cmma(pte_t *pte, unsigned long addr,<br /> <br /> void s390_reset_cmma(struct mm_struct *mm)<br /> {<br />-	struct mm_walk walk = { .pte_entry = __s390_reset_cmma };<br />+	static const struct mm_walk_ops ops = {<br />+		.pte_entry = __s390_reset_cmma,<br />+	};<br />+	struct mm_walk walk = {<br />+		.ops = &amp;ops,<br />+		.mm = mm,<br />+	};<br /> <br /> 	down_write(&amp;mm-&gt;mmap_sem);<br />-	walk.mm = mm;<br /> 	walk_page_range(0, TASK_SIZE, &amp;walk);<br /> 	up_write(&amp;mm-&gt;mmap_sem);<br /> }<br />diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c<br />index 818cedbed95f..fb5710830ffc 100644<br />--- a/fs/proc/task_mmu.c<br />+++ b/fs/proc/task_mmu.c<br />&#64;&#64; -729,16 +729,19 &#64;&#64; static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,<br /> 	}<br /> 	return 0;<br /> }<br />+#else<br />+ #define smaps_hugetlb_range NULL<br /> #endif /* HUGETLB_PAGE */<br /> <br /> static void smap_gather_stats(struct vm_area_struct *vma,<br /> 			     struct mem_size_stats *mss)<br /> {<br />-	struct mm_walk smaps_walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.pmd_entry = smaps_pte_range,<br />-#ifdef CONFIG_HUGETLB_PAGE<br /> 		.hugetlb_entry = smaps_hugetlb_range,<br />-#endif<br />+	};<br />+	struct mm_walk smaps_walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = vma-&gt;vm_mm,<br /> 	};<br /> <br />&#64;&#64; -764,8 +767,13 &#64;&#64; static void smap_gather_stats(struct vm_area_struct *vma,<br /> 					!(vma-&gt;vm_flags &amp; VM_WRITE)) {<br /> 			mss-&gt;swap += shmem_swapped;<br /> 		} else {<br />+			static const struct mm_walk_ops ops = {<br />+				.pmd_entry = smaps_pte_range,<br />+				.hugetlb_entry = smaps_hugetlb_range,<br />+				.pte_hole = smaps_pte_hole,<br />+			};<br /> 			mss-&gt;check_shmem_swap = true;<br />-			smaps_walk.pte_hole = smaps_pte_hole;<br />+			smaps_walk.ops = &amp;ops;<br /> 		}<br /> 	}<br /> #endif<br />&#64;&#64; -1150,9 +1158,12 &#64;&#64; static ssize_t clear_refs_write(struct file *file, const char __user *buf,<br /> 		struct clear_refs_private cp = {<br /> 			.type = type,<br /> 		};<br />-		struct mm_walk clear_refs_walk = {<br />+		static const struct mm_walk_ops ops = {<br /> 			.pmd_entry = clear_refs_pte_range,<br /> 			.test_walk = clear_refs_test_walk,<br />+		};<br />+		struct mm_walk clear_refs_walk = {<br />+			.ops = &amp;ops,<br /> 			.mm = mm,<br /> 			.private = &amp;cp,<br /> 		};<br />&#64;&#64; -1488,6 +1499,8 &#64;&#64; static int pagemap_hugetlb_range(pte_t *ptep, unsigned long hmask,<br /> <br /> 	return err;<br /> }<br />+#else<br />+ #define pagemap_hugetlb_range NULL<br /> #endif /* HUGETLB_PAGE */<br /> <br /> /*<br />&#64;&#64; -1521,7 +1534,12 &#64;&#64; static ssize_t pagemap_read(struct file *file, char __user *buf,<br /> {<br /> 	struct mm_struct *mm = file-&gt;private_data;<br /> 	struct pagemapread pm;<br />-	struct mm_walk pagemap_walk = {};<br />+	static const struct mm_walk_ops ops = {<br />+		.pmd_entry = pagemap_pmd_range,<br />+		.pte_hole = pagemap_pte_hole,<br />+		.hugetlb_entry = pagemap_hugetlb_range,<br />+	};<br />+	struct mm_walk pagemap_walk = { .ops = &amp;ops, };<br /> 	unsigned long src;<br /> 	unsigned long svpfn;<br /> 	unsigned long start_vaddr;<br />&#64;&#64; -1549,11 +1567,6 &#64;&#64; static ssize_t pagemap_read(struct file *file, char __user *buf,<br /> 	if (!pm.buffer)<br /> 		goto out_mm;<br /> <br />-	pagemap_walk.pmd_entry = pagemap_pmd_range;<br />-	pagemap_walk.pte_hole = pagemap_pte_hole;<br />-#ifdef CONFIG_HUGETLB_PAGE<br />-	pagemap_walk.hugetlb_entry = pagemap_hugetlb_range;<br />-#endif<br /> 	pagemap_walk.mm = mm;<br /> 	pagemap_walk.private = &amp;pm;<br /> <br />&#64;&#64; -1808,9 +1821,12 &#64;&#64; static int show_numa_map(struct seq_file *m, void *v)<br /> 	struct numa_maps *md = &amp;numa_priv-&gt;md;<br /> 	struct file *file = vma-&gt;vm_file;<br /> 	struct mm_struct *mm = vma-&gt;vm_mm;<br />-	struct mm_walk walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.hugetlb_entry = gather_hugetlb_stats,<br /> 		.pmd_entry = gather_pte_stats,<br />+	};<br />+	struct mm_walk walk = {<br />+		.ops = &amp;ops,<br /> 		.private = md,<br /> 		.mm = mm,<br /> 	};<br />diff --git a/include/linux/mm.h b/include/linux/mm.h<br />index 0389c34ac529..8133f24a3a28 100644<br />--- a/include/linux/mm.h<br />+++ b/include/linux/mm.h<br />&#64;&#64; -1426,8 +1426,10 &#64;&#64; void zap_page_range(struct vm_area_struct *vma, unsigned long address,<br /> void unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,<br /> 		unsigned long start, unsigned long end);<br /> <br />+struct mm_walk;<br />+<br /> /**<br />- * mm_walk - callbacks for walk_page_range<br />+ * mm_walk_ops - callbacks for walk_page_range<br />  * &#64;pud_entry: if set, called for each non-empty PUD (2nd-level) entry<br />  *	       this handler should only handle pud_trans_huge() puds.<br />  *	       the pmd_entry or pte_entry callbacks will be used for<br />&#64;&#64; -1444,13 +1446,8 &#64;&#64; void unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,<br />  *             value means "do page table walk over the current vma,"<br />  *             and a negative one means "abort current page table walk<br />  *             right now." 1 means "skip the current vma."<br />- * &#64;mm:        mm_struct representing the target process of page table walk<br />- * &#64;vma:       vma currently walked (NULL if walking outside vmas)<br />- * &#64;private:   private data for callbacks' usage<br />- *<br />- * (see the comment on walk_page_range() for more details)<br />  */<br />-struct mm_walk {<br />+struct mm_walk_ops {<br /> 	int (*pud_entry)(pud_t *pud, unsigned long addr,<br /> 			 unsigned long next, struct mm_walk *walk);<br /> 	int (*pmd_entry)(pmd_t *pmd, unsigned long addr,<br />&#64;&#64; -1464,6 +1461,18 &#64;&#64; struct mm_walk {<br /> 			     struct mm_walk *walk);<br /> 	int (*test_walk)(unsigned long addr, unsigned long next,<br /> 			struct mm_walk *walk);<br />+};<br />+<br />+/**<br />+ * mm_walk -   walk_page_range data<br />+ * &#64;mm:        mm_struct representing the target process of page table walk<br />+ * &#64;vma:       vma currently walked (NULL if walking outside vmas)<br />+ * &#64;private:   private data for callbacks' usage<br />+ *<br />+ * (see the comment on walk_page_range() for more details)<br />+ */<br />+struct mm_walk {<br />+	const struct mm_walk_ops *ops;<br /> 	struct mm_struct *mm;<br /> 	struct vm_area_struct *vma;<br /> 	void *private;<br />diff --git a/mm/hmm.c b/mm/hmm.c<br />index e1eedef129cf..756843ffa7cb 100644<br />--- a/mm/hmm.c<br />+++ b/mm/hmm.c<br />&#64;&#64; -961,7 +961,15 &#64;&#64; long hmm_range_snapshot(struct hmm_range *range)<br /> 	struct hmm_vma_walk hmm_vma_walk;<br /> 	struct hmm *hmm = range-&gt;hmm;<br /> 	struct vm_area_struct *vma;<br />-	struct mm_walk mm_walk;<br />+	static const struct mm_walk_ops ops = {<br />+		.pud_entry = hmm_vma_walk_pud,<br />+		.pmd_entry = hmm_vma_walk_pmd,<br />+		.pte_hole = hmm_vma_walk_hole,<br />+		.hugetlb_entry = hmm_vma_walk_hugetlb_entry,<br />+	};<br />+	struct mm_walk mm_walk = {<br />+		.ops = &amp;ops,<br />+	};<br /> <br /> 	lockdep_assert_held(&amp;hmm-&gt;mm-&gt;mmap_sem);<br /> 	do {<br />&#64;&#64; -1004,13 +1012,6 &#64;&#64; long hmm_range_snapshot(struct hmm_range *range)<br /> <br /> 		mm_walk.vma = vma;<br /> 		mm_walk.mm = vma-&gt;vm_mm;<br />-		mm_walk.pte_entry = NULL;<br />-		mm_walk.test_walk = NULL;<br />-		mm_walk.hugetlb_entry = NULL;<br />-		mm_walk.pud_entry = hmm_vma_walk_pud;<br />-		mm_walk.pmd_entry = hmm_vma_walk_pmd;<br />-		mm_walk.pte_hole = hmm_vma_walk_hole;<br />-		mm_walk.hugetlb_entry = hmm_vma_walk_hugetlb_entry;<br /> <br /> 		walk_page_range(start, end, &amp;mm_walk);<br /> 		start = end;<br />&#64;&#64; -1055,7 +1056,15 &#64;&#64; long hmm_range_fault(struct hmm_range *range, bool block)<br /> 	struct hmm_vma_walk hmm_vma_walk;<br /> 	struct hmm *hmm = range-&gt;hmm;<br /> 	struct vm_area_struct *vma;<br />-	struct mm_walk mm_walk;<br />+	static const struct mm_walk_ops ops = {<br />+		.pud_entry = hmm_vma_walk_pud,<br />+		.pmd_entry = hmm_vma_walk_pmd,<br />+		.pte_hole = hmm_vma_walk_hole,<br />+		.hugetlb_entry = hmm_vma_walk_hugetlb_entry,<br />+	};<br />+	struct mm_walk mm_walk = {<br />+		.ops = &amp;ops,<br />+	};<br /> 	int ret;<br /> <br /> 	lockdep_assert_held(&amp;hmm-&gt;mm-&gt;mmap_sem);<br />&#64;&#64; -1103,13 +1112,6 &#64;&#64; long hmm_range_fault(struct hmm_range *range, bool block)<br /> <br /> 		mm_walk.vma = vma;<br /> 		mm_walk.mm = vma-&gt;vm_mm;<br />-		mm_walk.pte_entry = NULL;<br />-		mm_walk.test_walk = NULL;<br />-		mm_walk.hugetlb_entry = NULL;<br />-		mm_walk.pud_entry = hmm_vma_walk_pud;<br />-		mm_walk.pmd_entry = hmm_vma_walk_pmd;<br />-		mm_walk.pte_hole = hmm_vma_walk_hole;<br />-		mm_walk.hugetlb_entry = hmm_vma_walk_hugetlb_entry;<br /> <br /> 		do {<br /> 			ret = walk_page_range(start, end, &amp;mm_walk);<br />diff --git a/mm/madvise.c b/mm/madvise.c<br />index 968df3aa069f..b9700060cafb 100644<br />--- a/mm/madvise.c<br />+++ b/mm/madvise.c<br />&#64;&#64; -228,9 +228,12 &#64;&#64; static int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,<br /> static void force_swapin_readahead(struct vm_area_struct *vma,<br /> 		unsigned long start, unsigned long end)<br /> {<br />+	static const struct mm_walk_ops ops = {<br />+		.pmd_entry = swapin_walk_pmd_entry,<br />+	};<br /> 	struct mm_walk walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = vma-&gt;vm_mm,<br />-		.pmd_entry = swapin_walk_pmd_entry,<br /> 		.private = vma,<br /> 	};<br /> <br />&#64;&#64; -444,8 +447,11 &#64;&#64; static void madvise_free_page_range(struct mmu_gather *tlb,<br /> 			     struct vm_area_struct *vma,<br /> 			     unsigned long addr, unsigned long end)<br /> {<br />-	struct mm_walk free_walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.pmd_entry = madvise_free_pte_range,<br />+	};<br />+	struct mm_walk free_walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = vma-&gt;vm_mm,<br /> 		.private = tlb,<br /> 	};<br />diff --git a/mm/memcontrol.c b/mm/memcontrol.c<br />index 249671873aa9..6d3912b9e508 100644<br />--- a/mm/memcontrol.c<br />+++ b/mm/memcontrol.c<br />&#64;&#64; -5239,9 +5239,11 &#64;&#64; static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,<br /> static unsigned long mem_cgroup_count_precharge(struct mm_struct *mm)<br /> {<br /> 	unsigned long precharge;<br />-<br />-	struct mm_walk mem_cgroup_count_precharge_walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.pmd_entry = mem_cgroup_count_precharge_pte_range,<br />+	};<br />+	struct mm_walk mem_cgroup_count_precharge_walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = mm,<br /> 	};<br /> 	down_read(&amp;mm-&gt;mmap_sem);<br />&#64;&#64; -5517,8 +5519,11 &#64;&#64; static int mem_cgroup_move_charge_pte_range(pmd_t *pmd,<br /> <br /> static void mem_cgroup_move_charge(void)<br /> {<br />-	struct mm_walk mem_cgroup_move_charge_walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.pmd_entry = mem_cgroup_move_charge_pte_range,<br />+	};<br />+	struct mm_walk mem_cgroup_move_charge_walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = mc.mm,<br /> 	};<br /> <br />diff --git a/mm/mempolicy.c b/mm/mempolicy.c<br />index f48693f75b37..ca10c9b55333 100644<br />--- a/mm/mempolicy.c<br />+++ b/mm/mempolicy.c<br />&#64;&#64; -652,10 +652,13 &#64;&#64; queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,<br /> 		.nmask = nodes,<br /> 		.prev = NULL,<br /> 	};<br />-	struct mm_walk queue_pages_walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.hugetlb_entry = queue_pages_hugetlb,<br /> 		.pmd_entry = queue_pages_pte_range,<br /> 		.test_walk = queue_pages_test_walk,<br />+	};<br />+	struct mm_walk queue_pages_walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = mm,<br /> 		.private = &amp;qp,<br /> 	};<br />diff --git a/mm/migrate.c b/mm/migrate.c<br />index 3445747e229d..0d1da0d72011 100644<br />--- a/mm/migrate.c<br />+++ b/mm/migrate.c<br />&#64;&#64; -2339,16 +2339,16 &#64;&#64; static int migrate_vma_collect_pmd(pmd_t *pmdp,<br /> static void migrate_vma_collect(struct migrate_vma *migrate)<br /> {<br /> 	struct mmu_notifier_range range;<br />-	struct mm_walk mm_walk;<br />-<br />-	mm_walk.pmd_entry = migrate_vma_collect_pmd;<br />-	mm_walk.pte_entry = NULL;<br />-	mm_walk.pte_hole = migrate_vma_collect_hole;<br />-	mm_walk.hugetlb_entry = NULL;<br />-	mm_walk.test_walk = NULL;<br />-	mm_walk.vma = migrate-&gt;vma;<br />-	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;<br />-	mm_walk.private = migrate;<br />+	static const struct mm_walk_ops ops = {<br />+		.pmd_entry = migrate_vma_collect_pmd,<br />+		.pte_hole = migrate_vma_collect_hole,<br />+	};<br />+	struct mm_walk mm_walk = {<br />+		.ops = &amp;ops,<br />+		.vma = migrate-&gt;vma,<br />+		.mm = migrate-&gt;vma-&gt;vm_mm,<br />+		.private = migrate,<br />+	};<br /> <br /> 	mmu_notifier_range_init(&amp;range, MMU_NOTIFY_CLEAR, 0, NULL, mm_walk.mm,<br /> 				migrate-&gt;start,<br />diff --git a/mm/mincore.c b/mm/mincore.c<br />index 4fe91d497436..8195d2099e77 100644<br />--- a/mm/mincore.c<br />+++ b/mm/mincore.c<br />&#64;&#64; -203,10 +203,13 &#64;&#64; static long do_mincore(unsigned long addr, unsigned long pages, unsigned char *v<br /> 	struct vm_area_struct *vma;<br /> 	unsigned long end;<br /> 	int err;<br />-	struct mm_walk mincore_walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.pmd_entry = mincore_pte_range,<br /> 		.pte_hole = mincore_unmapped_range,<br /> 		.hugetlb_entry = mincore_hugetlb,<br />+	};<br />+	struct mm_walk mincore_walk = {<br />+		.ops = &amp;ops,<br /> 		.private = vec,<br /> 	};<br /> <br />diff --git a/mm/mprotect.c b/mm/mprotect.c<br />index bf38dfbbb4b4..719b5f4b9fe5 100644<br />--- a/mm/mprotect.c<br />+++ b/mm/mprotect.c<br />&#64;&#64; -333,10 +333,13 &#64;&#64; static int prot_none_walk(struct vm_area_struct *vma, unsigned long start,<br /> 			   unsigned long end, unsigned long newflags)<br /> {<br /> 	pgprot_t new_pgprot = vm_get_page_prot(newflags);<br />-	struct mm_walk prot_none_walk = {<br />+	static const struct mm_walk_ops ops = {<br /> 		.pte_entry = prot_none_pte_entry,<br /> 		.hugetlb_entry = prot_none_hugetlb_entry,<br /> 		.test_walk = prot_none_test,<br />+	};<br />+	struct mm_walk prot_none_walk = {<br />+		.ops = &amp;ops,<br /> 		.mm = current-&gt;mm,<br /> 		.private = &amp;new_pgprot,<br /> 	};<br />diff --git a/mm/pagewalk.c b/mm/pagewalk.c<br />index c3084ff2569d..38ab762aef44 100644<br />--- a/mm/pagewalk.c<br />+++ b/mm/pagewalk.c<br />&#64;&#64; -9,10 +9,11 &#64;&#64; static int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,<br /> {<br /> 	pte_t *pte;<br /> 	int err = 0;<br />+	const struct mm_walk_ops *ops = walk-&gt;ops;<br /> <br /> 	pte = pte_offset_map(pmd, addr);<br /> 	for (;;) {<br />-		err = walk-&gt;pte_entry(pte, addr, addr + PAGE_SIZE, walk);<br />+		err = ops-&gt;pte_entry(pte, addr, addr + PAGE_SIZE, walk);<br /> 		if (err)<br /> 		       break;<br /> 		addr += PAGE_SIZE;<br />&#64;&#64; -30,6 +31,7 &#64;&#64; static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,<br /> {<br /> 	pmd_t *pmd;<br /> 	unsigned long next;<br />+	const struct mm_walk_ops *ops = walk-&gt;ops;<br /> 	int err = 0;<br /> <br /> 	pmd = pmd_offset(pud, addr);<br />&#64;&#64; -37,8 +39,8 &#64;&#64; static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,<br /> again:<br /> 		next = pmd_addr_end(addr, end);<br /> 		if (pmd_none(*pmd) || !walk-&gt;vma) {<br />-			if (walk-&gt;pte_hole)<br />-				err = walk-&gt;pte_hole(addr, next, walk);<br />+			if (ops-&gt;pte_hole)<br />+				err = ops-&gt;pte_hole(addr, next, walk);<br /> 			if (err)<br /> 				break;<br /> 			continue;<br />&#64;&#64; -47,8 +49,8 &#64;&#64; static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,<br /> 		 * This implies that each -&gt;pmd_entry() handler<br /> 		 * needs to know about pmd_trans_huge() pmds<br /> 		 */<br />-		if (walk-&gt;pmd_entry)<br />-			err = walk-&gt;pmd_entry(pmd, addr, next, walk);<br />+		if (ops-&gt;pmd_entry)<br />+			err = ops-&gt;pmd_entry(pmd, addr, next, walk);<br /> 		if (err)<br /> 			break;<br /> <br />&#64;&#64; -56,7 +58,7 &#64;&#64; static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,<br /> 		 * Check this here so we only break down trans_huge<br /> 		 * pages when we _need_ to<br /> 		 */<br />-		if (!walk-&gt;pte_entry)<br />+		if (!ops-&gt;pte_entry)<br /> 			continue;<br /> <br /> 		split_huge_pmd(walk-&gt;vma, pmd, addr);<br />&#64;&#64; -75,6 +77,7 &#64;&#64; static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,<br /> {<br /> 	pud_t *pud;<br /> 	unsigned long next;<br />+	const struct mm_walk_ops *ops = walk-&gt;ops;<br /> 	int err = 0;<br /> <br /> 	pud = pud_offset(p4d, addr);<br />&#64;&#64; -82,18 +85,18 &#64;&#64; static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,<br />  again:<br /> 		next = pud_addr_end(addr, end);<br /> 		if (pud_none(*pud) || !walk-&gt;vma) {<br />-			if (walk-&gt;pte_hole)<br />-				err = walk-&gt;pte_hole(addr, next, walk);<br />+			if (ops-&gt;pte_hole)<br />+				err = ops-&gt;pte_hole(addr, next, walk);<br /> 			if (err)<br /> 				break;<br /> 			continue;<br /> 		}<br /> <br />-		if (walk-&gt;pud_entry) {<br />+		if (ops-&gt;pud_entry) {<br /> 			spinlock_t *ptl = pud_trans_huge_lock(pud, walk-&gt;vma);<br /> <br /> 			if (ptl) {<br />-				err = walk-&gt;pud_entry(pud, addr, next, walk);<br />+				err = ops-&gt;pud_entry(pud, addr, next, walk);<br /> 				spin_unlock(ptl);<br /> 				if (err)<br /> 					break;<br />&#64;&#64; -105,7 +108,7 &#64;&#64; static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,<br /> 		if (pud_none(*pud))<br /> 			goto again;<br /> <br />-		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)<br />+		if (ops-&gt;pmd_entry || ops-&gt;pte_entry)<br /> 			err = walk_pmd_range(pud, addr, next, walk);<br /> 		if (err)<br /> 			break;<br />&#64;&#64; -119,19 +122,20 &#64;&#64; static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,<br /> {<br /> 	p4d_t *p4d;<br /> 	unsigned long next;<br />+	const struct mm_walk_ops *ops = walk-&gt;ops;<br /> 	int err = 0;<br /> <br /> 	p4d = p4d_offset(pgd, addr);<br /> 	do {<br /> 		next = p4d_addr_end(addr, end);<br /> 		if (p4d_none_or_clear_bad(p4d)) {<br />-			if (walk-&gt;pte_hole)<br />-				err = walk-&gt;pte_hole(addr, next, walk);<br />+			if (ops-&gt;pte_hole)<br />+				err = ops-&gt;pte_hole(addr, next, walk);<br /> 			if (err)<br /> 				break;<br /> 			continue;<br /> 		}<br />-		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)<br />+		if (ops-&gt;pmd_entry || ops-&gt;pte_entry)<br /> 			err = walk_pud_range(p4d, addr, next, walk);<br /> 		if (err)<br /> 			break;<br />&#64;&#64; -145,19 +149,20 &#64;&#64; static int walk_pgd_range(unsigned long addr, unsigned long end,<br /> {<br /> 	pgd_t *pgd;<br /> 	unsigned long next;<br />+	const struct mm_walk_ops *ops = walk-&gt;ops;<br /> 	int err = 0;<br /> <br /> 	pgd = pgd_offset(walk-&gt;mm, addr);<br /> 	do {<br /> 		next = pgd_addr_end(addr, end);<br /> 		if (pgd_none_or_clear_bad(pgd)) {<br />-			if (walk-&gt;pte_hole)<br />-				err = walk-&gt;pte_hole(addr, next, walk);<br />+			if (ops-&gt;pte_hole)<br />+				err = ops-&gt;pte_hole(addr, next, walk);<br /> 			if (err)<br /> 				break;<br /> 			continue;<br /> 		}<br />-		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)<br />+		if (ops-&gt;pmd_entry || ops-&gt;pte_entry)<br /> 			err = walk_p4d_range(pgd, addr, next, walk);<br /> 		if (err)<br /> 			break;<br />&#64;&#64; -183,6 +188,7 &#64;&#64; static int walk_hugetlb_range(unsigned long addr, unsigned long end,<br /> 	unsigned long hmask = huge_page_mask(h);<br /> 	unsigned long sz = huge_page_size(h);<br /> 	pte_t *pte;<br />+	const struct mm_walk_ops *ops = walk-&gt;ops;<br /> 	int err = 0;<br /> <br /> 	do {<br />&#64;&#64; -190,9 +196,9 &#64;&#64; static int walk_hugetlb_range(unsigned long addr, unsigned long end,<br /> 		pte = huge_pte_offset(walk-&gt;mm, addr &amp; hmask, sz);<br /> <br /> 		if (pte)<br />-			err = walk-&gt;hugetlb_entry(pte, hmask, addr, next, walk);<br />-		else if (walk-&gt;pte_hole)<br />-			err = walk-&gt;pte_hole(addr, next, walk);<br />+			err = ops-&gt;hugetlb_entry(pte, hmask, addr, next, walk);<br />+		else if (ops-&gt;pte_hole)<br />+			err = ops-&gt;pte_hole(addr, next, walk);<br /> <br /> 		if (err)<br /> 			break;<br />&#64;&#64; -220,9 +226,10 &#64;&#64; static int walk_page_test(unsigned long start, unsigned long end,<br /> 			struct mm_walk *walk)<br /> {<br /> 	struct vm_area_struct *vma = walk-&gt;vma;<br />+	const struct mm_walk_ops *ops = walk-&gt;ops;<br /> <br />-	if (walk-&gt;test_walk)<br />-		return walk-&gt;test_walk(start, end, walk);<br />+	if (ops-&gt;test_walk)<br />+		return ops-&gt;test_walk(start, end, walk);<br /> <br /> 	/*<br /> 	 * vma(VM_PFNMAP) doesn't have any valid struct pages behind VM_PFNMAP<br />&#64;&#64; -234,8 +241,8 &#64;&#64; static int walk_page_test(unsigned long start, unsigned long end,<br /> 	 */<br /> 	if (vma-&gt;vm_flags &amp; VM_PFNMAP) {<br /> 		int err = 1;<br />-		if (walk-&gt;pte_hole)<br />-			err = walk-&gt;pte_hole(start, end, walk);<br />+		if (ops-&gt;pte_hole)<br />+			err = ops-&gt;pte_hole(start, end, walk);<br /> 		return err ? err : 1;<br /> 	}<br /> 	return 0;<br />&#64;&#64; -248,7 +255,8 &#64;&#64; static int __walk_page_range(unsigned long start, unsigned long end,<br /> 	struct vm_area_struct *vma = walk-&gt;vma;<br /> <br /> 	if (vma &amp;&amp; is_vm_hugetlb_page(vma)) {<br />-		if (walk-&gt;hugetlb_entry)<br />+		const struct mm_walk_ops *ops = walk-&gt;ops;<br />+		if (ops-&gt;hugetlb_entry)<br /> 			err = walk_hugetlb_range(start, end, walk);<br /> 	} else<br /> 		err = walk_pgd_range(start, end, walk);<br />&#64;&#64; -331,7 +339,7 &#64;&#64; int walk_page_range(unsigned long start, unsigned long end,<br /> 			if (err &lt; 0)<br /> 				break;<br /> 		}<br />-		if (walk-&gt;vma || walk-&gt;pte_hole)<br />+		if (walk-&gt;vma || walk-&gt;ops-&gt;pte_hole)<br /> 			err = __walk_page_range(start, next, walk);<br /> 		if (err)<br /> 			break;</pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
