    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2013/10/25/47">First message in thread</a></li><li><a href="/lkml/2013/10/25/72">Linus Torvalds</a><ul><li><a href="/lkml/2013/10/26/46">Pavel Machek</a><ul><li><a href="/lkml/2013/10/26/128">Linus Torvalds</a></li></ul></li><li><a href="/lkml/2013/10/29/663">Jan Kara</a><ul><li class="origin"><a href="/lkml/2013/10/29/702">Linus Torvalds</a><ul><li><a href="/lkml/2013/10/29/702">Jan Kara</a><ul><li><a href="/lkml/2013/10/29/719">Linus Torvalds</a></li></ul></li></ul></li><li><a href="/lkml/2013/10/30/166">Mel Gorman</a><ul><li><a href="/lkml/2013/11/19/656">Rob Landley</a><ul><li><a href="/lkml/2013/11/20/492">One Thousand Gnomes</a></li></ul></li></ul></li></ul></li></ul></li></ul><div class="threadlist">Patch in this message</div><ul class="threadlist"><li><a href="/lkml/diff/2013/10/29/679/1">Get diff 1</a></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Tue, 29 Oct 2013 14:33:53 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: Disabling in-memory write cache for x86-64 in Linux II</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">On Tue, Oct 29, 2013 at 1:57 PM, Jan Kara &lt;jack&#64;suse.cz&gt; wrote:<br />&gt; On Fri 25-10-13 10:32:16, Linus Torvalds wrote:<br />&gt;&gt;<br />&gt;&gt; It definitely doesn't work. I can trivially reproduce problems by just<br />&gt;&gt; having a cheap (==slow) USB key with an ext3 filesystem, and going a<br />&gt;&gt; git clone to it. The end result is not pretty, and that's actually not<br />&gt;&gt; even a huge amount of data.<br />&gt;<br />&gt;   I'll try to reproduce this tomorrow so that I can have a look where<br />&gt; exactly are we stuck. But in last few releases problems like this were<br />&gt; caused by problems in reclaim which got fed up by seeing lots of dirty<br />&gt; / under writeback pages and ended up stuck waiting for IO to finish. Mel<br />&gt; has been tweaking the logic here and there but maybe it haven't got fixed<br />&gt; completely. Mel, do you know about any outstanding issues?<br /><br />I'm not sure this has ever worked, and in the last few years the<br />common desktop memory size has continued to grow.<br /><br />For servers and "serious" desktops, having tons of dirty data doesn't<br />tend to be as much of a problem, because those environments are pretty<br />much defined by also having fairly good IO subsystems, and people<br />seldom use crappy USB devices for more than doing things like reading<br />pictures off them etc. And you'd not even see the problem under any<br />such load.<br /><br />But it's actually really easy to reproduce by just taking your average<br />USB key and trying to write to it. I just did it with a random ISO<br />image, and it's _painful_. And it's not that it's painful for doing<br />most other things in the background, but if you just happen to run<br />anything that does "sync" (and it happens in scripts), the thing just<br />comes to a screeching halt. For minutes.<br /><br />Same obviously goes with trying to eject/unmount the media etc.<br /><br />We've had this problem before with the whole "ratio of dirty memory"<br />thing. It was a mistake. It made sense (and came from) back in the<br />days when people had 16MB or 32MB of RAM, and the concept of "let's<br />limit dirty memory to x% of that" was actually fairly reasonable. But<br />that "x%" doesn't make much sense any more. x% of 16GB (which is quite<br />the reasonable amount of memory for any modern desktop) is a huge<br />thing, and in the meantime the performance of disks have gone up a lot<br />(largely thanks to SSD's), but the *minimum* performance of disks<br />hasn't really improved all that much (largely thanks to USB ;).<br /><br />So how about we just admit that the whole "ratio" thing was a big<br />mistake, and tell people that if they want to set a dirty limit, they<br />should do so in bytes? Which we already really do, but we default to<br />that ratio nevertheless. Which is why I'd suggest we just say "the<br />ratio works fine up to a certain amount, and makes no sense past it".<br /><br />Why not make that "the ratio works fine up to a certain amount, and<br />makes no sense past it" be part of the calculations. We actually<br />*hace* exactly that on HIGHMEM machines, where we have this<br />configuration option of "vm_highmem_is_dirtyable" that defaults to<br />off. It just doesn't trigger on nonhighmem machines (today: "64-bit").<br /><br />So I would suggest that we just expose that "vm_highmem_is_dirtyable"<br />on 64-bit too, and just say that anything over 1GB is highmem. That<br />means that 32-bit and 64-bit environments will basically act the same,<br />and I think it makes the defaults a bit saner.<br /><br />Limiting the amount of dirty memory to 100MB/200MB (for "start<br />background writing" and "wait synchronously" respectively) even if you<br />happen to have 16GB of memory sounds like a good idea. Sure, it might<br />make some benchmarks a bit slower, but it will at least avoid the<br />"wait forever" symptom. And if you really have a very studly IO<br />subsystem, the fact that it starts writing out earlier won't really be<br />a problem.<br /><br />After all, there are two reasons to do delayed writes:<br /><br /> - temp-files may not be written out at all.<br /><br />   Quite frankly, if you have multi-hundred-megabyte temptiles, you've<br />got issues<br /><br /> - coalescing writes improves throughput<br /><br />   There are very much diminishing returns, and the big return is to<br />make sure that we write things out in a good order, which a 100MB<br />buffer should make more than possible.<br /><br />so I really think that it's insane to default to 1.6GB of dirty data<br />before you even start writing it out if you happen to have 16GB of<br />memory.<br /><br />And again: if your benchmark is to create a kernel tree and then<br />immediately delete it, and you used to do that without doing any<br />actual IO, then yes, the attached patch will make that go much slower.<br />But for that benchmark, maybe you should just set the dirty limits (in<br />bytes) by hand, rather than expect the default kernel values to prefer<br />benchmarks over sanity?<br /><br />Suggested patch attached. Comments?<br /><br />                            Linus<br /> kernel/sysctl.c     | 2 --<br /> mm/page-writeback.c | 7 ++++++-<br /> 2 files changed, 6 insertions(+), 3 deletions(-)<br /><br />diff --git a/kernel/sysctl.c b/kernel/sysctl.c<br />index b2f06f3c6a3f..411da56cd732 100644<br />--- a/kernel/sysctl.c<br />+++ b/kernel/sysctl.c<br />&#64;&#64; -1406,7 +1406,6 &#64;&#64; static struct ctl_table vm_table[] = {<br /> 		.extra1		= &amp;zero,<br /> 	},<br /> #endif<br />-#ifdef CONFIG_HIGHMEM<br /> 	{<br /> 		.procname	= "highmem_is_dirtyable",<br /> 		.data		= &amp;vm_highmem_is_dirtyable,<br />&#64;&#64; -1416,7 +1415,6 &#64;&#64; static struct ctl_table vm_table[] = {<br /> 		.extra1		= &amp;zero,<br /> 		.extra2		= &amp;one,<br /> 	},<br />-#endif<br /> 	{<br /> 		.procname	= "scan_unevictable_pages",<br /> 		.data		= &amp;scan_unevictable_pages,<br />diff --git a/mm/page-writeback.c b/mm/page-writeback.c<br />index 63807583d8e8..b3bce1cd59d5 100644<br />--- a/mm/page-writeback.c<br />+++ b/mm/page-writeback.c<br />&#64;&#64; -241,8 +241,13 &#64;&#64; static unsigned long global_dirtyable_memory(void)<br /> 	x = global_page_state(NR_FREE_PAGES) + global_reclaimable_pages();<br /> 	x -= min(x, dirty_balance_reserve);<br /> <br />-	if (!vm_highmem_is_dirtyable)<br />+	if (!vm_highmem_is_dirtyable) {<br />+		const unsigned long GB_pages = 1024*1024*1024 / PAGE_SIZE;<br />+<br /> 		x -= highmem_dirtyable_memory(x);<br />+		if (x &gt; GB_pages)<br />+			x = GB_pages;<br />+	}<br /> <br /> 	return x + 1;	/* Ensure that we never return 0 */<br /> }</pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
