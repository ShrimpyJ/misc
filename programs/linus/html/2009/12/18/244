    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2009/12/18/229">First message in thread</a></li><li><a href="/lkml/2009/12/18/229">"Jan Beulich"</a><ul><li class="origin"><a href="/lkml/2009/12/28/52">Linus Torvalds</a><ul><li><a href="/lkml/2009/12/28/52">Ingo Molnar</a></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Fri, 18 Dec 2009 08:36:14 -0800 (PST)</td></tr><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: [PATCH] x86: slightly shorten __ticket_spin_trylock() (v3)</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody"><br /><br />On Fri, 18 Dec 2009, Jan Beulich wrote:<br />&gt;<br />&gt; -static __always_inline int __ticket_spin_trylock(arch_spinlock_t *lock)<br />&gt; +static __always_inline u8 __ticket_spin_trylock(arch_spinlock_t *lock)<br />&gt;  {<br />&gt;  	int tmp, new;<br />&gt; <br />&gt; &#64;&#64; -87,8 +87,7 &#64;&#64; static __always_inline int __ticket_spin<br />&gt;  		     "jne 1f\n\t"<br />&gt;  		     LOCK_PREFIX "cmpxchgw %w1,%2\n\t"<br />&gt;  		     "1:"<br />&gt; -		     "sete %b1\n\t"<br />&gt; -		     "movzbl %b1,%0\n\t"<br />&gt; +		     "sete %b0\n\t"<br />&gt;  		     : "=&amp;a" (tmp), "=&amp;q" (new), "+m" (lock-&gt;slock)<br />&gt;  		     :<br />&gt;  		     : "memory", "cc");<br />&gt; &#64;&#64; -127,7 +126,7 &#64;&#64; static __always_inline void __ticket_spi<br /><br />Btw, I looked at that earlier, and it's still pretty sub-optimal.<br /><br />There's two problems - nobody actually uses the low-level code directly, <br />it goes through a <br /><br />	if (__ticket_spinlock()) {<br />		..<br />		return 1;<br />	}<br />	return 0;<br /><br />logic. Which means that regardless of what the asm does, the end result <br />will still be something like this _after_ the asm:<br /><br />        1:sete %al      # tmp<br /><br /># 0 "" 2<br />#NO_APP<br />        testb   %al, %al        # tmp<br />        leave<br />        setne   %al     #, tmp66<br /><br />ie you have that "sete" (in the asm) being then followed by testb/setne <br />again (from the C code wrappers).<br /><br />The other problem is that the compiler could actually generate better code <br />if you leave it to it, so doing<br /><br />        int tmp, new;<br /><br />        tmp = ACCESS_ONCE(lock-&gt;slock);<br />        new = tmp + 0x100;<br />        asm volatile("cmpb %h0,%b0\n\t"<br />                     "jne 1f\n\t"<br />                     LOCK_PREFIX "cmpxchgw %w2,%1\n\t"<br />                     "1:"<br />                     "sete %b0\n\t"<br />                     : "=a" (tmp), "+m" (lock-&gt;slock)<br />                     : "r" (new), "0" (tmp)<br />                     : "memory", "cc");<br /><br />        return tmp;<br /><br />actually seems to result in better code:<br /><br />	_raw_spin_trylock:<br />	        pushq   %rbp    #<br />	        movl    (%rdi), %eax    #* lock, D.17152<br />	        movq    %rsp, %rbp      #,<br />	        leal    256(%rax), %edx #, new<br />	#APP<br />	# 86 "/home/torvalds/v2.6/linux/arch/x86/include/asm/spinlock.h" 1<br />	        cmpb %ah,%al    # tmp<br />	        jne 1f<br />	        .section .smp_locks,"a"<br />	 .balign 8<br />	 .quad 661f<br />	.previous<br />	661:<br />	        lock; cmpxchgw %dx,(%rdi)       # new,* lock<br />	        1:sete %al      # tmp<br />	<br />	# 0 "" 2<br />	#NO_APP<br /><br />Look - no "movzwl" at all at the beginning, because it turns out that the <br />spinlock is a "unsigned int" and the "movl" to load the value pairs just <br />fine with the "leal" that the compiler can do too.  And we didn't <br />artificially constrain the second register to a byte register either (but <br />the compiler still picked %dx, of course).<br /><br />I dunno. I still don't get the feeling that any of this actually <br />_matters_.<br /><br />(Btw, the above isn't actually tested - I just edited the inline asm and <br />looked at what gcc generated, I didn't _run_ any of it).<br /><br />			Linus<br /><br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
