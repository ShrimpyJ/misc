    </div></td><td width="32">Â </td></tr><tr><td valign="top"><div class="es-jasper-simpleCalendar" baseurl="/lkml/"></div><div class="threadlist">Messages in this thread</div><ul class="threadlist"><li class="root"><a href="/lkml/2001/8/28/124">First message in thread</a></li><li><a href="/lkml/2001/8/28/124">"Van Maren, Kevin"</a><ul><li class="origin"><a href="/lkml/2001/8/28/138">Linus Torvalds</a><ul><li><a href="/lkml/2001/8/28/138">=?iso-8859-1?Q?Andr=E9?= Dahlqvist</a><ul><li><a href="/lkml/2001/8/29/43">Rik van Riel</a></li></ul></li><li><a href="/lkml/2001/8/29/18">Jens Axboe</a><ul><li><a href="/lkml/2001/8/29/20">Jens Axboe</a></li></ul></li></ul></li></ul></li></ul></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerl.gif" width="32" height="32" alt="/" /></td><td class="c" rowspan="2" valign="top" style="padding-top: 1em"><table><tr><td colspan="2"><!--BuySellAds Zone Code--><div id="bsap_1297613" class="bsarocks bsap_5aa49c00cc06c882289a1dd6a5e50b62"></div><!--End BuySellAds Zone Code--></td></tr><tr><td><table><tr><td class="lp">From</td><td class="rp" itemprop="author">Linus Torvalds &lt;&gt;</td></tr><tr><td class="lp">Date</td><td class="rp" itemprop="datePublished">Tue, 28 Aug 2001 11:52:50 -0700</td></tr><tr><td class="lp">Subject</td><td class="rp" itemprop="name">Re: The cause of the "VM" performance problem with 2.4.X</td></tr></table></td><td><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></td></tr></table><pre itemprop="articleBody">In article &lt;245F259ABD41D511A07000D0B71C4CBA289F5F&#64;us-slc-exch-3.slc.unisys.com&gt; you<br />write:<br />&gt;<br />&gt;Okay, here are some more results with Morton's "2nd" patch,<br />&gt;running 2.4.8,IA64-010815 on 4X IA64 (Lion) with 8GB RAM,<br />&gt;733MHz B3 (4MB) CPUs.  3 Adaptec 39160 channels, each with<br />&gt;8 (18GB) Cheetah drives, create 4 md's using 2 disks from<br />&gt;each channel.  Also using Gibbs' 6.2.0 adaptec driver.<br />&gt;I tried to extract the "important" info to keep this message<br />&gt;relatively small.  Meant to get this out earlier...<br /><br />Interesting.<br /><br />&gt;It looks like "getblk" could use some additional optimization.<br />&gt;More info to follow, but even doing one "mkfs" at a time,<br />&gt;the lru_list_lock is held over 82% of the time, mostly by<br />&gt;getblk().<br /><br />Note that the real fix is to make the block devices use the page cache.<br />Andrea has patches, but it's a 2.5.x thing.<br /><br />The buffer cache is really meant to be used only for meta-data, and<br />"getblk()" should not show up on profiles under any reasonable load. I<br />seriously doubt that running parallel (or even a single) "mkfs" is a<br />reasonable load under normal use.<br /><br />So I don't consider the buffer cache locking to be a critical<br />performance issue - but I _do_ want to make sure that we don't have any<br />_really_ horrid special cases where we just perform so abysmally that<br />it's not worth using Linux at all.<br /><br />So small inefficiencies and lack of parallelism is not a huge issue for<br />the buffer cache - at least not until somebody finds a more<br />"interesting" load per se. <br /><br />As to the really bad behaviour with the CPU spending all the time just<br />traversing the dirty buffer lists, I'd love to hear what 2.4.10-pre2<br />does.  It doesn't use Andrews advanced patch, but instead just makes<br />dirty buffer balancing not care about the device - which should allow<br />much better IO behaviour and not have any silly bad cases from a VM<br />standpoint. <br /><br /><br />&gt;Abbreiated/stripped kernprof/gprof output:<br />&gt;------------------------------------------<br />&gt;<br />&gt;Each sample counts as 0.000976562 seconds.<br />&gt;  %   cumulative   self              self     total<br />&gt; time   seconds   seconds    calls  ms/call  ms/call  name<br />&gt; 39.46    224.65   224.65                             cg_record_arc<br />&gt; 16.40    318.00    93.34  6722992     0.01     0.02  getblk<br />&gt;  9.02    369.33    51.33 50673121     0.00     0.00  spin_lock_<br />&gt;  6.67    407.27    37.95  6722669     0.01     0.01  _make_request<br />&gt;  4.51    432.97    25.70 13445261     0.00     0.00  blk_get_queue<br />&gt;  2.61    447.83    14.86                             long_copy_user<br />&gt;  2.59    462.56    14.72                             mcount<br />&gt;  2.06    474.27    11.71                             cpu_idle<br /><br />Now, while I don't worry about "getblk()" itself, the request stuff and<br />blk_get_queue() _can_ be quite an issue even under non-mkfs load, so<br />that certainly implies that we have some work to do. The area of the<br />block device request queueing is definitely something that 2.5.x will<br />start working on (Jens is already doing lots of stuff, obviously).<br /><br />And your lock profile certainly shows the io_request_lock as a _major_<br />lock user, although I'm happy to see that contention seems to be<br />reasonably low. Still, I'd bet that it is worth working on..<br /><br />		Linus<br />-<br />To unsubscribe from this list: send the line "unsubscribe linux-kernel" in<br />the body of a message to majordomo&#64;vger.kernel.org<br />More majordomo info at  <a href="http://vger.kernel.org/majordomo-info.html">http://vger.kernel.org/majordomo-info.html</a><br />Please read the FAQ at  <a href="http://www.tux.org/lkml/">http://www.tux.org/lkml/</a><br /><br /></pre><div align="center"><div class="shariff" data-services="[&quot;reddit&quot;]" data-theme="grey" data-lang="en" data-backend-url="//shariff.lkml.org/index.php"></div></div></td><td width="32" rowspan="2" class="c" valign="top"><img src="/images/icornerr.gif" width="32" height="32" alt="\" /></td></tr><tr><td align="right" valign="bottom">
